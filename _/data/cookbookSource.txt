package com.lingpipe.cookbook;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.PrintWriter;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Random;
import java.util.Set;

import au.com.bytecode.opencsv.CSVReader;
import au.com.bytecode.opencsv.CSVWriter;

import com.aliasi.classify.BaseClassifier;
import com.aliasi.classify.BaseClassifierEvaluator;
import com.aliasi.classify.Classification;
import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassification;
import com.aliasi.classify.ConditionalClassifierEvaluator;
import com.aliasi.classify.ConfusionMatrix;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.classify.PrecisionRecallEvaluation;
import com.aliasi.classify.ScoredClassification;
import com.aliasi.classify.ScoredClassifierEvaluator;
import com.aliasi.classify.ScoredPrecisionRecallEvaluation;
import com.aliasi.corpus.Corpus;
import com.aliasi.corpus.Handler;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;
import com.aliasi.spell.JaccardDistance;
import com.aliasi.stats.AnnealingSchedule;
import com.aliasi.stats.RegressionPrior;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenFeatureExtractor;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.Strings;
import com.lingpipe.cookbook.chapter3.OverfittingClassifier;

public class Util {

	public static int SCORE = 0;
	public static int GUESSED_CLASS = 1;
	public static int ANNOTATION_OFFSET = 2;
	public static int TEXT_OFFSET = 3;

	public static String SCORE_LABEL = "SCORE";
	public static String GUESS_LABEL = "GUESS";
	public static String TRUTH_LABEL = "TRUTH";
	public static String TEXT_LABEL = "TEXT";

	public static String[] ANNOTATION_HEADER_ROW = {SCORE_LABEL,GUESS_LABEL,TRUTH_LABEL,TEXT_LABEL};
	public static int ROW_LENGTH = ANNOTATION_HEADER_ROW.length;

	public static void printPrecRecall(BaseClassifierEvaluator<CharSequence> evaluator) {
		for (String category : evaluator.categories()) {
			PrecisionRecallEvaluation prEval = evaluator.oneVersusAll(category);
			System.out.println("Category " + category);
			System.out.printf("Recall: %.2f\n", prEval.recall());
			System.out.printf("Prec  : %.2f\n", prEval.precision());
		}
	}

	public static <E> void printPRcurve(ScoredClassifierEvaluator<E> evaluator) {
		PrintWriter progressWriter = new PrintWriter(System.out,true);
		boolean interpolate = false;
		for (String category : evaluator.categories()) {
			ScoredPrecisionRecallEvaluation scoredPrEval = evaluator.scoredOneVersusAll(category);
			double[][] scoredCurve = scoredPrEval.prScoreCurve(interpolate);
			progressWriter.println("PR Curve for Category: " + category);
			ScoredPrecisionRecallEvaluation.printScorePrecisionRecallCurve(scoredCurve,progressWriter);
		}
	}

	public static XValidatingObjectCorpus<Classified<CharSequence>> loadXValCorpus(List<String[]> rows, int numFolds) throws IOException { //this is covered in Ch 1 How to train and evaluate with cross validation

		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
		= new XValidatingObjectCorpus<Classified<CharSequence>>(numFolds);
		for (String[] row : rows) {
			String annotation = row[ANNOTATION_OFFSET];
			if (annotation.equals("")) {
				continue;
			}
			Classification classification = new Classification(row[ANNOTATION_OFFSET]);
			Classified<CharSequence> classified = new Classified<CharSequence>(row[TEXT_OFFSET],classification);
			corpus.handle(classified);
		}
		return corpus;
	}



	public static ObjectHandler<Classified<CharSequence>> corpusPrinter () {
		return new ObjectHandler<Classified<CharSequence>>() {

			@Override
			public void handle(Classified<CharSequence> e) {
				System.out.println(e.toString());
			}
		};
	}


	/*public static ObjectHandler<E> corpusPrinter2 () {
		return new ObjectHandler<E>() {

			@Override
			public void handle(E e) {
				System.out.println(e.toString());
			}
		};
	}
	 */


	public static String[] getCategoryArray(XValidatingObjectCorpus<Classified<CharSequence>> corpus) {
		final Set<String> categories =  new HashSet<String>();
		corpus.visitCorpus(new ObjectHandler<Classified<CharSequence>> () {
			@Override
			public void handle(Classified<CharSequence> e) {
				categories.add(e.getClassification().bestCategory());	
			}
		});
		return categories.toArray(new String[0]);
	}

	public static LogisticRegressionClassifier<CharSequence> trainLogReg(XValidatingObjectCorpus<Classified<CharSequence>> corpus, 
			TokenizerFactory tokenizerFactory, PrintWriter progressWriter) throws IOException {
		FeatureExtractor<CharSequence> featureExtractor
		= new TokenFeatureExtractor(tokenizerFactory);
		int minFeatureCount = 1;
		boolean addInterceptFeature = true;
		boolean noninformativeIntercept = true;
		RegressionPrior prior = RegressionPrior.gaussian(1.0,noninformativeIntercept);
		AnnealingSchedule annealingSchedule
		= AnnealingSchedule.exponential(0.00025,0.999);
		double minImprovement = 0.000000001;
		int minEpochs = 100;
		int maxEpochs = 2000;
		int blockSize = corpus.size(); 
		LogisticRegressionClassifier<CharSequence> hotStart = null;
		int rollingAvgSize = 10;
		ObjectHandler<LogisticRegressionClassifier<CharSequence>> classifierHandler
		= null;
		Reporter reporter = Reporters.writer(progressWriter);
		reporter.setLevel(LogLevel.WARN);
		/*return LogisticRegressionClassifier.<CharSequence>train(corpus,
				featureExtractor,
				minFeatureCount,
				addInterceptFeature,
				prior,
				blockSize,
				hotStart,
				annealingSchedule,
				minImprovement,
				rollingAvgSize,
				minEpochs,
				maxEpochs,
				classifierHandler,
				reporter);
				*/
		return LogisticRegressionClassifier.<CharSequence>train(corpus,
				featureExtractor,
				minFeatureCount,
				addInterceptFeature,
				prior,
				annealingSchedule,
				minImprovement,
				minEpochs,
				maxEpochs,
				reporter);
	}



	public static List<String[]> filterJaccard(List<String[]> texts,
			TokenizerFactory tokFactory,
			double cutoff) {
		JaccardDistance jaccardD = new JaccardDistance(tokFactory);
		List<String[]> filteredTexts = new ArrayList<String[]>();
		for (int i = 0; i < texts.size(); ++i) {
			String targetText = texts.get(i)[TEXT_OFFSET];
			boolean addText = true;
			//big research literature on making the below loop more efficient
			for (int j = i + 1; j < texts.size(); ++j ) {
				String comparisionText = texts.get(j)[TEXT_OFFSET];
				double proximity 
				= jaccardD.proximity(targetText,comparisionText);
				if (proximity >= cutoff) {
					addText = false;
					//System.out.printf(" Texts too close, proximity %.2f\n", proximity);
					//System.out.println("\t" + targetText);
					//System.out.println("\t" + comparisionText);
					break; //one nod to efficency
				}
			}
			if (addText) {
				filteredTexts.add(texts.get(i));
			}
		}
		return filteredTexts;
	}
	/*
	 * Described in Chapter 1: Deserializing and running a classifier
	 */
	public static void consoleInputBestCategory(BaseClassifier<CharSequence> classifier) throws IOException {
		BufferedReader reader = new BufferedReader(new 	InputStreamReader(System.in));
		while (true) {
			System.out.println("\nType a string to be classified. Empty string to quit.");
			String data = reader.readLine();
			if (data.equals("")) {
				return;
			}
			Classification classification 
			= classifier.classify(data);
			System.out.println("Best Category: " + classification.bestCategory());
		}
	}
	/*
	 * Described in Chapter 1: Getting confidence estimates from a classifier
	 */
	public static void consoleInputPrintClassification(BaseClassifier<CharSequence> classifier) throws IOException {
		BufferedReader reader = new BufferedReader(new 	InputStreamReader(System.in));
		while (true) {
			System.out.println("\nType a string to be classified. Empty string to quit.");
			String data = reader.readLine();
			if (data.equals("")) {
				return;
			}
			Classification classification 
			= classifier.classify(data);
			System.out.println(classification);
		}
	}
	/*
	 * Described in Chapter 1: Evaluation of classifiers—the confusion matrix
	 */
	public static List<String[]> readAnnotatedCsvRemoveHeader(File file) throws IOException {
		FileInputStream fileInputStream = new FileInputStream(file);
		InputStreamReader inputStreamReader = new InputStreamReader(fileInputStream,Strings.UTF8);
		CSVReader csvReader = new CSVReader(inputStreamReader);
		csvReader.readNext();//skip headers
		List<String[]> rows = new ArrayList<String[]>();
		String[] row;
	
		while ((row = csvReader.readNext()) != null) {
	
			if (row[ANNOTATION_OFFSET].equals("")) {
				continue;
			}
			rows.add(row);
		}
		csvReader.close();
		return rows;
	}

	/*
	 * Described in Chapter 1: Apply a classifier to a .csv file
	 */
	public static List<String[]> readCsvRemoveHeader(File file) throws IOException {
		FileInputStream fileIn = new FileInputStream(file);
		InputStreamReader inputStreamReader = new InputStreamReader(fileIn,Strings.UTF8);
		CSVReader csvReader = new CSVReader(inputStreamReader);
		csvReader.readNext();//skip headers
		List<String[]> rows = new ArrayList<String[]>();
		String[] row;
		while ((row = csvReader.readNext()) != null) {
			if (row[TEXT_OFFSET] == null || row[TEXT_OFFSET].equals("")) {
				continue;
			}
			rows.add(row);
		}
		csvReader.close();
		return rows; 
	}


	/*
	 * Described minimally in Chapter 1: Evaluation of classifiers—the confusion matrix
	 */

	public static String confusionMatrixToString(ConfusionMatrix confMatrix) {
		StringBuilder sb = new StringBuilder();
		String[] labels = confMatrix.categories();
		int[][] outcomes = confMatrix.matrix();
		sb.append("reference\\response\n");
		sb.append("          \\");
		for (String category : labels) {
			sb.append(category + ",");
		}
		for (int i = 0; i< outcomes.length; ++ i) {
			sb.append("\n         " + labels[i] + " ");
			for (int j = 0; j < outcomes[i].length; ++j) {
				sb.append(outcomes[i][j] + ",");
			}
		}
		return sb.toString();
	}

	/*
	 * Described in Chapter 1: Evaluation of classifiers—the confusion matrix
	 */

	public static String[] getCategories(List<String[]> data) {
		Set<String> categories = new HashSet<String>();
		for (String[] csvData : data) {
			if (!csvData[ANNOTATION_OFFSET].equals("")) {
				categories.add(csvData[ANNOTATION_OFFSET]);
			}
		}
		return categories.toArray(new String[0]);
	}

	public static List<String[]> readCsv(File file) throws IOException {
		CSVReader csvReader = new CSVReader(new InputStreamReader(new FileInputStream(file),Strings.UTF8));
		List<String[]> data = csvReader.readAll();
		csvReader.close();
		return data;
	}

	public static void writeCsv(List<String[]> data, File file) throws IOException {
		CSVWriter csvWriter = new CSVWriter(new OutputStreamWriter(new FileOutputStream(file),Strings.UTF8));
		csvWriter.writeAll(data);
		csvWriter.close();
	}

	/**
	 * Writes csv outfile used throughout book with standard header. See ANNOTATION_HEADER_ROW for values
	 *
	 * @param data List<String[]> data with each list element a row in spreadsheet. Index 0 = score, 1 = response class, 2 = reference class (truth), 3 Text
	 * @param File file output file should end with .csv
	 * 
	 */

	/*
	 * Described in Chapter 1: Getting data from Twitter API
	 */
	public static void writeCsvAddHeader(List<String[]> data, File file) throws IOException {
		CSVWriter csvWriter = new CSVWriter(new OutputStreamWriter(new FileOutputStream(file),Strings.UTF8));
		csvWriter.writeNext(ANNOTATION_HEADER_ROW);
		csvWriter.writeAll(data);
		csvWriter.close();
	}

	public static void dedupeCsvFile(File inputCsv,File outputCsv, double threshold) throws IOException {

		List<String[]> data = readCsvRemoveHeader(inputCsv);
		System.out.println("Start with " + data.size());
		data = filterJaccard(data, IndoEuropeanTokenizerFactory.INSTANCE, threshold);
		System.out.println("Post dedupe size of " + data.size());
		writeCsvAddHeader(data, outputCsv);
	}

	public static void main(String args[]) throws IOException {
		File input = new File(args[0]);
		File output = new File(args[1]);
		dedupeCsvFile(input,output,.5d);
		System.out.println("Done");
	}
	
	
	
	/*
	 * Described in Chapter 1: Viewing error categories: False Positives
	 */
	public static <E> void printFalsePositives(String categoryToShow, ScoredClassifierEvaluator<E> evaluator, Corpus<ObjectHandler<Classified<E>>> corpus) throws IOException {
		final Map<E,Classification> truthMap = new HashMap<E,Classification>();
		corpus.visitCorpus(new ObjectHandler<Classified<E>>() {
			@Override
			public void handle(Classified<E> data) {
				truthMap.put(data.getObject(),data.getClassification());
			}
		});
		//System.out.println(evaluator.scoredOneVersusAll(category));
		List<Classified<E>> falsePositives 
			= evaluator.falsePositives(categoryToShow);
		System.out.println("\nFalse Positives for " + categoryToShow);
		System.out.println("*<category> is truth category");
		for (Classified<E> classified : falsePositives) {
			E data = classified.getObject();
			ScoredClassification sysClassification = (ScoredClassification) classified.getClassification();
			System.out.println("\n" + data);
			String truth = truthMap.get(data).bestCategory();
			for (int i = 0; i < evaluator.categories().length; ++i) {
				String category = sysClassification.category(i);
				String truthStar = category.equals(truth) ? "*" : " ";
				System.out.println(truthStar + category + " " + sysClassification.score(i));
			}
			
			//
			//System.out.println(data + " : " + truthClassification.bestCategory());
		}
	}


	public static void printConfusionMatrix(ConfusionMatrix confMatrix) {
		System.out.println(confusionMatrixToString(confMatrix));
	}

/* described in Chapter 3: TuneLogRegParam
 * 
 */
	public static <E> ConditionalClassifierEvaluator<E> xvalLogRegMultiThread(
			final XValidatingObjectCorpus<Classified<E>> corpus,
			final FeatureExtractor<E> featureExtractor,
			final int minFeatureCount, final boolean addInterceptFeature,
			final RegressionPrior prior, final AnnealingSchedule annealingSchedule,
			final double minImprovement, final int minEpochs, final int maxEpochs,
			final Reporter reporter, final int numFolds, final int numThreads, final String[] categories) throws IOException {
		
		corpus.setNumFolds(numFolds);
		corpus.permuteCorpus(new Random(11211));
		final boolean storeInputs = true;
		final ConditionalClassifierEvaluator<E> crossFoldEvaluator
			= new ConditionalClassifierEvaluator<E>(null, categories, storeInputs);
		List<Thread> threads = new ArrayList<Thread>();
		for (int i = 0; i < numFolds; ++i) {
			final XValidatingObjectCorpus<Classified<E>> fold = corpus.itemView();
			fold.setFold(i);
			Runnable runnable = new Runnable() {
				@Override
				public void run() {
					try {
						LogisticRegressionClassifier<E> classifier
							= LogisticRegressionClassifier.<E>train(fold,
								featureExtractor,
								minFeatureCount,
								addInterceptFeature,
								prior,
								annealingSchedule,
								minImprovement,
								minEpochs,
								maxEpochs,
								reporter);
						
						ConditionalClassifierEvaluator<E> withinFoldEvaluator 
							= new ConditionalClassifierEvaluator<E>(classifier, categories, storeInputs);
						crossFoldEvaluator.setClassifier(classifier);
						addToEvaluator2(fold,crossFoldEvaluator);
						//addToEvaluator(withinFoldEvaluator,crossFoldEvaluator);
					}
					catch (Exception e) {
						e.printStackTrace();
					}
				}
			};
			threads.add(new Thread(runnable,"Fold " + i));
		}
		runThreads(threads,numThreads);
		//printPRcurve(crossFoldEvaluator);
		return crossFoldEvaluator;
	}
	
	
	
	
	//does not increment data for prCurves
	/*public synchronized static <E> void addToEvaluator(BaseClassifierEvaluator<E> foldEval, ScoredClassifierEvaluator<E> crossFoldEval) {
		for (String category : foldEval.categories()) {
			for (Classified<E> classified : foldEval.truePositives(category)) {
				crossFoldEval.addClassification(category,classified.getClassification(),classified.getObject());
			}
			for (Classified<E> classified : foldEval.falseNegatives(category)) {
				crossFoldEval.addClassification(category,classified.getClassification(),classified.getObject());
			}
		}
	}
	*/
	
	public synchronized static <E> void addToEvaluator2(XValidatingObjectCorpus<Classified<E>> fold , 
														ObjectHandler<Classified<E>> crossFoldEvaluator) {
		fold.visitTest(crossFoldEvaluator);
	}
	
	
	public synchronized static <E> void addToEvaluator(final ConditionalClassifierEvaluator<E> foldEval, ConditionalClassifierEvaluator<E> crossFoldEval) {
		
		
		
		for (String category : foldEval.categories()) {
			for (Classified<E> classified : foldEval.truePositives(category)) {
				
				crossFoldEval.addClassification(category,classified.getClassification(),classified.getObject());
			}
			for (Classified<E> classified : foldEval.falseNegatives(category)) {
				crossFoldEval.addClassification(category,classified.getClassification(),classified.getObject());
			}
		}
	}
	
	public static void runThreads(List<Thread> threads, int maxThreads) {
		Set<Thread> threadsToRun = new HashSet<Thread>(threads);
		List<Thread> running = new ArrayList<Thread>();
		int numThreads = threads.size();
		int runCount = 0;
		while(threadsToRun.size() > 0 || running.size() > 0) {
			Iterator<Thread> it = running.iterator();
			while(it.hasNext()) {
				if(!it.next().isAlive()) {
					it.remove();
				}
			}
			it = threadsToRun.iterator();
			while(running.size() < maxThreads && it.hasNext()) {
				Thread t = it.next();
				it.remove();
				running.add(t);
				System.out.println("RUNNING thread " + t.getName() + " (" + (++runCount) + " of " + numThreads + ")");
				t.start();
			}
		}
	}


}
package com.lingpipe.cookbook.chapter1;

import java.io.File;
import java.io.IOException;
import java.util.List;

import com.aliasi.tokenizer.RegExTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.lingpipe.cookbook.Util;

public class DeduplicateCsvData {
	
		public static void main(String[] args) throws IOException {
			String inputPath = args.length > 0 ? args[0] : "data/disney.csv";	
			String outputPath = args.length > 1 ? args[1] : "data/disneyDeduped.csv";	
			List<String[]> data = Util.readCsvRemoveHeader(new File(inputPath));
			System.out.println(data.size());
			TokenizerFactory tokenizerFactory = new RegExTokenizerFactory("\\w+");
			double cutoff = .5d;
			List<String[]> dedupedData = Util.filterJaccard(data, tokenizerFactory, cutoff);
			System.out.println(dedupedData.size());
			Util.writeCsvAddHeader(dedupedData, new File(outputPath));
		}
}
package com.lingpipe.cookbook.chapter1;

import java.io.File;
import java.io.IOException;
import java.util.List;

import com.aliasi.classify.BaseClassifier;
import com.aliasi.classify.Classification;
import com.aliasi.util.AbstractExternalizable;
import com.lingpipe.cookbook.Util;

public class ReadClassifierRunOnCsv {

	public static void main(String[] args) throws IOException, ClassNotFoundException {
		String inputPath = args.length > 0 ? inputPath = args[0] : "data/disney.csv";	
		String classifierPath = args.length > 1 ? args[1] : "models/3LangId.LMClassifier";
		@SuppressWarnings("unchecked")
		BaseClassifier<CharSequence> classifier 
			= (BaseClassifier<CharSequence>) 
				AbstractExternalizable.readObject(new File(classifierPath));
		List<String[]> lines = Util.readCsvRemoveHeader(new File(inputPath));
		for(String [] line: lines) {
			String text = line[Util.TEXT_OFFSET];
			Classification classified = classifier.classify(text);
			System.out.println("InputText: " + text);
			System.out.println("Best Classified Language: " + classified.bestCategory());
		}
	}
}
package com.lingpipe.cookbook.chapter1;

import java.io.File;
import java.io.IOException;
import java.util.List;
import java.util.Random;

import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassifierEvaluator;
import com.aliasi.classify.ConfusionMatrix;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.lm.NGramBoundaryLM;
import com.lingpipe.cookbook.Util;

public class ReportFalsePositivesOverXValidation {

	public static void main(String[] args) throws IOException {
		String inputPath = args.length > 0 ? args[0] : "data/disney_e_n.csv";	
		System.out.println("Training data is: " + inputPath);
		List<String[]> truthData = Util.readAnnotatedCsvRemoveHeader(new File(inputPath));
		int numFolds = 4;
		XValidatingObjectCorpus<Classified<CharSequence>> corpus =
				Util.loadXValCorpus(truthData, numFolds);
		corpus.permuteCorpus(new Random(123413));
		String[] categories = Util.getCategories(truthData);
		boolean storeInputs = true;
		ConditionalClassifierEvaluator<CharSequence> evaluator 
		= new ConditionalClassifierEvaluator<CharSequence>(null, categories, storeInputs);
		int maxCharNGram = 3;
		for (int i = 0; i < numFolds; ++i) {
			corpus.setFold(i);
			DynamicLMClassifier<NGramBoundaryLM> classifier 
			= DynamicLMClassifier.createNGramBoundary(categories, maxCharNGram);
			corpus.visitTrain(classifier);
			evaluator.setClassifier(classifier);
			corpus.visitTest(evaluator);
		}
		ConfusionMatrix confMatrix = evaluator.confusionMatrix();
		Util.printConfusionMatrix(confMatrix);
		for (String category : categories) {
			Util.printFalsePositives(category, evaluator, corpus);
		}
	}
}	
package com.lingpipe.cookbook.chapter1;

import java.io.File;
import java.io.IOException;

import com.aliasi.classify.BaseClassifier;
import com.aliasi.util.AbstractExternalizable;
import com.lingpipe.cookbook.Util;

public class RunClassifierFromDisk {

	public static void main(String[] args) throws IOException, ClassNotFoundException {
		String classifierPath = args.length > 0 ? args[0] : "models/3LangId.LMClassifier";
		System.out.println("Loading: " + classifierPath);
		File serializedClassifier = new File(classifierPath);
		@SuppressWarnings("unchecked")
		BaseClassifier<CharSequence> classifier
			= (BaseClassifier<CharSequence>) AbstractExternalizable.readObject(serializedClassifier);
		Util.consoleInputBestCategory(classifier);	
	}
	
}
package com.lingpipe.cookbook.chapter1;

import java.io.File;
import java.io.IOException;

import com.aliasi.classify.JointClassifier;
import com.aliasi.util.AbstractExternalizable;
import com.lingpipe.cookbook.Util;

public class RunClassifierJoint {
	public static void main(String[] args) throws IOException, ClassNotFoundException {
		String classifierPath = args.length > 0 ? args[0] : "models/3LangId.LMClassifier";
		@SuppressWarnings("unchecked")
		JointClassifier<CharSequence> classifier 
			= (JointClassifier<CharSequence>) AbstractExternalizable.readObject(new File(classifierPath));
		Util.consoleInputPrintClassification(classifier);
	}
}
package com.lingpipe.cookbook.chapter1;

import java.io.File;
import java.io.IOException;
import java.util.List;

import com.aliasi.classify.BaseClassifier;
import com.aliasi.classify.BaseClassifierEvaluator;
import com.aliasi.classify.Classification;
import com.aliasi.classify.Classified;
import com.aliasi.util.AbstractExternalizable;
import com.lingpipe.cookbook.Util;

public class RunConfusionMatrix {

	public static void main(String[] args) throws IOException, ClassNotFoundException {
		String inputPath = args.length > 0 ? args[0] : "data/disney_e_n.csv";	
		String classifierPath = args.length > 1 ? args[1] : "models/1LangId.LMClassifier";
		@SuppressWarnings("unchecked")
		BaseClassifier<CharSequence> classifier = (BaseClassifier<CharSequence>) AbstractExternalizable.readObject(new File(classifierPath));
		List<String[]> rows = Util.readAnnotatedCsvRemoveHeader(new File(inputPath));
		String[] categories = Util.getCategories(rows);
		boolean storeInputs = false;
		BaseClassifierEvaluator<CharSequence> evaluator 
			= new BaseClassifierEvaluator<CharSequence>(classifier,categories, storeInputs);
		for (String[] row : rows) {
			String truth = row[Util.ANNOTATION_OFFSET];
			String text = row[Util.TEXT_OFFSET];
			Classification classification = new Classification(truth);
			Classified<CharSequence> classified = new Classified<CharSequence>(text,classification);
			evaluator.handle(classified);
		}
		Util.printConfusionMatrix(evaluator.confusionMatrix());
	}
}
package com.lingpipe.cookbook.chapter1;

import java.io.File;
import java.io.IOException;
import java.util.List;
import java.util.Random;

import com.aliasi.classify.BaseClassifierEvaluator;
import com.aliasi.classify.Classified;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.lm.NGramBoundaryLM;
import com.lingpipe.cookbook.Util;

public class RunXValidate {

	public static void main(String[] args) throws IOException {
		String inputPath = args.length > 0 ? args[0] : "data/disney_e_n.csv";	
		System.out.println("Training data is: " + inputPath);
		List<String[]> truthData = Util.readAnnotatedCsvRemoveHeader(new File(inputPath));
		int numFolds = 4;
		XValidatingObjectCorpus<Classified<CharSequence>> corpus =
			Util.loadXValCorpus(truthData, numFolds);
		corpus.permuteCorpus(new Random(123413));
		String[] categories = Util.getCategories(truthData);
		boolean storeInputs = false;
		BaseClassifierEvaluator<CharSequence> evaluator 
			= new BaseClassifierEvaluator<CharSequence>(null, categories, storeInputs);
		int maxCharNGram = 3;
		for (int i = 0; i < numFolds; ++i) {
			corpus.setFold(i);
			DynamicLMClassifier<NGramBoundaryLM> classifier 
				= DynamicLMClassifier.createNGramBoundary(categories, maxCharNGram);
			System.out.println("Training on fold " + i);
			//corpus.visitTrain(Util.corpusPrinter());
			corpus.visitTrain(classifier);
			evaluator.setClassifier(classifier);
			System.out.println("Testing on fold " + i);
			//corpus.visitTest(Util.corpusPrinter());
			corpus.visitTest(evaluator);
		}
		System.out.println(Util.confusionMatrixToString(evaluator.confusionMatrix()));
	}
}
package com.lingpipe.cookbook.chapter1;


import java.io.File;
import java.io.IOException;
import java.util.List;

import com.aliasi.classify.Classification;
import com.aliasi.classify.Classified;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.lm.NGramBoundaryLM;
import com.lingpipe.cookbook.Util;

public class TrainAndRunLMClassifier {

	public static void main(String[] args) throws IOException {
		String dataPath = args.length > 0 ? args[0] : "data/disney_e_n.csv";
		List<String[]> annotatedData = Util.readAnnotatedCsvRemoveHeader(new File(dataPath));
		String[] categories = Util.getCategories(annotatedData);
		int maxCharNGram = 3;
		DynamicLMClassifier<NGramBoundaryLM> classifier 
			= DynamicLMClassifier.createNGramBoundary(categories,maxCharNGram);
		for (String[] row: annotatedData) {
			String truth = row[Util.ANNOTATION_OFFSET];
			String text = row[Util.TEXT_OFFSET];
			Classification classification = new Classification(truth);
			Classified<CharSequence> classified = new Classified<CharSequence>(text,classification);
			classifier.handle(classified);
			//int count = 1;
			//classifier.train(truth,text,count);
		}
		Util.consoleInputPrintClassification(classifier);
	}
}

package com.lingpipe.cookbook.chapter1;

import java.io.File;
import java.io.IOException;
import java.util.List;

import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.lm.NGramBoundaryLM;
import com.aliasi.util.AbstractExternalizable;
import com.lingpipe.cookbook.Util;


public class TrainAndWriteClassifierToDisk {
	public static void main(String[] args) throws IOException, ClassNotFoundException {
		String inputPath = args.length > 0 ? args[0] : "data/disney_e_n.csv";	
		String outputPath = args.length > 1 ? args[1] : "models/my_disney_e_n.LMClassifier";
		System.out.println("Training on " + inputPath);
		List<String[]> annotatedData = Util.readAnnotatedCsvRemoveHeader(new File(inputPath));
		File outFile = new File(outputPath);
		String[] categoriesList = Util.getCategories(annotatedData);
		int maxCharNGram = 3;
		DynamicLMClassifier<NGramBoundaryLM> classifier 
			= DynamicLMClassifier.createNGramBoundary(categoriesList, maxCharNGram);
		int count = 1;
		for (String[] row : annotatedData) {
				classifier.train(row[Util.ANNOTATION_OFFSET], row[Util.TEXT_OFFSET], count);
		}
		AbstractExternalizable.compileTo(classifier,outFile);
		System.out.println("Wrote model to " + outFile);
	}

}

package com.lingpipe.cookbook.chapter1;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.List;

import com.lingpipe.cookbook.Util;

import twitter4j.Query;
import twitter4j.QueryResult;
import twitter4j.Status;
import twitter4j.Twitter;
import twitter4j.TwitterException;
import twitter4j.TwitterFactory;

public class TwitterSearch {

	static final int TWEETS_PER_PAGE = 100;
	static final int MAX_TWEETS = 1500;

	public static void main (String[] args) 
			throws IOException, TwitterException {
		String outFilePath = args.length > 0 ? args[0] : "data/twitterSearch.csv";
		File outFile = new File(outFilePath);
		System.out.println("Writing output to " + outFile);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		System.out.print("Enter Twitter Query:");
		String queryString = reader.readLine();
		Twitter twitter = new TwitterFactory().getInstance();
		Query query = new Query(queryString + " -filter:retweets");
		query.setLang("en");
		query.setCount(TWEETS_PER_PAGE);
		query.setResultType(Query.RECENT);
		List<String[]> csvRows = new ArrayList<String[]>();
		while(csvRows.size() < MAX_TWEETS) {
			QueryResult result = twitter.search(query);
			List<Status> resultTweets = result.getTweets();
			for (Status tweetStatus : resultTweets) {
				String row[] = new String[Util.ROW_LENGTH];
				row[Util.TEXT_OFFSET] = tweetStatus.getText();
				csvRows.add(row);
			}
			System.out.println("Tweets Accumulated: " + csvRows.size());
			if ((query = result.nextQuery()) == null) {
				break;
			}
		}
		
		System.out.println("writing to disk " + csvRows.size() + " tweets at " + outFilePath);
		Util.writeCsvAddHeader(csvRows, outFile);
		
	}
}
package com.lingpipe.cookbook.chapter2;

import java.io.CharArrayReader;
import java.io.IOException;
import java.io.ObjectInput;
import java.io.ObjectOutput;
import java.io.Reader;
import java.io.Serializable;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
import org.apache.lucene.util.Version;

import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;

public class LuceneAnalyzerTokenizerFactory implements TokenizerFactory, Serializable {

	private static final long serialVersionUID = -8376017491713196935L;
	private Analyzer analyzer;
	private String field;

	public LuceneAnalyzerTokenizerFactory(Analyzer analyzer, String field) {
		this.analyzer = analyzer;
		this.field = field;
	}

	private Object writeReplace(){
		return new Serializer(this);
	}

	@Override
	public Tokenizer tokenizer(char[] charSeq , int start, int length) {
		Reader reader = new CharArrayReader(charSeq,start,length);
		TokenStream tokenStream = null;
		try {
			tokenStream = analyzer.tokenStream(field,reader);
			tokenStream.reset();
		} catch (IOException e) {
			e.printStackTrace();
		}
		return new LuceneTokenStreamTokenizer(tokenStream);
	}

	

	private static class Serializer extends AbstractExternalizable {
		private LuceneAnalyzerTokenizerFactory latf;

		public Serializer(LuceneAnalyzerTokenizerFactory luceneAnalyzerTokenizerFactory) {
			this.latf = luceneAnalyzerTokenizerFactory;
		}

		@Override
		protected Object read(ObjectInput in) throws ClassNotFoundException,IOException {
			Analyzer analyzer = (Analyzer) in.readObject();
			String field = in.readUTF();
			return new LuceneAnalyzerTokenizerFactory(analyzer, field);
		}

		@Override
		public void writeExternal(ObjectOutput out) throws IOException {
			out.writeObject(latf.analyzer);
			out.writeUTF(latf.field);
		}

	}

	static class LuceneTokenStreamTokenizer extends Tokenizer {

		private TokenStream tokenStream;
		private CharTermAttribute termAttribute;
		private OffsetAttribute offsetAttribute;

		private int lastTokenStartPosition = -1;
		private int lastTokenEndPosition = -1;

		public LuceneTokenStreamTokenizer(TokenStream ts) {
			tokenStream = ts;
			termAttribute = tokenStream.addAttribute(CharTermAttribute.class);
			offsetAttribute = tokenStream.addAttribute(OffsetAttribute.class);
		}
		
		@Override
		public String nextWhitespace() {
			return "default";
		}
		
		@Override
		public String nextToken() {
			try {
				if(tokenStream.incrementToken()){
					lastTokenStartPosition = offsetAttribute.startOffset();
					lastTokenEndPosition = offsetAttribute.endOffset();
					return termAttribute.toString();
				} else {
					endAndClose();
					return null;
				}
			} catch (IOException e) {
				endAndClose();
				return null;
			}
		}

		@Override
		public int lastTokenStartPosition(){
			return lastTokenStartPosition;
		}

		@Override
		public int lastTokenEndPosition(){
			return lastTokenEndPosition;
		}

		private void endAndClose() {

			try {
				tokenStream.end();
			} catch (IOException e) {
				//Cant do anything with this.
			}
			try{
				tokenStream.close();
			} catch (IOException e) {
				//Cant do anything with this.
			}
		}
	}
	
	public static void main(String[] args){
		String text = "Hi how are you? Are the numbers 1 2 3 4.5 all integers?";
		Analyzer analyzer
			= new StandardAnalyzer(Version.LUCENE_46);
		TokenizerFactory tokFactory = new LuceneAnalyzerTokenizerFactory(analyzer, "DEFAULT");
		Tokenizer tokenizer = tokFactory.tokenizer(text.toCharArray(), 0, text.length());
		/*for (String token: tokenizer.tokenize()){
			System.out.println("Token: " + token);
		}
		*/
		String token = null;
		while ((token = tokenizer.nextToken()) != null) {
			String ws = tokenizer.nextWhitespace();
			System.out.println("Token:'" + token + "'");
			System.out.println("WhiteSpace:'" + ws + "'");
		}

	}
	
}

package com.lingpipe.cookbook.chapter2;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;

import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.LowerCaseTokenizerFactory;
import com.aliasi.tokenizer.ModifyTokenTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;

public class Rot13TokenizerFactory extends ModifyTokenTokenizerFactory{

	
	
	private static final long serialVersionUID = -3200042384201663330L;

	
	public Rot13TokenizerFactory(TokenizerFactory f) {
        super(f);
    }
	
	@Override
    public String modifyToken(String tok) {
        return rot13(tok);
    }

    public static String rot13(String input) {
    	StringBuilder sb = new StringBuilder();
    	for (int i = 0; i < input.length(); i++) {
    		char c = input.charAt(i);
    		if       (c >= 'a' && c <= 'm') c += 13;
    		else if  (c >= 'A' && c <= 'M') c += 13;
    		else if  (c >= 'n' && c <= 'z') c -= 13;
    		else if  (c >= 'N' && c <= 'Z') c -= 13;
    		sb.append(c);
    	}
    	return sb.toString();
    }


	public static void main(String[] args) throws IOException {
		
		TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		tokFactory = new LowerCaseTokenizerFactory(tokFactory);
		tokFactory = new Rot13TokenizerFactory(tokFactory);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.println("type a sentence below to see the tokens and white spaces:");
			String input = reader.readLine();
			Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(), 0, input.length());
			String token = null;
			StringBuilder sb = new StringBuilder();
			while ((token = tokenizer.nextToken()) != null) {
				String ws = tokenizer.nextWhitespace();
				System.out.println("Token:'" + token + "'");
				sb.append(token);
				sb.append(ws);
			}
			System.out.println("Modified Output: " + sb.toString());
		}

	}

}
package com.lingpipe.cookbook.chapter2;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;

import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;

public class RunBaseTokenizerFactory {
	public static void main(String[] args) throws IOException {
		TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.println("type a sentence to see to see tokens and white spaces");
			String input = reader.readLine();
			Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(), 0, input.length());
			String token = null;
			while ((token = tokenizer.nextToken()) != null) {
				System.out.println("Token:'" + token + "'");
				System.out.println("WhiteSpace:'" + tokenizer.nextWhitespace() + "'");
			}
		}
	}
}
package com.lingpipe.cookbook.chapter2;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;

import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.LowerCaseTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.tokenizer.WhitespaceNormTokenizerFactory;

public class RunLowerCaseTokenizerFactory {

	
	public static void main(String[] args) throws IOException {
		TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		tokFactory = new LowerCaseTokenizerFactory(tokFactory);
		tokFactory = new WhitespaceNormTokenizerFactory(tokFactory);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.println("type a sentence below to see the tokens and white spaces are:");
			String input = reader.readLine();
			Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(), 0, input.length());
			String token = null;
			while ((token = tokenizer.nextToken()) != null) {
				System.out.println("Token:'" + token + "'");
				System.out.println("WhiteSpace:'" + tokenizer.nextWhitespace() + "'");
			}
		}

	}

}
package com.lingpipe.cookbook.chapter2;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.Reader;
import java.io.StringReader;

import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.core.LowerCaseFilter;
import org.apache.lucene.analysis.standard.StandardTokenizer;
import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
import org.apache.lucene.util.Version;



public class RunLuceneTokenizer {

	public static void main(String[] args) throws IOException{


		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));

		while (true) {
			System.out.println("type a sentence below to see the tokens and white spaces:");
			String input = reader.readLine();	
			Reader stringReader = new StringReader(input);
			TokenStream tokenStream = new StandardTokenizer(Version.LUCENE_46,stringReader);
			tokenStream = new LowerCaseFilter(Version.LUCENE_46,tokenStream);
			CharTermAttribute terms = tokenStream.addAttribute(CharTermAttribute.class);
			OffsetAttribute offset = tokenStream.addAttribute(OffsetAttribute.class);
			tokenStream.reset();
			while (tokenStream.incrementToken()) {
				String token = terms.toString();
				int start = offset.startOffset();
				int end = offset.endOffset();
				System.out.println("Token:'" + token + "'" + " Start: " + start + " End:" + end);
			}
			tokenStream.end();
			tokenStream.close();
			
		}
	}
}
package com.lingpipe.cookbook.chapter2;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.HashSet;
import java.util.Set;

import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.LowerCaseTokenizerFactory;
import com.aliasi.tokenizer.StopTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;

public class RunStopTokenizerFactory {

	
	public static void main(String[] args) throws IOException {
		TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		tokFactory = new LowerCaseTokenizerFactory(tokFactory);
		Set<String> stopWords = new HashSet<String>();
		stopWords.add("the");
		stopWords.add("of");
		stopWords.add("to");
		stopWords.add("is");
		
		tokFactory = new StopTokenizerFactory(tokFactory, stopWords);
		
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.println("type a sentence below to see the tokens and white spaces:");
			String input = reader.readLine();
			Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(), 0, input.length());
			String token = null;
			while ((token = tokenizer.nextToken()) != null) {
				System.out.println("Token:'" + token + "'");
				System.out.println("WhiteSpace:'" + tokenizer.nextWhitespace() + "'");
			}
		}
	}
}
package com.lingpipe.cookbook.chapter2;

import com.aliasi.tokenizer.RegExTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;

public class TestTokenizerFactory {

	static void checkTokens(TokenizerFactory tokFactory, String input, String[] correctTokens) {
		 Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(),0,input.length());
			String[] tokens =	tokenizer.tokenize();
		if (tokens.length != correctTokens.length) {
			System.out.println("Token list lengths do not match");
			System.exit(-1);
		}
		for (int i = 0; i < tokens.length; ++i) {
			if (!correctTokens[i].equals(tokens[i])) {
				System.out.println("Token mismatch: got |" + tokens[i] + "|");
				System.out.println(" expected |" + correctTokens[i] + "|" );
				System.exit(-1);
			}
		}
	}
	
	static void checkTokensAndWhiteSpaces(TokenizerFactory tokFactory, String string, String[] correctTokens, String[] correctWhiteSpaces) {
		Tokenizer tokenizer = tokFactory.tokenizer(string.toCharArray(),0,string.length());
		String token = null;
		int index = -1;
		while ((token = tokenizer.nextToken()) != null) {
			String whiteSpace = tokenizer.nextWhitespace();
			++index;
			System.out.println(index + " " + token + "|" + whiteSpace + "|");
			if (index >= correctTokens.length ) {
				System.out.println("Token list lengths do not match");
				System.exit(-1);
			}
			if (!correctTokens[index].equals(token)) {
				System.out.println("Token mismatch: got |" + token + "|");
				System.out.println(" expected |" + correctTokens[index] + "|" );
				System.exit(-1);
			}
			if (index > correctWhiteSpaces.length ) {
				System.out.println("White space list lengths do not match");
				System.exit(-1);
			}
			if (!correctWhiteSpaces[index].equals(whiteSpace)) {
				System.out.println("White space mismatch: got |" + whiteSpace + "|");
				System.out.println(" expected |" + correctWhiteSpaces[index] + "|" );
				System.out.println("at index " + index);
				System.exit(-1);
			}	
		}
		
	}
	
	public static void main(String[] args) {
		String pattern = "[a-zA-Z]+|[0-9]+|\\S";
		TokenizerFactory tokFactory = new RegExTokenizerFactory(pattern);
		String[] tokens = {"Tokenizers","need","unit","tests","."};
		String text = "Tokenizers need unit tests.";
		checkTokens(tokFactory,text,tokens);
		String[] whiteSpaces = {" "," "," ","",""};
		checkTokensAndWhiteSpaces(tokFactory,text,tokens,whiteSpaces);
		System.out.println("All tests passed!");
	}
}
package com.lingpipe.cookbook.chapter2;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;

import com.aliasi.lm.NGramProcessLM;
import com.aliasi.spell.CompiledSpellChecker;
import com.aliasi.spell.TrainSpellChecker;
import com.aliasi.spell.WeightedEditDistance;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.Files;
import com.aliasi.util.Strings;

public class TokenizeWithoutWhiteSpaces {
	
	public static void main (String[] args) throws IOException, ClassNotFoundException {
		String dataPath = args.length > 0 ? args[0] : "data/connecticut_yankee_king_arthur.txt";
		int nGram = 5;
		NGramProcessLM lm = new NGramProcessLM(nGram);
		WeightedEditDistance spaceInsertingEditDistance 
			= CompiledSpellChecker.TOKENIZING;
		TrainSpellChecker trainer = new TrainSpellChecker(lm, spaceInsertingEditDistance);
		File trainingFile = new File(dataPath);
		String training = Files.readFromFile(trainingFile, Strings.UTF8);
		trainer.handle(training);
		System.out.println("Compiling Spell Checker");
		CompiledSpellChecker spellChecker
			= (CompiledSpellChecker) AbstractExternalizable.compile(trainer);
		spellChecker.setAllowInsert(true);
		spellChecker.setAllowMatch(true);
		spellChecker.setAllowDelete(false);
		spellChecker.setAllowSubstitute(false);
		spellChecker.setAllowTranspose(false);
		spellChecker.setNumConsecutiveInsertionsAllowed(1);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.println("type an Englese sentence (English without spaces like Chinese)");
			String input = reader.readLine();
			String result = spellChecker.didYouMean(input);
			System.out.println(result);
		}
	}
}
package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.List;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassification;
import com.aliasi.classify.ConditionalClassifierEvaluator;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.classify.ScoredClassification;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.ObjectToDoubleMap;
import com.lingpipe.cookbook.Util;


public class ActiveLearner {

	static String getLatestEpochFile(String fileName) throws FileNotFoundException {
		File file = new File(fileName);
		if (!file.exists()) {
			System.out.println("No file found, did you create your own directory and put in a <file name>.0.csv?");
			throw new FileNotFoundException();
		}
		File dir = file.getParentFile();
		Pattern rootAndEpochFinder = Pattern.compile("(.*)\\.(\\d+)\\.csv");
		Matcher matcher = rootAndEpochFinder.matcher(file.getAbsolutePath());
		matcher.find();
		String root = matcher.group(1);
		int maxAnnot = 0;
		for (File annotatedFile : dir.listFiles()) {
			matcher = rootAndEpochFinder.matcher(annotatedFile.getAbsolutePath());
			matcher.find();
			if (!matcher.group(1).equals(root)){
				continue;
			}
			int fileAnnotEpoch = Integer.valueOf(matcher.group(2));
			if (maxAnnot < fileAnnotEpoch) {
				maxAnnot = fileAnnotEpoch;
			}
		}	
		return root + "." + maxAnnot + ".csv";
	}

	static String incrementFileName(String fileName) {
		Pattern rootAndEpochFinder = Pattern.compile("(.*)\\.(\\d+)\\.csv");
		Matcher matcher = rootAndEpochFinder.matcher(fileName);
		matcher.find();
		String root = matcher.group(1);
		int nextIncrement = Integer.valueOf(matcher.group(2)) + 1;
		return root + "." + nextIncrement + ".csv";
	}

	public static void main(String[] args) throws IOException {
		String fileName = args.length > 0 ? args[0] : "data/activeLearningCompleted/disneySentimentDedupe.0.csv"; 
		System.out.println("First file:  " + fileName);
		String latestFile = getLatestEpochFile(fileName);
		System.out.println("Reading from file " + latestFile);
		List<String[]> data = Util.readCsvRemoveHeader(new File(latestFile));
		int numFolds = 10;
		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
		= Util.loadXValCorpus(data,numFolds);
		String[] categories = Util.getCategoryArray(corpus);
		PrintWriter progressWriter = new PrintWriter(System.out,true);
		boolean storeInputs = true;
		ConditionalClassifierEvaluator<CharSequence> evaluator 
		= new ConditionalClassifierEvaluator<CharSequence>(null, categories, storeInputs);
		TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		for (int i = 0; i < numFolds; ++i) {
			corpus.setFold(i);
			final LogisticRegressionClassifier<CharSequence> classifier 
			= Util.trainLogReg(corpus,tokFactory, progressWriter);
			evaluator.setClassifier(classifier);
			corpus.visitTest(evaluator);
		}
		final ObjectToDoubleMap<String[]> accumulator = new ObjectToDoubleMap<String[]>();
		for (String category : categories) {
			List<Classified<CharSequence>> inCategory = evaluator.truePositives(category);
			inCategory.addAll(evaluator.falseNegatives(category));
			for (Classified<CharSequence> testCase : inCategory) {
				CharSequence text = testCase.getObject();
				ConditionalClassification classification = (ConditionalClassification) testCase.getClassification();
				double score = classification.conditionalProbability(0);
				String[] xFoldRow = new String[Util.TEXT_OFFSET + 1];
				xFoldRow[Util.SCORE] = String.valueOf(score);
				xFoldRow[Util.GUESSED_CLASS] = classification.bestCategory();
				xFoldRow[Util.ANNOTATION_OFFSET] = category;
				xFoldRow[Util.TEXT_OFFSET] = text.toString();
				accumulator.set(xFoldRow,score);
			}
		}
		//Util.printPRcurve(evaluator);
		Util.printConfusionMatrix(evaluator.confusionMatrix());
		Util.printPrecRecall(evaluator);	
		corpus.setNumFolds(0);
		LogisticRegressionClassifier<CharSequence> classifier = Util.trainLogReg(corpus,tokFactory,progressWriter);
		for (String[] csvData : data) {
			if (!csvData[Util.ANNOTATION_OFFSET].equals("")) {
				continue;
			}
			ScoredClassification classification = classifier.classify(csvData[Util.TEXT_OFFSET]);
			csvData[Util.GUESSED_CLASS] = classification.category(0);
			double estimate = classification.score(0);
			csvData[Util.SCORE] = String.valueOf(estimate);
			accumulator.set(csvData,estimate);
		}
		String outfile = incrementFileName(latestFile);
		Util.writeCsvAddHeader(accumulator.keysOrderedByValueList(), new File(outfile));
		System.out.println("Corpus size: " + corpus.size());
		System.out.println("Writing to file: " + outfile);
		System.out.println("Done, now go annotate and save with same file name");
	}
}
package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.List;

import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassifierEvaluator;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.classify.ScoredPrecisionRecallEvaluation;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;
import com.aliasi.stats.AnnealingSchedule;
import com.aliasi.stats.RegressionPrior;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.NGramTokenizerFactory;
import com.aliasi.tokenizer.TokenFeatureExtractor;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.ObjectToDoubleMap;
import com.lingpipe.cookbook.Util;



public class BuildLogRegSys {
	
	public static void main(String[] args) throws IOException {
		String trainingFile = args.length > 0 ? args[0] : "data2/allDisneyDeduped.4.csv";
		int numFolds = 10;
		List<String[]> training 
			= Util.readAnnotatedCsvRemoveHeader(new File(trainingFile));
		String[] categories = Util.getCategories(training);
		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
			= Util.loadXValCorpus(training,numFolds);
		int min = 2;
		int max = 4;
		TokenizerFactory tokenizerFactory 
			//= new NGramTokenizerFactory(min,max);
			= IndoEuropeanTokenizerFactory.INSTANCE;
		FeatureExtractor<CharSequence> featureExtractor
			= new TokenFeatureExtractor(tokenizerFactory);
		int minFeatureCount = 2;
		boolean addInterceptFeature = true;
		boolean noninformativeIntercept = false;
		double priorVariance = 1;
		RegressionPrior prior 
			= RegressionPrior.laplace(priorVariance,noninformativeIntercept);
		//= RegressionPrior.gaussian(priorVariance,noninformativeIntercept);
		AnnealingSchedule annealingSchedule
			= AnnealingSchedule.exponential(0.00025,0.999);
		double minImprovement = 0.000000001;
		int minEpochs = 2;
		int maxEpochs = 10000;
		PrintWriter progressWriter = new PrintWriter(System.out,true);
		Reporter reporter = Reporters.writer(progressWriter);
		reporter.setLevel(LogLevel.INFO);
		int numThreads = 3;
		ConditionalClassifierEvaluator<CharSequence> evaluator = Util.xvalLogRegMultiThread(corpus,
				featureExtractor,
				minFeatureCount,
				addInterceptFeature,
				prior,
				annealingSchedule,
				minImprovement,
				minEpochs,
				maxEpochs,
				reporter,
				numFolds,
				numThreads,
				categories);
		
		reporter.setLevel(LogLevel.INFO);
		Util.printConfusionMatrix(evaluator.confusionMatrix());
		for (String category : categories) {
			Util.printFalsePositives(category, evaluator,corpus);
			ScoredPrecisionRecallEvaluation prEval = evaluator.scoredOneVersusAll(category);
			boolean interpolation = false;
			double[][] prScoreCurve = prEval.prScoreCurve(interpolation);
			reporter.report(LogLevel.INFO,"PR curve for: " + category );
			reporter.report(LogLevel.INFO,"Recall, Prec, Score");
			for (double[] row : prScoreCurve) {
				reporter.report(LogLevel.INFO,String.format("%.3f,%.3f,%.3f\n",row[0],row[1],row[2]));
			}
			//System.out.println(prEval.toString());	
		}
	/*	reporter.setLevel(LogLevel.WARN);
		corpus.setNumFolds(0);
		LogisticRegressionClassifier<CharSequence> classifier
			= LogisticRegressionClassifier.<CharSequence>train(corpus,
			featureExtractor,
			minFeatureCount,
			addInterceptFeature,
			prior,
			annealingSchedule,
			minImprovement,
			minEpochs,
			maxEpochs,
			reporter);
		int featureCount = 0;
		for (String category : categories) {
			ObjectToDoubleMap<String> featureCoeff = classifier.featureValues(category);
			System.out.println("Feature coefficients for category " + category);
			for (String feature : featureCoeff.keysOrderedByValueList()) {
				++featureCount;
				System.out.print(feature);
				System.out.printf(" : %.8f\n",featureCoeff.getValue(feature));
			}
		}
		System.out.println("Got feature count: " + featureCount);
		Util.consoleInputPrintClassification(classifier);
		*/
		
	}
}

package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.List;

import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassifierEvaluator;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.lingpipe.cookbook.Util;



public class ClassifierBuilder {

	public static void main(String args[]) throws IOException {
		String trainingFile = args.length > 0 ? args[0] : "data/activeLearningCompleted/disneySentimentDedupe.2.csv";
		int numFolds = 10;
		List<String[]> training 
		= Util.readAnnotatedCsvRemoveHeader(new File(trainingFile));
		String[] categories = Util.getCategories(training);
		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
		= Util.loadXValCorpus(training,numFolds);
		TokenizerFactory tokenizerFactory 
		= IndoEuropeanTokenizerFactory.INSTANCE;

		PrintWriter progressWriter = new PrintWriter(System.out,true);
		Reporter reporter = Reporters.writer(progressWriter);
		reporter.setLevel(LogLevel.WARN);
		boolean storeInputs = true;	
		ConditionalClassifierEvaluator<CharSequence> evaluator 
		= new ConditionalClassifierEvaluator<CharSequence>(null, categories, storeInputs);
		corpus.setNumFolds(numFolds);
		/*for (int i = 0; i < numFolds; ++i) {
			corpus.setFold(i);
			LogisticRegressionClassifier<CharSequence> classifier 
			= Util.trainLogReg(corpus, tokenizerFactory, progressWriter);
			evaluator.setClassifier(classifier);
			corpus.visitTest(evaluator);
		}
		*/
		
		corpus.setNumFolds(0);
		LogisticRegressionClassifier<CharSequence> classifier = Util.trainLogReg(corpus, tokenizerFactory, progressWriter);
		evaluator.setClassifier(classifier);
		corpus.visitTrain(evaluator);
		System.out.println("!!!Testing on training!!!");
		Util.printConfusionMatrix(evaluator.confusionMatrix());

		
		//AbstractExternalizable.compileTo(classifier, 
			//new File("models/ClassifierBuilder.LogisticRegression"));

		
	}
}


package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.List;

import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassifierEvaluator;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.lingpipe.cookbook.Util;



public class ClassifierBuilderFinal {

	public static void main(String args[]) throws IOException {
		String trainingFile = args.length > 0 ? args[0] : "data/activeLearningCompleted/disneySentimentDedupe.2.csv";
		int numFolds = 10;
		List<String[]> training 
		= Util.readAnnotatedCsvRemoveHeader(new File(trainingFile));
		String[] categories = Util.getCategories(training);
		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
		= Util.loadXValCorpus(training,numFolds);
		TokenizerFactory tokenizerFactory 
		= IndoEuropeanTokenizerFactory.INSTANCE;

		PrintWriter progressWriter = new PrintWriter(System.out,true);
		Reporter reporter = Reporters.writer(progressWriter);
		reporter.setLevel(LogLevel.WARN);
		boolean storeInputs = true;	
		ConditionalClassifierEvaluator<CharSequence> evaluator 
		= new ConditionalClassifierEvaluator<CharSequence>(null, categories, storeInputs);
		corpus.setNumFolds(numFolds);
		for (int i = 0; i < numFolds; ++i) {
			corpus.setFold(i);
			LogisticRegressionClassifier<CharSequence> classifier 
			= Util.trainLogReg(corpus, tokenizerFactory, progressWriter);
			evaluator.setClassifier(classifier);
			corpus.visitTest(evaluator);
		}
		
		
		/*corpus.setNumFolds(0);
		LogisticRegressionClassifier<CharSequence> classifier = Util.trainLogReg(corpus, tokenizerFactory, progressWriter);
		evaluator.setClassifier(classifier);
		corpus.visitTrain(evaluator);
		System.out.println("!!!Testing on training!!!");
		*/
		Util.printConfusionMatrix(evaluator.confusionMatrix());

		
		//AbstractExternalizable.compileTo(classifier, 
			//new File("models/ClassifierBuilder.LogisticRegression"));

		
	}
}


package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.List;

import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassifierEvaluator;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.lingpipe.cookbook.Util;



public class ClassifierBuilderStart {

	public static void main(String args[]) throws IOException {
		String trainingFile = args.length > 0 ? args[0] : "data/activeLearningCompleted/disneySentimentDedupe.2.csv";
		int numFolds = 10;
		List<String[]> training 
		= Util.readAnnotatedCsvRemoveHeader(new File(trainingFile));
		String[] categories = Util.getCategories(training);
		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
		= Util.loadXValCorpus(training,numFolds);
		TokenizerFactory tokenizerFactory 
		= IndoEuropeanTokenizerFactory.INSTANCE;

		PrintWriter progressWriter = new PrintWriter(System.out,true);
		Reporter reporter = Reporters.writer(progressWriter);
		reporter.setLevel(LogLevel.WARN);
		boolean storeInputs = true;	
		ConditionalClassifierEvaluator<CharSequence> evaluator 
		= new ConditionalClassifierEvaluator<CharSequence>(null, categories, storeInputs);
		corpus.setNumFolds(numFolds);
		/*for (int i = 0; i < numFolds; ++i) {
			corpus.setFold(i);
			LogisticRegressionClassifier<CharSequence> classifier 
			= Util.trainLogReg(corpus, tokenizerFactory, progressWriter);
			evaluator.setClassifier(classifier);
			corpus.visitTest(evaluator);
		}
		*/
		
		corpus.setNumFolds(0);
		LogisticRegressionClassifier<CharSequence> classifier = Util.trainLogReg(corpus, tokenizerFactory, progressWriter);
		evaluator.setClassifier(classifier);
		corpus.visitTrain(evaluator);
		System.out.println("!!!Testing on training!!!");
		Util.printConfusionMatrix(evaluator.confusionMatrix());

		
		//AbstractExternalizable.compileTo(classifier, 
			//new File("models/ClassifierBuilder.LogisticRegression"));

		
	}
}


package com.lingpipe.cookbook.chapter3;

import java.util.Map;

import com.aliasi.util.Counter;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.ObjectToCounterMap;


public class ContainsNumberFeatureExtractor implements FeatureExtractor<CharSequence> {

	@Override
	public Map<String,Counter> features(CharSequence text) {
		ObjectToCounterMap<String> featureMap = new ObjectToCounterMap<String>();
		if (text.toString().matches(".*\\d.*")) {
			featureMap.set("CONTAINS_NUMBER", 1);
		}
		return featureMap;
	}
	
	public static void main(String[] args) {
		FeatureExtractor<CharSequence> featureExtractor = new ContainsNumberFeatureExtractor();
		System.out.println(featureExtractor.features("I have a number 1"));
	}
}	

package com.lingpipe.cookbook.chapter3;

import java.util.Map;

import com.aliasi.features.AddFeatureExtractor;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.NGramTokenizerFactory;
import com.aliasi.tokenizer.RegExTokenizerFactory;
import com.aliasi.tokenizer.TokenFeatureExtractor;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.Counter;
import com.aliasi.util.FeatureExtractor;

public class CustomFeatureExtractor {
	
	public static void main(String[] args) {
		int min = 2;
		int max = 4;
		TokenizerFactory tokenizerFactory = new NGramTokenizerFactory(min,max);
		FeatureExtractor<CharSequence> tokenFeatures = new TokenFeatureExtractor(tokenizerFactory);
		FeatureExtractor<CharSequence> numberFeatures = new ContainsNumberFeatureExtractor();
		FeatureExtractor<CharSequence> joinedFeatureExtractors 
			= new AddFeatureExtractor<CharSequence>(tokenFeatures,numberFeatures);
		String input = "show me 1!";
		Map<String,? extends Number> features = joinedFeatureExtractors.features(input);
		System.out.println(features);
	}

}
package com.lingpipe.cookbook.chapter3;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.HashSet;
import java.util.Set;

import com.aliasi.spell.FixedWeightEditDistance;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.Files;
import com.aliasi.util.Strings;

public class EditDistance {

	public static void main(String[] args) throws IOException {
		//String text = Files.readFromFile(new File(args[0]), Strings.UTF8);
		String text = "Hello my name is Foobar";
		String[] tokens = IndoEuropeanTokenizerFactory.INSTANCE.tokenizer(text.toCharArray(), 0, text.length()).tokenize();
		Set<String> tokenSet  = new HashSet<String>();
		for (String token : tokens) {
			tokenSet.add(token);
		}
		double matchWeight = 0;
		double deleteWeight = -1;
		double insertWeight = -1;
		double substituteWeight = -1;
		double transposeWeight = -1;
		FixedWeightEditDistance editDistance 
			= new FixedWeightEditDistance(matchWeight,deleteWeight,insertWeight,substituteWeight,transposeWeight);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.println("enter a token to compare:");
			String inputToken = reader.readLine();
			for (String token : tokenSet) {
				double proximity = editDistance.proximity(token, inputToken);
				//if (proximity >= -1.0d) {
					System.out.printf("Proximity is %.2f between " + token + " and " + inputToken + "\n",proximity);
				//}
			}
		}
	}
}
package com.lingpipe.cookbook.chapter3;

public class GenericLM {

	/*
	 * Goal is to create proper language models 
	 * 1- Build a proper framework as hypothesis space--e.g. uniform models for chars, tokens, phrases, coref...
	 * 2- Inform models with example data to reduce hypothesis space
	 * 3- Handle unknown data well
	 */
	
	
}
package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.IOException;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import com.aliasi.classify.BaseClassifierEvaluator;
import com.aliasi.classify.Classification;
import com.aliasi.classify.PrecisionRecallEvaluation;
import com.lingpipe.cookbook.Util;

public class InterAnnotatorAgreement {

	public static void main(String[] args) throws IOException {
		String truthFile = args.length > 0 ? args[0] : "data/disney_e_n.csv";
		String responseFile = args.length > 1 ? args[1] : "data/disney1_e_n.csv";
		System.out.println(truthFile + " treated as truth \n" + responseFile + " treated as response");
		List<String[]> truth = Util.readAnnotatedCsvRemoveHeader(new File(truthFile));
		List<String[]> response = Util.readAnnotatedCsvRemoveHeader(new File(responseFile));
		Map<String,String[]> dataToTruth = new HashMap<String,String[]>();
		Set<String> categorySet = new HashSet<String>();
		for (String[] annot : truth) {
			dataToTruth.put(annot[Util.TEXT_OFFSET],annot);
			categorySet.add(annot[Util.ANNOTATION_OFFSET]);
		}
		for (String[] annot : response) {
			categorySet.add(annot[Util.ANNOTATION_OFFSET]);
		}
		String[] categories = categorySet.toArray(new String[0]);
		boolean storeInputs = false;
		BaseClassifierEvaluator<CharSequence> evaluator 
			= new BaseClassifierEvaluator<CharSequence>(null, categories, storeInputs);
		for (String[] responseRow : response) {
			String responseCategory = responseRow[Util.ANNOTATION_OFFSET];
			String text = responseRow[Util.TEXT_OFFSET];
			String[] truthRow = dataToTruth.get(text);
			if (truthRow == null) {
				System.out.println("no truth data for " + text);
				continue;
			}
			String truthCategory = truthRow[Util.ANNOTATION_OFFSET];
			Classification responseClassification = new Classification(responseCategory);
			if (!responseCategory.equals(truthCategory)) {
				System.out.print("Disagreement: " + truthCategory + " x " + responseCategory + " for: ");
				System.out.println(text);
			}
			evaluator.addClassification(truthCategory, responseClassification, text);
		}
		Util.printConfusionMatrix(evaluator.confusionMatrix());
		for (String category : categories) {
			PrecisionRecallEvaluation prEval = evaluator.oneVersusAll(category);
			System.out.printf("Category: " + category + " Precision: %.2f, Recall: %.2f \n",prEval.precision(),prEval.recall());
		}
	}
	
	
}
package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import au.com.bytecode.opencsv.CSVReader;

import com.aliasi.classify.BaseClassifier;
import com.aliasi.classify.Classification;
import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassifierEvaluator;
import com.aliasi.classify.ConfusionMatrix;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.classify.JointClassification;
import com.aliasi.classify.JointClassifier;
import com.aliasi.classify.JointClassifierEvaluator;
import com.aliasi.classify.LMClassifier;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.classify.PrecisionRecallEvaluation;
import com.aliasi.classify.ScoredClassification;
import com.aliasi.classify.ScoredClassifier;
import com.aliasi.classify.ScoredClassifierEvaluator;
import com.aliasi.classify.ScoredPrecisionRecallEvaluation;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;
import com.aliasi.lm.CompiledNGramBoundaryLM;
import com.aliasi.lm.NGramBoundaryLM;
import com.aliasi.stats.AnnealingSchedule;
import com.aliasi.stats.MultivariateDistribution;
import com.aliasi.stats.RegressionPrior;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.LowerCaseTokenizerFactory;
import com.aliasi.tokenizer.NGramTokenizerFactory;
import com.aliasi.tokenizer.TokenFeatureExtractor;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.ObjectToDoubleMap;
import com.aliasi.util.ScoredObject;
import com.lingpipe.cookbook.Util;

public class LinguisticTuning {

	

	static void featPrint(LogisticRegressionClassifier<CharSequence> lr) {
		for (String cat : lr.categorySymbols()) {
			ObjectToDoubleMap<String> feats = lr.featureValues(cat);
			String ZeroCategory = feats.size() > 0 ? "NON_ZERO " : "ZERO ";
			System.out.println("######################" + 
					"Printing features for category " + cat 
					+ " " + ZeroCategory);
			for (String feat : feats.keysOrderedByValueList()) {
				System.out.print(feat);
				System.out.printf(": %.2f\n",feats.getValue(feat));
			}
		}
	}

	public static void main(String[] args) throws IOException {
		String trainingFile = args.length > 0 ? args[0] 
					: "data/activeLearningCompleted/disneySentimentDedupe.2.csv";
		List<String[]> rows = Util.readAnnotatedCsvRemoveHeader(new File(trainingFile));
		int numFolds = 10;
		XValidatingObjectCorpus<Classified<CharSequence>> corpus =
			Util.loadXValCorpus(rows, numFolds);
		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
			//= new NGramTokenizerFactory(2,4);
			//tokenizerFactory = new LowerCaseTokenizerFactory(tokenizerFactory);
		
		FeatureExtractor<CharSequence> featureExtractor
		= new TokenFeatureExtractor(tokenizerFactory);
		int minFeatureCount = 1;
		boolean addInterceptFeature = true;
		boolean noninformativeIntercept = true;
		RegressionPrior prior = RegressionPrior.gaussian(1.0,noninformativeIntercept);
		AnnealingSchedule annealingSchedule
		= AnnealingSchedule.exponential(0.00025,0.999);
		double minImprovement = 0.000000001;
		int minEpochs = 100;
		int maxEpochs = 2000;
		PrintWriter progressWriter = new PrintWriter(System.out,true);
		progressWriter.println("Reading data.");
		Reporter reporter = Reporters.writer(progressWriter);
		reporter.setLevel(LogLevel.WARN);
		boolean storeInputs = true;
		String[] categories = Util.getCategories(rows);
		ScoredClassifierEvaluator<CharSequence> evaluator 
			= new ScoredClassifierEvaluator<CharSequence>(null, categories, storeInputs);
		for (int i = 0; i < numFolds; ++i) {
			corpus.setFold(i);
			System.out.println("Training on fold " + i);
			LogisticRegressionClassifier<CharSequence> classifier
				= LogisticRegressionClassifier.<CharSequence>train(corpus,
					featureExtractor,
					minFeatureCount,
					addInterceptFeature,
					prior,
					annealingSchedule,
					minImprovement,
					minEpochs,
					maxEpochs,
					reporter);
					
			featPrint(classifier);
			evaluator.setClassifier(classifier);		
			System.out.println("Testing on fold " + i);
			corpus.visitTest(evaluator);
		}
		
		Util.printConfusionMatrix(evaluator.confusionMatrix());
		Util.printPrecRecall(evaluator);
		Util.printFalsePositives("p", evaluator, corpus);
		
	}
}
package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import au.com.bytecode.opencsv.CSVReader;

import com.aliasi.classify.BaseClassifier;
import com.aliasi.classify.Classification;
import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassifierEvaluator;
import com.aliasi.classify.ConfusionMatrix;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.classify.JointClassification;
import com.aliasi.classify.JointClassifier;
import com.aliasi.classify.JointClassifierEvaluator;
import com.aliasi.classify.LMClassifier;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.classify.PrecisionRecallEvaluation;
import com.aliasi.classify.ScoredClassification;
import com.aliasi.classify.ScoredClassifier;
import com.aliasi.classify.ScoredClassifierEvaluator;
import com.aliasi.classify.ScoredPrecisionRecallEvaluation;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;
import com.aliasi.lm.CompiledNGramBoundaryLM;
import com.aliasi.lm.NGramBoundaryLM;
import com.aliasi.stats.AnnealingSchedule;
import com.aliasi.stats.MultivariateDistribution;
import com.aliasi.stats.RegressionPrior;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.LowerCaseTokenizerFactory;
import com.aliasi.tokenizer.NGramTokenizerFactory;
import com.aliasi.tokenizer.TokenFeatureExtractor;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.ObjectToDoubleMap;
import com.aliasi.util.ScoredObject;

public class LinguisticTuningLogisticRegression {

	static int ANNOTATION_OFFSET = 2;
	static int TEXT_OFFSET = 3;
	//static int NUM_FOLDS = 2;
	static int NUM_FOLDS = 10;

	static void featPrint(LogisticRegressionClassifier<CharSequence> lr) {
		for (String cat : lr.categorySymbols()) {
			ObjectToDoubleMap<String> feats = lr.featureValues(cat);
			String ZeroCategory = feats.size() > 0 ? "NON_ZERO " : "ZERO ";
			System.out.println("######################" + 
					"Printing features for category " + cat 
					+ " " + ZeroCategory);
			for (String feat : feats.keysOrderedByValueList()) {
				System.out.print(feat);
				System.out.printf(": %.2f\n",feats.getValue(feat));
			}
		}
	}

	public static void main(String[] args) throws IOException {
		String fileName = args[0];
		CSVReader csvReader = new CSVReader(new FileReader(fileName));
		csvReader.readNext();//skip headers
		List<String[]> annotatedData = csvReader.readAll();
		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
		= new XValidatingObjectCorpus<Classified<CharSequence>>(NUM_FOLDS);
		Set<String> categories = new HashSet<String>();
		for (String[] tweetData : annotatedData) {
			if (tweetData[ANNOTATION_OFFSET].equals("")) {
				continue;
			}
			Classification classification = new Classification(tweetData[ANNOTATION_OFFSET]);
			Classified<CharSequence> classified = new Classified<CharSequence>(tweetData[TEXT_OFFSET],classification);
			corpus.handle(classified);
			categories.add(tweetData[ANNOTATION_OFFSET]);
		}
		csvReader.close();
		TokenizerFactory tokenizerFactory 
			= new NGramTokenizerFactory(2,4);
			tokenizerFactory = new LowerCaseTokenizerFactory(tokenizerFactory);
			//= IndoEuropeanTokenizerFactory.INSTANCE;
		
			//= new NGramTokenizerFactory(2,5);
		
		FeatureExtractor<CharSequence> featureExtractor
		= new TokenFeatureExtractor(tokenizerFactory);
		int minFeatureCount = 2;
		boolean addInterceptFeature = true;
		boolean noninformativeIntercept = false;
		RegressionPrior prior = RegressionPrior.gaussian(1.0,noninformativeIntercept);
		AnnealingSchedule annealingSchedule
		= AnnealingSchedule.exponential(0.00025,0.999);
		double minImprovement = 0.000000001;
		int minEpochs = 100;
		int maxEpochs = 5000;

		int blockSize = corpus.size(); // reduces to conjugate gradient
		LogisticRegressionClassifier<CharSequence> hotStart = null;
		int rollingAvgSize = 10;
		ObjectHandler<LogisticRegressionClassifier<CharSequence>> classifierHandler
		= null;
		PrintWriter progressWriter = new PrintWriter(System.out,true);
		progressWriter.println("Reading data.");
		Reporter reporter = Reporters.writer(progressWriter);
		reporter.setLevel(LogLevel.WARN);
		boolean storeInputs = true;
		ScoredClassifierEvaluator<CharSequence> evaluator 
		= new ScoredClassifierEvaluator<CharSequence>(null, categories.toArray(new String[0]), storeInputs);
		for (int i = 0; i < NUM_FOLDS; ++i) {
			corpus.setFold(i);
			System.out.println("Training on fold " + i);
			LogisticRegressionClassifier<CharSequence> classifier
			= LogisticRegressionClassifier.<CharSequence>train(corpus,
					featureExtractor,
					minFeatureCount,
					addInterceptFeature,
					prior,
					blockSize,
					hotStart,
					annealingSchedule,
					minImprovement,
					rollingAvgSize,
					minEpochs,
					maxEpochs,
					classifierHandler,
					reporter);
			//featPrint(classifier);

			ScoredClassifier<CharSequence> thresholdedClassifier 
			= new ThresholdedClassifierComplete<CharSequence>(classifier);
			evaluator.setClassifier(thresholdedClassifier);		
			System.out.println("Testing on fold " + i);
			corpus.visitTest(evaluator);
			//System.out.println("!!!TESTING ON TRAINING!!!");
			//corpus.visitTrain(evaluator);
		}
		//System.out.println(evaluator);
		ConfusionMatrix confMatrix = evaluator.confusionMatrix();
		String[] labels = confMatrix.categories();
		int[][] outcomes = confMatrix.matrix();
		System.out.println("reference\\response");
		System.out.print("          \\");
		for (String category : labels) {
			System.out.print(category + ",");
		}
		for (int i = 0; i< outcomes.length; ++ i) {
			System.out.print("\n         " + labels[i] + " ");
			for (int j = 0; j < outcomes[i].length; ++j) {
				System.out.print(outcomes[i][j] + ",");
			}
		}
		
		System.out.println("");
		String category = "p";
		PrecisionRecallEvaluation prEval = evaluator.oneVersusAll(category);
		System.out.println("Category " + category);
		System.out.printf("Recall: %.2f\n", prEval.recall());
		System.out.printf("Prec  : %.2f\n", prEval.precision());

		category = "n";
		prEval = evaluator.oneVersusAll(category);
		System.out.println("Category " + category);
		System.out.printf("Recall: %.2f\n", prEval.recall());
		System.out.printf("Prec  : %.2f\n", prEval.precision());
		
		category = "p";
		ScoredPrecisionRecallEvaluation scoredPrEval = evaluator.scoredOneVersusAll(category);
		boolean interpolate = false;
		double[][] scoredCurve = scoredPrEval.prScoreCurve(interpolate);
		ScoredPrecisionRecallEvaluation.printScorePrecisionRecallCurve(scoredCurve,progressWriter);
		System.out.println(scoredPrEval);
		List<Classified<CharSequence>> falsePositives = evaluator.falsePositives(category);
		System.out.println("False Positives for " + category);
		for (Classified<CharSequence> classified: falsePositives) {
			System.out.println(classified.getClassification().bestCategory() + ": " + classified.getObject());
		}
		System.out.println("False Negatives for " + category);
		for (Classified<CharSequence> classified: evaluator.falseNegatives(category)) {
			System.out.println(classified.getClassification().bestCategory() + ": " + classified.getObject());
		}
	}
}
package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import au.com.bytecode.opencsv.CSVReader;

import com.aliasi.classify.BaseClassifier;
import com.aliasi.classify.Classification;
import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassifierEvaluator;
import com.aliasi.classify.ConfusionMatrix;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.classify.JointClassification;
import com.aliasi.classify.JointClassifier;
import com.aliasi.classify.JointClassifierEvaluator;
import com.aliasi.classify.LMClassifier;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.classify.PrecisionRecallEvaluation;
import com.aliasi.classify.ScoredClassification;
import com.aliasi.classify.ScoredClassifier;
import com.aliasi.classify.ScoredClassifierEvaluator;
import com.aliasi.classify.ScoredPrecisionRecallEvaluation;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;
import com.aliasi.lm.CompiledNGramBoundaryLM;
import com.aliasi.lm.NGramBoundaryLM;
import com.aliasi.stats.AnnealingSchedule;
import com.aliasi.stats.MultivariateDistribution;
import com.aliasi.stats.RegressionPrior;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.LowerCaseTokenizerFactory;
import com.aliasi.tokenizer.NGramTokenizerFactory;
import com.aliasi.tokenizer.TokenFeatureExtractor;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.ObjectToDoubleMap;
import com.aliasi.util.ScoredObject;

public class LinguisticTuningLogisticRegressionStart {

	static int ANNOTATION_OFFSET = 2;
	static int TEXT_OFFSET = 3;
	//static int NUM_FOLDS = 2;
	static int NUM_FOLDS = 10;

	static void featPrint(LogisticRegressionClassifier<CharSequence> lr) {
		for (String cat : lr.categorySymbols()) {
			ObjectToDoubleMap<String> feats = lr.featureValues(cat);
			String ZeroCategory = feats.size() > 0 ? "NON_ZERO " : "ZERO ";
			System.out.println("######################" + 
					"Printing features for category " + cat 
					+ " " + ZeroCategory);
			for (String feat : feats.keysOrderedByValueList()) {
				System.out.print(feat);
				System.out.printf(": %.2f\n",feats.getValue(feat));
			}
		}
	}

	public static void main(String[] args) throws IOException {
		String fileName = args[0];
		CSVReader csvReader = new CSVReader(new FileReader(fileName));
		csvReader.readNext();//skip headers
		List<String[]> annotatedData = csvReader.readAll();
		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
		= new XValidatingObjectCorpus<Classified<CharSequence>>(NUM_FOLDS);
		Set<String> categories = new HashSet<String>();
		for (String[] tweetData : annotatedData) {
			if (tweetData[ANNOTATION_OFFSET].equals("")) {
				continue;
			}
			Classification classification = new Classification(tweetData[ANNOTATION_OFFSET]);
			Classified<CharSequence> classified = new Classified<CharSequence>(tweetData[TEXT_OFFSET],classification);
			corpus.handle(classified);
			categories.add(tweetData[ANNOTATION_OFFSET]);
		}
		csvReader.close();
		TokenizerFactory tokenizerFactory 
			= IndoEuropeanTokenizerFactory.INSTANCE;
		//tokenizerFactory = new LowerCaseTokenizerFactory(tokenizerFactory);
			//= new NGramTokenizerFactory(2,5);
		//tokenizerFactory = new LowerCaseTokenizerFactory(tokenizerFactory);
		FeatureExtractor<CharSequence> featureExtractor
		= new TokenFeatureExtractor(tokenizerFactory);
		int minFeatureCount = 1;
		boolean addInterceptFeature = true;
		boolean noninformativeIntercept = false;
		RegressionPrior prior = RegressionPrior.gaussian(1.0,noninformativeIntercept);
		AnnealingSchedule annealingSchedule
		= AnnealingSchedule.exponential(0.00025,0.999);
		double minImprovement = 0.000000001;
		int minEpochs = 100;
		int maxEpochs = 2000;

		int blockSize = corpus.size(); // reduces to conjugate gradient
		LogisticRegressionClassifier<CharSequence> hotStart = null;
		int rollingAvgSize = 10;
		ObjectHandler<LogisticRegressionClassifier<CharSequence>> classifierHandler
		= null;
		PrintWriter progressWriter = new PrintWriter(System.out,true);
		progressWriter.println("Reading data.");
		Reporter reporter = Reporters.writer(progressWriter);
		reporter.setLevel(LogLevel.INFO);
		boolean storeInputs = false;
		ScoredClassifierEvaluator<CharSequence> evaluator 
		= new ScoredClassifierEvaluator<CharSequence>(null, categories.toArray(new String[0]), storeInputs);
		for (int i = 0; i < NUM_FOLDS; ++i) {
			corpus.setFold(i);
			System.out.println("Training on fold " + i);
			LogisticRegressionClassifier<CharSequence> classifier
			= LogisticRegressionClassifier.<CharSequence>train(corpus,
					featureExtractor,
					minFeatureCount,
					addInterceptFeature,
					prior,
					blockSize,
					hotStart,
					annealingSchedule,
					minImprovement,
					rollingAvgSize,
					minEpochs,
					maxEpochs,
					classifierHandler,
					reporter);
			featPrint(classifier);

			ScoredClassifier<CharSequence> thresholdedClassifier 
			= new ThresholdedClassifier<CharSequence>(classifier);
			evaluator.setClassifier(thresholdedClassifier);		
			System.out.println("Testing on fold " + i);
			corpus.visitTest(evaluator);
			//System.out.println("!!!TESTING ON TRAINING!!!");
			//corpus.visitTrain(evaluator);
		}
		//System.out.println(evaluator);
		ConfusionMatrix confMatrix = evaluator.confusionMatrix();
		String[] labels = confMatrix.categories();
		int[][] outcomes = confMatrix.matrix();
		System.out.println("reference\\response");
		System.out.print("          \\");
		for (String category : labels) {
			System.out.print(category + ",");
		}
		for (int i = 0; i< outcomes.length; ++ i) {
			System.out.print("\n         " + labels[i] + " ");
			for (int j = 0; j < outcomes[i].length; ++j) {
				System.out.print(outcomes[i][j] + ",");
			}
		}
		
		System.out.println("");
		String category = "n";
		PrecisionRecallEvaluation prEval = evaluator.oneVersusAll(category);
		System.out.println("Category " + category);
		System.out.printf("Recall: %.2f\n", prEval.recall());
		System.out.printf("Prec  : %.2f\n", prEval.precision());

		ScoredPrecisionRecallEvaluation scoredPrEval = evaluator.scoredOneVersusAll(category);
		boolean interpolate = false;
		double[][] scoredCurve = scoredPrEval.prScoreCurve(interpolate);
		ScoredPrecisionRecallEvaluation.printScorePrecisionRecallCurve(scoredCurve,progressWriter);
		System.out.println(scoredPrEval);	
	}
}
package com.lingpipe.cookbook.chapter3;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import com.aliasi.classify.BaseClassifier;
import com.aliasi.classify.Classification;
import com.aliasi.classify.Classified;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.lm.NGramBoundaryLM;
import com.lingpipe.cookbook.Util;

public class OverfittingClassifier implements BaseClassifier<CharSequence> {

	Map<String,Classification> mMap = new HashMap<String,Classification>();	

	@Override
	public Classification classify(CharSequence text) {
		if (mMap.containsKey(text)) {
			return mMap.get(text);
		}
		return new Classification("n");
	}
	
	public void handle(String text, Classification classification) {
		mMap.put(text, classification);
	}
	
	public static void main(String[] args) throws IOException {
		String dataPath = args.length > 0 ? args[0] : "data/disney_e_n.csv";
		List<String[]> annotatedData = Util.readAnnotatedCsvRemoveHeader(new File(dataPath));
		OverfittingClassifier classifier = new OverfittingClassifier();
		System.out.println("Training");
		for (String[] row: annotatedData) {
			String truth = row[Util.ANNOTATION_OFFSET];
			String text = row[Util.TEXT_OFFSET];
			classifier.handle(text,new Classification(truth));
		}
		Util.consoleInputBestCategory(classifier);
	}
}
package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.IOException;
import java.util.List;

import com.aliasi.classify.Classified;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.classify.ScoredClassification;
import com.aliasi.classify.ScoredClassifier;
import com.aliasi.classify.ScoredClassifierEvaluator;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.util.AbstractExternalizable;
import com.lingpipe.cookbook.Util;

public class RunClassifier {
	public static void main(String[] args) throws IOException, ClassNotFoundException {
		String filePath = args.length > 0 ? args[0] : "data/freshDisney.csv";
		String modelPath = args.length > 1 ? args[1] : "models/ClassifierBuilder.LogisticRegression";
		System.out.println("Data is: " + filePath + " model is: " + modelPath);
		
		@SuppressWarnings("unchecked")
		LogisticRegressionClassifier<CharSequence> classifier
			= (LogisticRegressionClassifier<CharSequence>) AbstractExternalizable.readObject(new File(modelPath));
	
		ScoredClassifier<CharSequence> thresholdedClassifier = new ThresholdedClassifier<CharSequence>(classifier);
		File annotsFile = new File(filePath);
		List<String[]> rows = Util.readCsvRemoveHeader(annotsFile);
		int numFolds = 0;
		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
			= Util.loadXValCorpus(rows, 0);
		boolean storeInputs = false;
		String[] categories = Util.getCategories(rows);
		if (categories.length < 2) {
			System.out.println("No annotations found, not evaluating");
		}
		else {
			ScoredClassifierEvaluator<CharSequence> evaluator 
				= new ScoredClassifierEvaluator<CharSequence>(classifier, categories, storeInputs);
			corpus.visitCorpus(evaluator);
			Util.printConfusionMatrix(evaluator.confusionMatrix());
			Util.printPrecRecall(evaluator);
			Util.printPRcurve(evaluator);
		}
		for (String[] row : rows) {
			ScoredClassification classification = classifier.classify(row[Util.TEXT_OFFSET]);
			row[Util.GUESSED_CLASS] = classification.bestCategory();
			row[Util.SCORE] = String.valueOf(classification.score(0));
		}
		System.out.println("writing scored output to " + annotsFile);
		Util.writeCsvAddHeader(rows, annotsFile);
	}
}
package com.lingpipe.cookbook.chapter3;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.List;

import com.aliasi.classify.ConditionalClassification;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.ObjectToDoubleMap;


public class RunLogisticRegression {

	public static void main (String[] args) throws IOException, ClassNotFoundException {
		String modelFile = args.length > 0 ? args[0] : "models/disney_e_n.LogisticRegression";
		@SuppressWarnings("unchecked")
		LogisticRegressionClassifier<CharSequence> classifier 
			= (LogisticRegressionClassifier<CharSequence>) AbstractExternalizable.readObject(new File(modelFile));

		FeatureExtractor<CharSequence> featureExtractor = classifier.featureExtractor();
		List<String> categories = classifier.categorySymbols();
		for (String category : categories) {
			ObjectToDoubleMap<String> featureCoeff = classifier.featureValues(category);
			System.out.println("Feature coefficients for category " + category);
			for (String feature : featureCoeff.keysOrderedByValueList()) {
				System.out.print(feature);
				System.out.printf(" : %.2f\n",featureCoeff.getValue(feature));
			}
		}
		
		BufferedReader reader = new BufferedReader(new 	InputStreamReader(System.in));
		while (true) {
			System.out.println("\nType a string to be classified");
			String data = reader.readLine();
			ConditionalClassification classification 
				= classifier.classify(data);
			System.out.println(classification);
		}
		
		
		
	}
	
}
package com.lingpipe.cookbook.chapter3;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.Map;

import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenFeatureExtractor;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.FeatureExtractor;

public class SimpleFeatureExtractor {

	public static void main(String[] args) throws IOException {
		TokenizerFactory tokFact = IndoEuropeanTokenizerFactory.INSTANCE;
		FeatureExtractor<CharSequence> tokenFeatureExtractor = new TokenFeatureExtractor(tokFact);
		BufferedReader reader = new BufferedReader(new 	InputStreamReader(System.in));
		while (true) {
			System.out.println("\nType a string to see its features");
			String text = reader.readLine();
			Map<String, ? extends Number > features = tokenFeatureExtractor.features(text);
			System.out.println(features);
		}
	}
	
}
package com.lingpipe.cookbook.chapter3;

import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.TreeSet;

import com.aliasi.classify.Classification;
import com.aliasi.classify.ConditionalClassification;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.classify.NaiveBayesClassifier;
import com.aliasi.classify.TradNaiveBayesClassifier;
import com.aliasi.lm.NGramBoundaryLM;
import com.aliasi.lm.TokenizedLM;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;

import au.com.bytecode.opencsv.CSVReader;


public class StoopidBreck {
	TokenizerFactory mTokFact;
	String[] mCategories;
	
	public StoopidBreck(String[] categories, TokenizerFactory tokFact) {
		mTokFact = tokFact;
		mCategories = categories;
	}

	public void train(String text, Classification classification ) {
		String[] tokens = mTokFact.tokenizer(text.toCharArray(), 0, text.length()).tokenize();
		String bestCategory = classification.bestCategory();
		
		
	}

	public ConditionalClassification classify(String data) {
		// TODO Auto-generated method stub
		return null;
	}
	
}
package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

import com.aliasi.classify.Classification;
import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassification;
import com.aliasi.classify.ConditionalClassifier;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.classify.ScoredClassification;
import com.aliasi.classify.ScoredClassifier;
import com.aliasi.classify.ScoredClassifierEvaluator;
import com.aliasi.classify.ScoredPrecisionRecallEvaluation;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.ObjectToDoubleMap;
import com.aliasi.util.ScoredObject;
import com.lingpipe.cookbook.Util;

public class ThresholdedClassifier<E> implements ScoredClassifier<E> {
	ConditionalClassifier<E> mNonThresholdedClassifier;

	public ThresholdedClassifier (ConditionalClassifier<E> classifier) {
		mNonThresholdedClassifier = classifier;
	}

	@Override
	public ScoredClassification classify(E input) {
		ConditionalClassification classification 
		= mNonThresholdedClassifier.classify(input);
		List<ScoredObject<String>> scores = new ArrayList<ScoredObject<String>>();
		for (int i = 0; i < classification.size(); ++i) {
			String category = classification.category(i);
			Double score = classification.score(i);
			if (category.equals("p") && score < .76d) {
				score = 0.0;
			}
			if (category.equals("n") && score < .549d) {
				score = 0.0;
			}
			ScoredObject<String> scored 
			= new ScoredObject<String>(category,score);
			scores.add(scored);
		}
		ScoredClassification thresholded = ScoredClassification.create(scores);
		return thresholded;
	}

	public static void main(String[] args) throws IOException, ClassNotFoundException {
		String filePath = args.length > 0 ? args[0] : "data/freshDisneyAnnotated.csv";
		String modelPath = args.length > 1 ? args[1] : "models/ClassifierBuilder.LogisticRegression";
		System.out.println("Data is: " + filePath + " model is: " + modelPath);

		@SuppressWarnings("unchecked")
		LogisticRegressionClassifier<CharSequence> baseClassifier
		= (LogisticRegressionClassifier<CharSequence>) AbstractExternalizable.readObject(new File(modelPath));

		ScoredClassifier<CharSequence> classifier = new ThresholdedClassifier<CharSequence>(baseClassifier);
		File annotsFile = new File(filePath);
		List<String[]> rows = Util.readCsvRemoveHeader(annotsFile);
		int numFolds = 0;
		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
		= Util.loadXValCorpus(rows, numFolds);
		boolean storeInputs = false;
		String[] categories = Util.getCategories(rows);

		ScoredClassifierEvaluator<CharSequence> evaluator 
		= new ScoredClassifierEvaluator<CharSequence>(classifier, categories, storeInputs);
		corpus.visitCorpus(evaluator);
		Util.printConfusionMatrix(evaluator.confusionMatrix());
		Util.printPrecRecall(evaluator);
		Util.printPRcurve(evaluator);
	}


}package com.lingpipe.cookbook.chapter3;

import java.util.ArrayList;
import java.util.List;

import com.aliasi.classify.ScoredClassification;
import com.aliasi.classify.ScoredClassifier;
import com.aliasi.util.ScoredObject;

public class ThresholdedClassifierComplete<E> implements ScoredClassifier<E> {
	ScoredClassifier<CharSequence> mNonThresholdedClassifier;

	public ThresholdedClassifierComplete (ScoredClassifier<CharSequence> classifier) {
		mNonThresholdedClassifier = classifier;
	}

	@Override
	public ScoredClassification classify(E input) {
		ScoredClassification classification 
			= mNonThresholdedClassifier.classify((CharSequence) input);
		List<ScoredObject<String>> scores = new ArrayList<ScoredObject<String>>();
		for (int i = 0; i < classification.size(); ++i) {
			String category = classification.category(i);
			Double score = classification.score(i);
			if (category.equals("p") && score < .94d) {
			//if (category.equals("p") && score < .69d) {
				score = 0.0;
			}
			if (category.equals("n") && score < .4d) {
				score = 0.0;
			}
			ScoredObject<String> scored 
				= new ScoredObject<String>(category,score);
			scores.add(scored);
		}
		classification = ScoredClassification.create(scores);
		return classification;
	}
}package com.lingpipe.cookbook.chapter3;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.List;
import java.util.SortedSet;

import au.com.bytecode.opencsv.CSVReader;

import com.aliasi.classify.LMClassifier;
import com.aliasi.lm.CompiledNGramBoundaryLM;
import com.aliasi.lm.LanguageModel;
import com.aliasi.lm.LanguageModel.Sequence;
import com.aliasi.lm.NGramBoundaryLM;
import com.aliasi.lm.NGramProcessLM;
import com.aliasi.lm.TokenizedLM;
import com.aliasi.lm.UniformBoundaryLM;
import com.aliasi.stats.MultivariateDistribution;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.ObjectToCounterMap;
import com.aliasi.util.ScoredObject;

public class TokenizedLMExps {

	static int TEXT_INDEX = 3;

	static void dumpProbs(String[] tokens, TokenizedLM lm) {
        System.out.println("TOKENS: " + java.util.Arrays.asList(tokens));
        System.out.println("lm.tokenProbability(): "
                           + lm.tokenProbability(tokens,0,tokens.length));
       /* System.out.println("lm.tokenProbCharSmooth(): "
                           + lm.tokenProbCharSmooth(tokens,0,tokens.length));
        System.out.println("lm.tokenProbCharSmoothNoBound(): "
                           + lm.tokenProbCharSmoothNoBounds(tokens,0,tokens.length));
                           */
        System.out.println("lm.processLog2Probability(): "
                + lm.processLog2Probability(tokens));
        System.out.println();
    }
	
	public static void testBackoff() {
        TokenizerFactory tokenizerFactory 
        = IndoEuropeanTokenizerFactory.INSTANCE;
    int nGramOrder = 3;
    NGramBoundaryLM unknownTokenModel = new NGramBoundaryLM(3);
    NGramBoundaryLM  whitespaceModel = new NGramBoundaryLM(3);
    double lambdaFactor = 1.0;
    TokenizedLM lm 
        = new TokenizedLM(tokenizerFactory, 
                          nGramOrder,
                          unknownTokenModel, whitespaceModel, 
                          lambdaFactor);
    TokenizedLM lm2 
    	= new TokenizedLM(tokenizerFactory, nGramOrder);
    String training = "ab abc ab bd";
    lm.train(training);
    lm2.train(training);
    dumpProbs(new String[] {"b"},lm);
    dumpProbs(new String[] {"b"},lm2);
    }
	
	public static void main(String[] args) throws IOException {
		CSVReader csvReader = null;
		List<String[]> lines = null;
		csvReader = new CSVReader(new FileReader(args[0]));		
		lines = csvReader.readAll();
		int nGramOrder = 1;
		UniformBoundaryLM uniLM = new UniformBoundaryLM(0);
		
		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		
		NGramBoundaryLM unknownWhiteSpaceModel = new NGramBoundaryLM(3);
		NGramBoundaryLM unknownTokenModel = new NGramBoundaryLM(3);
		TokenizedLM bgs 
			= new TokenizedLM(tokenizerFactory,nGramOrder);
		TokenizedLM backgroundLanguageModel 
		    = new TokenizedLM(tokenizerFactory, nGramOrder, 
		    		unknownTokenModel, unknownWhiteSpaceModel, 1.0);
		
		 NGramBoundaryLM unknownTokenModel2 = new NGramBoundaryLM(3);
		    NGramBoundaryLM  whitespaceModel = new NGramBoundaryLM(3);
		    double lambdaFactor = 1.0;
		    TokenizedLM lm 
		        = new TokenizedLM(tokenizerFactory, 
		                          nGramOrder,
		                          unknownTokenModel2, whitespaceModel, 
		                          lambdaFactor);
		
		String training = "ab abc ab bd";
	    lm.train(training);
		for (String [] line: lines) {
			//backgroundLanguageModel.train(line[TEXT_INDEX]);
			//bgs.train(line[TEXT_INDEX]);
		}
		backgroundLanguageModel.train("ab abc ab bd");
		bgs.train("ab abc ab bd");
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			StringBuilder sb = new StringBuilder();
			System.out.print("Enter Phrase to score:");
			String string = reader.readLine();
			String[] tokens = tokenizerFactory.tokenizer(string.toCharArray(), 0, string.length()).tokenize();
			double bgToks = backgroundLanguageModel.tokenProbability(tokens, 0, tokens.length);
			double bgScore = backgroundLanguageModel.log2Estimate(string);
			double bgSimpleToks = bgs.tokenProbability(tokens,0,tokens.length);
			double bgSimpleScore = bgs.log2Estimate(string);
			sb.append("Complex:" + bgScore + " " + bgToks);
			sb.append("Simple: " + bgSimpleScore + " " + bgSimpleToks);
			//sb.append("Uniform:" + uniLM.log2Estimate(string) + " ");
			System.out.println(sb);
			//dumpProbs(new String[] {"b"},bgs);
			//dumpProbs(new String[] {"b"},backgroundLanguageModel);
			testBackoff();
		}
		
		
		
	}
	
}


package com.lingpipe.cookbook.chapter3;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.PrintWriter;
import java.util.Arrays;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import com.aliasi.classify.Classification;
import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassification;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.classify.NaiveBayesClassifier;
import com.aliasi.classify.TradNaiveBayesClassifier;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;
import com.aliasi.lm.NGramBoundaryLM;
import com.aliasi.lm.TokenizedLM;
import com.aliasi.stats.AnnealingSchedule;
import com.aliasi.stats.RegressionPrior;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenFeatureExtractor;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.ObjectToDoubleMap;
import com.lingpipe.cookbook.Util;

import au.com.bytecode.opencsv.CSVReader;


public class TrainAndRunLogReg {
	
	public static void main(String[] args) throws IOException {
		String trainingFile = args.length > 0 ? args[0] : "data/disney_e_n.csv";
		String modelFile = args.length > 1 ? args[1] : "models/disney_e_n.LogisticRegression";
		int numFolds = 0;
		List<String[]> training 
			= Util.readAnnotatedCsvRemoveHeader(new File(trainingFile));
		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
			= Util.loadXValCorpus(training,numFolds);
		
		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		FeatureExtractor<CharSequence> featureExtractor
			= new TokenFeatureExtractor(tokenizerFactory);
		int minFeatureCount = 1;
		boolean addInterceptFeature = false;
		boolean noninformativeIntercept = true;
		double priorVariance = 2 ;
		RegressionPrior prior = RegressionPrior.laplace(priorVariance,noninformativeIntercept);
		AnnealingSchedule annealingSchedule
			= AnnealingSchedule.exponential(0.00025,0.999);
		double minImprovement = 0.000000001;
		int minEpochs = 100;
		int maxEpochs = 2000;
		PrintWriter progressWriter = new PrintWriter(System.out,true);
		progressWriter.println("Reading data.");
		Reporter reporter = Reporters.writer(progressWriter);
		reporter.setLevel(LogLevel.INFO);

		LogisticRegressionClassifier<CharSequence> classifier
		= LogisticRegressionClassifier.<CharSequence>train(corpus,
				featureExtractor,
				minFeatureCount,
				addInterceptFeature,
				prior,
				annealingSchedule,
				minImprovement,
				minEpochs,
				maxEpochs,
				reporter);
	
		AbstractExternalizable.compileTo(classifier, new File("models/myModel.LogisticRegression"));	
		Util.consoleInputPrintClassification(classifier);
	}
	
}

package com.lingpipe.cookbook.chapter3;

import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.TreeSet;

import com.aliasi.classify.Classification;
import com.aliasi.classify.ConditionalClassification;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.classify.NaiveBayesClassifier;
import com.aliasi.classify.TradNaiveBayesClassifier;
import com.aliasi.lm.NGramBoundaryLM;
import com.aliasi.lm.TokenizedLM;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.lingpipe.cookbook.Util;

import au.com.bytecode.opencsv.CSVReader;


public class TrainAndRunNaiveBayesClassifier {
	static int ANNOTATION_OFFSET = 2;
	static int TEXT_OFFSET = 3;

	public static void main(String[] args) throws IOException {
		String inFile = args.length > 0 ? args[0] : "data/hotcold.csv";
		CSVReader csvReader = new CSVReader(new FileReader(inFile));
		List<String[]> annotatedData = csvReader.readAll();
		Set<String> categories = new TreeSet<String>();
		for (String[] tweetData : annotatedData) {
			if (!tweetData[ANNOTATION_OFFSET].equals("")) {
				categories.add(tweetData[ANNOTATION_OFFSET]);
			}
		}
		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		String[] categoriesList = categories.toArray(new String[0]);
		double categoryPrior = 1.0;
		double tokenInCategoryPrior = 1.0;
		double lengthNorm = Double.NaN;
		TradNaiveBayesClassifier classifier 
			= new TradNaiveBayesClassifier(categories,tokenizerFactory,categoryPrior,tokenInCategoryPrior,lengthNorm);
		for (String[] tweetData : annotatedData) {
			if (!tweetData[ANNOTATION_OFFSET].equals("")) {
				int count = 1;
				classifier.train(tweetData[TEXT_OFFSET],
						new Classification(tweetData[ANNOTATION_OFFSET]),  			
						count);
			}
		}

		System.out.println(classifier);
		BufferedReader reader = new BufferedReader(new 	InputStreamReader(System.in));
		while (true) {
			System.out.println("\nType a string to be classified");
			String data = reader.readLine();
			ConditionalClassification classification 
			= classifier.classify(data);
			for (int i = 0; i < categoriesList.length; ++i) {
				System.out.format(classification.category(i) 
						+ " %.2f 	%n",classification.conditionalProbability(i));
			}
		}
	}

}

package com.lingpipe.cookbook.chapter3;

import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.TreeSet;

import com.aliasi.classify.Classification;
import com.aliasi.classify.ConditionalClassification;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.classify.NaiveBayesClassifier;
import com.aliasi.classify.TradNaiveBayesClassifier;
import com.aliasi.lm.NGramBoundaryLM;
import com.aliasi.lm.TokenizedLM;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;

import au.com.bytecode.opencsv.CSVReader;


public class TrainAndRunStoopidBreck {
	static int ANNOTATION_OFFSET = 2;
	static int TEXT_OFFSET = 3;

	public static void main(String[] args) throws IOException {
		String inFile = args[0];
		CSVReader csvReader = new CSVReader(new FileReader(inFile));
		List<String[]> annotatedData = csvReader.readAll();
		Set<String> categories = new TreeSet<String>();
		for (String[] tweetData : annotatedData) {
			if (!tweetData[ANNOTATION_OFFSET].equals("")) {
				categories.add(tweetData[ANNOTATION_OFFSET]);
			}
		}
		int maxTokenNGram = 2;
		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		String[] categoriesList = categories.toArray(new String[0]);
		StoopidBreck classifier 
			= new StoopidBreck(categoriesList,tokenizerFactory);
		for (String[] tweetData : annotatedData) {
			if (!tweetData[ANNOTATION_OFFSET].equals("")) {
				int count = 1;
				classifier.train(tweetData[TEXT_OFFSET],
						new Classification(tweetData[ANNOTATION_OFFSET]));
			}
		}
		
		System.out.println(classifier);
		BufferedReader reader = new BufferedReader(new 	InputStreamReader(System.in));
		while (true) {
			System.out.println("\nType a string to be classified");
			try {
				String data = reader.readLine();
				ConditionalClassification classification 
				= classifier.classify(data);
				for (int i = 0; i < categoriesList.length; ++i) {
					System.out.format(classification.category(i) 
							+ " %.2f 	%n",classification.conditionalProbability(i));
				}
			} catch (IOException e) {
				e.printStackTrace();
			}
		}
	}

}


package com.lingpipe.cookbook.chapter3;

import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import com.aliasi.classify.ConditionalClassification;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.lm.NGramBoundaryLM;
import com.aliasi.lm.TokenizedLM;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;

import au.com.bytecode.opencsv.CSVReader;


public class TrainAndRunTokenizedLMClassifier {
	static int ANNOTATION_OFFSET = 2;
	static int TEXT_OFFSET = 3;

	public static void main(String[] args) throws IOException {
		String inFile = args[0];
		CSVReader csvReader = new CSVReader(new FileReader(inFile));
		List<String[]> annotatedData = csvReader.readAll();
		Set<String> categories = new HashSet<String>();
		for (String[] tweetData : annotatedData) {
			if (!tweetData[ANNOTATION_OFFSET].equals("")) {
				categories.add(tweetData[ANNOTATION_OFFSET]);
			}
		}
		int maxTokenNGram = 2;
		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		String[] categoriesList = categories.toArray(new String[0]);
		DynamicLMClassifier<TokenizedLM> classifier 
		= DynamicLMClassifier.createTokenized(categoriesList,tokenizerFactory,maxTokenNGram);
		for (String[] tweetData : annotatedData) {
			if (!tweetData[ANNOTATION_OFFSET].equals("")) {
				int count = 1;
				classifier.train(tweetData[ANNOTATION_OFFSET],  			
						tweetData[TEXT_OFFSET], count);
			}
		}
		BufferedReader reader = new BufferedReader(new 	InputStreamReader(System.in));
		while (true) {
			System.out.println("\nType a string to be classified");
			try {
				String data = reader.readLine();
				ConditionalClassification classification 
				= classifier.classify(data);
				for (int i = 0; i < categoriesList.length; ++i) {
					System.out.format(classification.category(i) 
							+ " %.2f 	%n",classification.conditionalProbability(i));
				}
			} catch (IOException e) {
				e.printStackTrace();
			}
		}
	}

}

package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.List;

import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassifierEvaluator;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;
import com.aliasi.stats.AnnealingSchedule;
import com.aliasi.stats.RegressionPrior;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenFeatureExtractor;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.ObjectToDoubleMap;
import com.lingpipe.cookbook.Util;



public class TuneLogRegParams {
	
	public static void main(String[] args) throws IOException {
		String trainingFile = args.length > 0 ? args[0] : "data/disney_e_n.csv";
		int numFolds = 10;
		List<String[]> training 
			= Util.readAnnotatedCsvRemoveHeader(new File(trainingFile));
		String[] categories = Util.getCategories(training);
		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
			= Util.loadXValCorpus(training,numFolds);
		int min = 2;
		int max = 4;
		TokenizerFactory tokenizerFactory //= new NGramTokenizerFactory(min,max);
			= IndoEuropeanTokenizerFactory.INSTANCE;
		FeatureExtractor<CharSequence> featureExtractor
			= new TokenFeatureExtractor(tokenizerFactory);
		int minFeatureCount = 1;
		boolean addInterceptFeature = true;
		boolean noninformativeIntercept = false;
		double priorVariance = 2;
		RegressionPrior prior 
			//= RegressionPrior.laplace(priorVariance,noninformativeIntercept);
		= RegressionPrior.gaussian(priorVariance,noninformativeIntercept);
		AnnealingSchedule annealingSchedule
			= AnnealingSchedule.exponential(0.00025,0.999);
		double minImprovement = 0.000000001;
		int minEpochs = 10;
		int maxEpochs = 20;
		PrintWriter progressWriter = new PrintWriter(System.out,true);
		Reporter reporter = Reporters.writer(progressWriter);
		reporter.setLevel(LogLevel.WARN);
		int numThreads = 2;
		ConditionalClassifierEvaluator<CharSequence> eval = Util.xvalLogRegMultiThread(corpus,
				featureExtractor,
				minFeatureCount,
				addInterceptFeature,
				prior,
				annealingSchedule,
				minImprovement,
				minEpochs,
				maxEpochs,
				reporter,
				numFolds,
				numThreads,
				categories);
		Util.printPRcurve(eval);
		Util.printConfusionMatrix(eval.confusionMatrix());
		corpus.setNumFolds(0);
		LogisticRegressionClassifier<CharSequence> classifier
			= LogisticRegressionClassifier.<CharSequence>train(corpus,
			featureExtractor,
			minFeatureCount,
			addInterceptFeature,
			prior,
			annealingSchedule,
			minImprovement,
			minEpochs,
			maxEpochs,
			reporter);
		int featureCount = 0;
		for (String category : categories) {
			ObjectToDoubleMap<String> featureCoeff = classifier.featureValues(category);
			System.out.println("Feature coefficients for category " + category);
			for (String feature : featureCoeff.keysOrderedByValueList()) {
				++featureCount;
				System.out.print(feature);
				System.out.printf(" : %.8f\n",featureCoeff.getValue(feature));
			}
		}
		System.out.println("Got feature count: " + featureCount);
		Util.consoleInputPrintClassification(classifier);
	}
}

package com.lingpipe.cookbook.chapter3;

import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import au.com.bytecode.opencsv.CSVReader;

import com.aliasi.classify.BaseClassifier;
import com.aliasi.classify.Classification;
import com.aliasi.classify.Classified;
import com.aliasi.classify.ConditionalClassifierEvaluator;
import com.aliasi.classify.ConfusionMatrix;
import com.aliasi.classify.DynamicLMClassifier;
import com.aliasi.classify.JointClassification;
import com.aliasi.classify.JointClassifier;
import com.aliasi.classify.JointClassifierEvaluator;
import com.aliasi.classify.LMClassifier;
import com.aliasi.classify.LogisticRegressionClassifier;
import com.aliasi.classify.PrecisionRecallEvaluation;
import com.aliasi.classify.ScoredClassification;
import com.aliasi.classify.ScoredClassifier;
import com.aliasi.classify.ScoredClassifierEvaluator;
import com.aliasi.classify.ScoredPrecisionRecallEvaluation;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;
import com.aliasi.lm.CompiledNGramBoundaryLM;
import com.aliasi.lm.NGramBoundaryLM;
import com.aliasi.stats.AnnealingSchedule;
import com.aliasi.stats.MultivariateDistribution;
import com.aliasi.stats.RegressionPrior;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.NGramTokenizerFactory;
import com.aliasi.tokenizer.TokenFeatureExtractor;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.ObjectToDoubleMap;
import com.aliasi.util.ScoredObject;

public class TuneLogisticRegression {

	static int ANNOTATION_OFFSET = 2;
	static int TEXT_OFFSET = 3;
	//static int NUM_FOLDS = 2;
	static int NUM_FOLDS = 10;
	public static void main(String[] args) throws IOException {
		String fileName = args.length > 0 ? args[0] : "data2/allDisneyDeduped.5.csv";
		CSVReader csvReader = new CSVReader(new FileReader(fileName));
		csvReader.readNext();//skip headers
		List<String[]> annotatedData = csvReader.readAll();
		XValidatingObjectCorpus<Classified<CharSequence>> corpus 
			= new XValidatingObjectCorpus<Classified<CharSequence>>(NUM_FOLDS);
		Set<String> categories = new HashSet<String>();
		for (String[] tweetData : annotatedData) {
			if (tweetData[ANNOTATION_OFFSET].equals("")) {
				continue;
			}
			Classification classification = new Classification(tweetData[ANNOTATION_OFFSET]);
			Classified<CharSequence> classified = new Classified<CharSequence>(tweetData[TEXT_OFFSET],classification);
			corpus.handle(classified);
			categories.add(tweetData[ANNOTATION_OFFSET]);
		}
		csvReader.close();
		TokenizerFactory tokenizerFactory = new NGramTokenizerFactory(2, 4);
			//= IndoEuropeanTokenizerFactory.INSTANCE;
		FeatureExtractor<CharSequence> featureExtractor
		= new TokenFeatureExtractor(tokenizerFactory);
		int minFeatureCount = 1;
		boolean addInterceptFeature = true;
		boolean noninformativeIntercept = true;
		RegressionPrior prior = RegressionPrior.gaussian(1.0,noninformativeIntercept);
		AnnealingSchedule annealingSchedule
		= AnnealingSchedule.exponential(0.00025,0.999);
		double minImprovement = 0.000000001;
		int minEpochs = 100;
		int maxEpochs = 10000;

		int blockSize = corpus.size(); 
		LogisticRegressionClassifier<CharSequence> hotStart = null;
		int rollingAvgSize = 10;
		ObjectHandler<LogisticRegressionClassifier<CharSequence>> classifierHandler
		= null;
		PrintWriter progressWriter = new PrintWriter(System.out,true);
		progressWriter.println("Reading data.");
		Reporter reporter = Reporters.writer(progressWriter);
		reporter.setLevel(LogLevel.INFO);
		boolean storeInputs = false;
		ScoredClassifierEvaluator<CharSequence> evaluator 
			= new ScoredClassifierEvaluator<CharSequence>(null, categories.toArray(new String[0]), storeInputs);
		for (int i = 0; i < NUM_FOLDS; ++i) {
			corpus.setFold(i);
			System.out.println("Training on fold " + i);
			LogisticRegressionClassifier<CharSequence> classifier
			= LogisticRegressionClassifier.<CharSequence>train(corpus,
					featureExtractor,
					minFeatureCount,
					addInterceptFeature,
					prior,
					blockSize,
					hotStart,
					annealingSchedule,
					minImprovement,
					rollingAvgSize,
					minEpochs,
					maxEpochs,
					classifierHandler,
					reporter);
			ScoredClassifier<CharSequence> thresholdedClassifier 
				= new ThresholdedClassifierComplete<CharSequence>(classifier);
			evaluator.setClassifier(classifier);
			System.out.println("Testing on fold " + i);
			corpus.visitTest(evaluator);
			//System.out.println("!!!TESTING ON TRAINING!!!");
			//corpus.visitTrain(evaluator);
		}
		//System.out.println(evaluator);
		ConfusionMatrix confMatrix = evaluator.confusionMatrix();
		String[] labels = confMatrix.categories();
		int[][] outcomes = confMatrix.matrix();
		System.out.println("reference\\response");
		System.out.print("          \\");
		for (String category : labels) {
			System.out.print(category + ",");
		}
		for (int i = 0; i< outcomes.length; ++ i) {
			System.out.print("\n         " + labels[i] + " ");
			for (int j = 0; j < outcomes[i].length; ++j) {
				System.out.print(outcomes[i][j] + ",");
			}
		}
		System.out.println("");
		String category = "n";
		//String category = "p";
		System.out.println("Corpus size is: " + corpus.size());
		PrecisionRecallEvaluation prEval = evaluator.oneVersusAll(category);
		System.out.println("Category " + category);
		System.out.printf("Recall: %.2f\n", prEval.recall());
		System.out.printf("Prec  : %.2f\n", prEval.precision());

		ScoredPrecisionRecallEvaluation scoredPrEval = evaluator.scoredOneVersusAll(category);
		boolean interpolate = false;
		double[][] scoredCurve = scoredPrEval.prScoreCurve(interpolate);
		ScoredPrecisionRecallEvaluation.printScorePrecisionRecallCurve(scoredCurve,progressWriter);
		System.out.println(scoredPrEval);
		
	}
}
package com.lingpipe.cookbook.chapter4;


import com.aliasi.corpus.Corpus;

import com.aliasi.corpus.ObjectHandler;

import com.aliasi.crf.ChainCrf;
import com.aliasi.crf.ChainCrfFeatureExtractor;

import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;


import com.aliasi.stats.AnnealingSchedule;
import com.aliasi.stats.RegressionPrior;


import com.aliasi.tag.Tagging;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;



import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;




import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;


public class CRFTagger {

	public static void main(String[] args) throws IOException {
		Corpus<ObjectHandler<Tagging<String>>> corpus = new TinyPosCorpus();
		final ChainCrfFeatureExtractor<String> featureExtractor
			= new SimpleCrfFeatureExtractor();
			//= new ModifiedCrfFeatureExtractor();
		boolean addIntercept = true;
		int minFeatureCount = 1;
		boolean cacheFeatures = false;
		boolean allowUnseenTransitions = true;
		double priorVariance = 4.0;
		boolean uninformativeIntercept = true;
		RegressionPrior prior
			= RegressionPrior.gaussian(priorVariance,
				uninformativeIntercept);
		int priorBlockSize = 3;
		double initialLearningRate = 0.05;
		double learningRateDecay = 0.995;
		AnnealingSchedule annealingSchedule
			= AnnealingSchedule.exponential(initialLearningRate,
				learningRateDecay);
		double minImprovement = 0.00001;
		int minEpochs = 2;
		int maxEpochs = 2000;
		Reporter reporter
			= Reporters.stdOut().setLevel(LogLevel.DEBUG);
		System.out.println("\nEstimating");
		ChainCrf<String> crf
			= ChainCrf.estimate(corpus,
				featureExtractor,
				addIntercept,
				minFeatureCount,
				cacheFeatures,
				allowUnseenTransitions,
				prior,
				priorBlockSize,
				annealingSchedule,
				minImprovement,
				minEpochs,
				maxEpochs,
				reporter);

		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.print("Enter text followed by new line\n>");
			System.out.flush();
			String text = reader.readLine();
			Tokenizer tokenizer = tokenizerFactory.tokenizer(text.toCharArray(),0,text.length());
			List<String> evalTokens = Arrays.asList(tokenizer.tokenize());
			Tagging<String> evalTagging = crf.tag(evalTokens);
			System.out.println(evalTagging);
			/*int maxNBest = 5;
			Iterator<ScoredTagging<String>> it
			= crf.tagNBestConditional(evalTokens,maxNBest);
			for (int rank = 0; rank < maxNBest && it.hasNext(); ++rank) {
				ScoredTagging<String> scoredTagging = it.next();
				System.out.println(rank + "    " + scoredTagging);
			}
			TagLattice<String> fbLattice
			= crf.tagMarginal(evalTokens);
			for (int n = 0; n < evalTokens.size(); ++n) {
				System.out.println(evalTokens.get(n));
				for (int k = 0; k < fbLattice.numTags(); ++k) {
					String tag = fbLattice.tag(k);
					double prob = fbLattice.logProbability(n,k);
					System.out.println("     " + tag + " " + prob);
				}
			}
			*/

		}
	}
}


package com.lingpipe.cookbook.chapter4;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Iterator;
import java.util.List;

import com.aliasi.classify.ConditionalClassification;
import com.aliasi.hmm.HiddenMarkovModel;
import com.aliasi.hmm.HmmDecoder;
import com.aliasi.tag.ScoredTagging;
import com.aliasi.tag.TagLattice;
import com.aliasi.tag.Tagging;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;


public class ConfidenceBasedTagger {

	public static void main(String[] args) 
			throws ClassNotFoundException, IOException {
		TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		String hmmModelPath = args.length > 0 ? args[0] : "models/pos-en-general-brown.HiddenMarkovModel";
		HiddenMarkovModel hmm = (HiddenMarkovModel) AbstractExternalizable.readObject(new File(hmmModelPath));
		HmmDecoder decoder = new HmmDecoder(hmm);
		BufferedReader bufReader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.print("\n\nINPUT> ");
			System.out.flush();
			String input = bufReader.readLine();
			Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(),0,input.length());
			String[] tokens = tokenizer.tokenize();
			List<String> tokenList = Arrays.asList(tokens);
			confidence(tokenList,decoder);
		}
	}


	static void confidence(List<String> tokenList, HmmDecoder decoder) {
		System.out.println("\nCONFIDENCE");
		System.out.println("#   Token          (Prob:Tag)*");
		TagLattice<String> lattice = decoder.tagMarginal(tokenList);
		for (int tokenIndex = 0; tokenIndex < tokenList.size(); ++tokenIndex) {
			ConditionalClassification tagScores = lattice.tokenClassification(tokenIndex);
			System.out.print(pad(Integer.toString(tokenIndex),4));
			System.out.print(pad(tokenList.get(tokenIndex),15));
			for (int i = 0; i < 3; ++i) {
				double conditionalProb = tagScores.score(i);
				String tag = tagScores.category(i);
				System.out.printf(" %9.3f:" + pad(tag,4),conditionalProb);
			}
			System.out.println();
		}
	}
	static String pad(String in, int length) {
		if (in.length() > length) return in.substring(0,length-3) + "...";
		if (in.length() == length) return in;
		StringBuilder sb = new StringBuilder(length);
		sb.append(in);
		while (sb.length() < length) sb.append(' ');
		return sb.toString();

	}

}

package com.lingpipe.cookbook.chapter4;

import com.aliasi.io.FileExtensionFilter;
import com.aliasi.corpus.Corpus;
import com.aliasi.corpus.ListCorpus;
import com.aliasi.corpus.Parser;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.hmm.HmmCharLmEstimator;
import com.aliasi.hmm.HmmDecoder;
import com.aliasi.tag.Tagging;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.Files;
import com.aliasi.util.Strings;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.ObjectOutputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

public class HmmTrainer {

	static void addTagging(TokenizerFactory tokenizerFactory, List<Tagging<String>> taggingList, char[] text) {
		
		Tokenizer tokenizer = tokenizerFactory.tokenizer(text, 0, text.length);
		List<String> tokens = new ArrayList<String>();
		List<String> tags = new ArrayList<String>();
		boolean bosFound = false;
		for (String token : tokenizer.tokenize()) {
			if (token.equals("[")) {
				bosFound = true;
			}
			else if (token.equals("]")) {
				tags.set(tags.size() - 1,"EOS");
			}
			else {
				tokens.add(token);
				if (bosFound) {
					tags.add("BOS");
					bosFound = false;
				}
				else {
					tags.add("WORD");
				}
			}
		}
		if (tokens.size() > 0) {
			taggingList.add(new Tagging<String>(tokens,tags));
		}
	}
	
	public static void main(String[] args) throws IOException {
		String inputFile = args.length > 0 ? args[0] : "data/connecticut_yankee_EOS.txt";
		char[] text = Files.readCharsFromFile(new File(inputFile), Strings.UTF8);
		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		
		/*List<Tagging<String>> taggingList = new ArrayList<Tagging<String>>();
		addTagging(tokenizerFactory,taggingList,text);
		ListCorpus<Tagging<String>> corpus
			= new ListCorpus<Tagging<String>> ();
		for (Tagging<String> tagging : taggingList) {
			System.out.println("Training " + tagging);
			corpus.addTrain(tagging);
		}
		*/
		
		  Corpus<ObjectHandler<Tagging<String>>> corpus = new TinyPosCorpus();
		 
		HmmCharLmEstimator estimator
			= new HmmCharLmEstimator();
		corpus.visitTrain(estimator);
		System.out.println("done training, token count: " + estimator.numTrainingTokens());
		HmmDecoder decoder = new HmmDecoder(estimator);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.print("Enter text followed by new line\n>");
			String evalText = reader.readLine();
			Tokenizer tokenizer = tokenizerFactory.tokenizer(evalText.toCharArray(),0,evalText.length());
			List<String> evalTokens = Arrays.asList(tokenizer.tokenize());
			Tagging<String> evalTagging = decoder.tag(evalTokens);
			System.out.println(evalTagging);
		}
	}
}
package com.lingpipe.cookbook.chapter4;

import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.util.List;
import java.util.SortedSet;

import au.com.bytecode.opencsv.CSVReader;

import com.aliasi.lm.TokenizedLM;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.util.ScoredObject;
import com.lingpipe.cookbook.Util;

public class InterestingPhrases {
	static int TEXT_INDEX = 3;
	public static void main(String[] args) throws IOException {
		String inputCsv = args.length > 0 ? args[0] : "data/disney.csv";	
		List<String[]> lines = Util.readCsv(new File(inputCsv));
		int ngramSize = 3;
		TokenizedLM languageModel 
			= new TokenizedLM(IndoEuropeanTokenizerFactory.INSTANCE,ngramSize);
		for (String [] line: lines) {
			languageModel.train(line[TEXT_INDEX]);
		}
		int phraseLength = 3;
		int minCount = 2;
		int maxReturned = 100;
		SortedSet<ScoredObject<String[]>> collocations 
			= languageModel.collocationSet(phraseLength, minCount, maxReturned);
		for (ScoredObject<String[]> scoredTokens : collocations) {
			double score = scoredTokens.score();
			StringBuilder sb = new StringBuilder();
			for (String token : scoredTokens.getObject()) {
				sb.append(token + " ");
			}
			System.out.printf("Score %.1f : ", score);
			System.out.println(sb);
		}
		
	}
}
package com.lingpipe.cookbook.chapter4;

import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.util.List;
import java.util.SortedSet;

import au.com.bytecode.opencsv.CSVReader;

import com.aliasi.classify.LMClassifier;
import com.aliasi.lm.CompiledNGramBoundaryLM;
import com.aliasi.lm.LanguageModel;
import com.aliasi.lm.LanguageModel.Sequence;
import com.aliasi.lm.NGramBoundaryLM;
import com.aliasi.lm.NGramProcessLM;
import com.aliasi.lm.TokenizedLM;
import com.aliasi.stats.MultivariateDistribution;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.LowerCaseTokenizerFactory;
import com.aliasi.tokenizer.ModifyTokenTokenizerFactory;
import com.aliasi.tokenizer.TokenLengthTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.ScoredObject;
import com.lingpipe.cookbook.Util;

public class InterestingPhrasesForegroundBackground {
	
	public static void main(String[] args) throws IOException {
		String backgroundCsv = args.length > 0 ? args[0] : "data/disneyWorld.csv";	
		List<String[]> backgroundData = Util.readCsv(new File(backgroundCsv));
		String foregroundCsv = args.length > 1 ? args[1] : "data/disneyLand.csv";	
		List<String[]> foregroundData = Util.readCsv(new File(foregroundCsv));
		
		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		tokenizerFactory = new LowerCaseTokenizerFactory(tokenizerFactory);		
		int minLength = 5;
		tokenizerFactory = new LengthFilterTokenizerFactoryPreserveToken(tokenizerFactory,minLength);
		int nGramOrder = 3;
		TokenizedLM backgroundLanguageModel 
		 	= new TokenizedLM(tokenizerFactory, nGramOrder);
		for (String [] line: backgroundData) {
			backgroundLanguageModel.train(line[Util.TEXT_OFFSET]);
		}		
		
		TokenizedLM foregroundLanguageModel = new TokenizedLM(tokenizerFactory,nGramOrder);
		for (String [] line: foregroundData) {
			foregroundLanguageModel.train(line[Util.TEXT_OFFSET]);
		}
		int phraseSize = 2;
		int minCount = 3;
		int maxReturned = 100;
		SortedSet<ScoredObject<String[]>> suprisinglyNewPhrases
			= foregroundLanguageModel.newTermSet(phraseSize, minCount, maxReturned,backgroundLanguageModel);
		for (ScoredObject<String[]> scoredTokens : suprisinglyNewPhrases) {
			double score = scoredTokens.score();
			String[] tokens = scoredTokens.getObject();
			System.out.printf("Score %f : ", score);
			System.out.println(java.util.Arrays.asList(tokens));
		}
	}
}


package com.lingpipe.cookbook.chapter4;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;

import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.ModifyTokenTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;

public class LengthFilterTokenizerFactoryPreserveToken extends ModifyTokenTokenizerFactory{

	int mMinLength;
	int mCounter = 0;
	
	public LengthFilterTokenizerFactoryPreserveToken(TokenizerFactory factory,int minLength) {
		super(factory);
		mMinLength = minLength;
	}
	
	public String modifyToken(String token) {
		if (token.length() < 5) {
			++mCounter;
			return "_" + mCounter;
		}
		return token;
	}
	
	private static final long serialVersionUID = -8346896300824818296L;
	
	public static void main(String[] args) throws IOException {
		TokenizerFactory tf = new IndoEuropeanTokenizerFactory();
		tf = new LengthFilterTokenizerFactoryPreserveToken(tf, 5);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.println("type a sentence below to see the tokens and white spaces:");
			String input = reader.readLine();
			Tokenizer tokenizer = tf.tokenizer(input.toCharArray(), 0, input.length());
			String token = null;
			StringBuilder sb = new StringBuilder();
			while ((token = tokenizer.nextToken()) != null) {
				String ws = tokenizer.nextWhitespace();
				System.out.println("Token:'" + token + "'");
				System.out.println("WhiteSpace:'" + ws + "'");
				sb.append(token);
				sb.append(ws);
			}
			System.out.println("Modified Output: " + sb.toString());
		}
	}
	}
	

package com.lingpipe.cookbook.chapter4;

import java.util.List;

import com.aliasi.classify.ConditionalClassification;
import com.aliasi.corpus.Corpus;
import com.aliasi.corpus.Handler;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.crf.ChainCrfFeatureExtractor;
import com.aliasi.crf.ChainCrfFeatures;
import com.aliasi.hmm.HiddenMarkovModel;
import com.aliasi.hmm.HmmDecoder;
import com.aliasi.tag.TagLattice;
import com.aliasi.tag.Tagging;
import com.aliasi.tokenizer.IndoEuropeanTokenCategorizer;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.ObjectToCounterMap;
import com.aliasi.util.ObjectToDoubleMap;

import java.io.File;
import java.io.IOException;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Map;

public class ModifiedCrfFeatureExtractor
implements ChainCrfFeatureExtractor<String> {

	HmmDecoder mDecoder;
	
	public ModifiedCrfFeatureExtractor() throws IOException, ClassNotFoundException {	
		File hmmFile = new File("models/pos-en-general-brown.HiddenMarkovModel");
		HiddenMarkovModel hmm = (HiddenMarkovModel) AbstractExternalizable.readObject(hmmFile);
		mDecoder = new HmmDecoder(hmm);
	}
	
	public ChainCrfFeatures<String> extract(List<String> tokens,
			List<String> tags) {
		return new ModChainCrfFeatures(tokens,tags);
	}

	class ModChainCrfFeatures extends ChainCrfFeatures<String> {
		
		TagLattice<String> mBrownTaggingLattice;
		
		public ModChainCrfFeatures(List<String> tokens,
				List<String> tags) {
			super(tokens,tags);
			mBrownTaggingLattice = mDecoder.tagMarginal(tokens);	
		}

		public Map<String,? extends Number> edgeFeatures(int n, int k) {
			ObjectToDoubleMap<String> features = new ObjectToDoubleMap<String>();
			features.set("TAG_" + tag(k),
					1.0d);
			String category = IndoEuropeanTokenCategorizer
					.CATEGORIZER
					.categorize(token(n));
			features.set("TOKEN_SHAPE_" + category,1.0d);
			return features;
		}
		
		public Map<String,? extends Number> nodeFeatures(int n) {
			ObjectToDoubleMap<String> features = new ObjectToDoubleMap<String>();
			features.set("TOK_" + token(n), 1);
			ConditionalClassification tagScores 
				= mBrownTaggingLattice.tokenClassification(n);
			for (int i = 0; i < 3; ++ i) {
				double conditionalProb = tagScores.score(i);
				String tag = tagScores.category(i);
				features.increment(tag, conditionalProb);
			}
			return features;
		}

	}

	public static void main(String[] args) throws IOException, ClassNotFoundException {
		Corpus <ObjectHandler<Tagging<String>>> corpus = new TinyPosCorpus();
		final ChainCrfFeatureExtractor<String> featureExtractor 
			= new ModifiedCrfFeatureExtractor();
		corpus.visitCorpus(new ObjectHandler<Tagging<String>>() {
			@Override
			public void handle(Tagging<String> tagging) {
				ChainCrfFeatures<String> features = featureExtractor.extract(tagging.tokens(),tagging.tags());
				for (int i = 0; i < tagging.size(); ++i) {
					System.out.println("-------------------");
					System.out.println("Tagging:  " + tagging.token(i) + "/" + tagging.tag(i));
					System.out.println("Node Feats:" + features.nodeFeatures(i));
					for (int j = 0; j < tagging.size(); ++j) {
						System.out.println("Edge Feats:" + features.edgeFeatures(i, j));
					}
				}
			}
		});
	}
}
package com.lingpipe.cookbook.chapter4;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Iterator;
import java.util.List;

import com.aliasi.classify.ConditionalClassification;
import com.aliasi.hmm.HiddenMarkovModel;
import com.aliasi.hmm.HmmDecoder;
import com.aliasi.tag.ScoredTagging;
import com.aliasi.tag.TagLattice;
import com.aliasi.tag.Tagging;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;


public class NbestPosTagger {

	public static void main(String[] args) 
			throws ClassNotFoundException, IOException {
		TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		String hmmModelPath = args.length > 0 ? args[0] : "models/pos-en-general-brown.HiddenMarkovModel";
		HiddenMarkovModel hmm = (HiddenMarkovModel) AbstractExternalizable.readObject(new File(hmmModelPath));
		HmmDecoder decoder = new HmmDecoder(hmm);
		BufferedReader bufReader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.print("\n\nINPUT> ");
			System.out.flush();
			String input = bufReader.readLine();
			Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(),0,input.length());
			String[] tokens = tokenizer.tokenize();
			List<String> tokenList = Arrays.asList(tokens);
			nBest(tokenList,decoder,5);
		}
	}


	static void nBest(List<String> tokenList, HmmDecoder decoder, int maxNBest) {
		System.out.println("\nN BEST");
		System.out.println("#   JointLogProb         Analysis");
		Iterator<ScoredTagging<String>> nBestIt = decoder.tagNBest(tokenList,maxNBest);
		for (int n = 0; nBestIt.hasNext(); ++n) {
			ScoredTagging<String> scoredTagging = nBestIt.next();
			System.out.printf(n + "   %9.3f  ",scoredTagging.score());
			for (int i = 0; i < tokenList.size(); ++i) {
				System.out.print(scoredTagging.token(i) + "_" + pad(scoredTagging.tag(i),5));
			}
			System.out.println();
		}        
	}

	static String pad(String in, int length) {
		if (in.length() > length) return in.substring(0,length-3) + "...";
		if (in.length() == length) return in;
		StringBuilder sb = new StringBuilder(length);
		sb.append(in);
		while (sb.length() < length) sb.append(' ');
		return sb.toString();

	}

}
package com.lingpipe.cookbook.chapter4;

import java.io.File;
import java.io.FileOutputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.OutputStreamWriter;
import java.util.Arrays;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import com.aliasi.classify.LMClassifier;
import com.aliasi.hmm.HiddenMarkovModel;
import com.aliasi.hmm.HmmDecoder;
import com.aliasi.lm.CompiledNGramBoundaryLM;
import com.aliasi.stats.MultivariateDistribution;
import com.aliasi.tag.Tagging;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.RegExTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.Strings;

import au.com.bytecode.opencsv.CSVReader;
import au.com.bytecode.opencsv.CSVWriter;

public class PosTagTweetsHMM {

	//language ID
	//postag with '_~_' 
	
	static int TEXT_OFFSET = 3;
	static String ENGLISH = "english";
	
	public static void main(String[] args) throws IOException, ClassNotFoundException {
		@SuppressWarnings("unchecked")
		LMClassifier<CompiledNGramBoundaryLM, MultivariateDistribution> langClassifier 
			= (LMClassifier<CompiledNGramBoundaryLM, MultivariateDistribution>) 
				AbstractExternalizable.readObject(new File("models/3LangId.LMClassifier"));
		TokenizerFactory tokFactory = new RegExTokenizerFactory("[\\w@#$]+");
		HiddenMarkovModel hmm = (HiddenMarkovModel) AbstractExternalizable.readObject(new File("models/pos-en-general-brown.HiddenMarkovModel"));
		HmmDecoder decoder = new HmmDecoder(hmm);
		String inFile = args[0];
		CSVReader csvReader = new CSVReader(new FileReader(inFile));
		List<String[]> annotatedData = csvReader.readAll();
		Set<String> categories = new HashSet<String>();
		String outFile = args[1];
		FileOutputStream fileOut =  new FileOutputStream(outFile);
		OutputStreamWriter streamWriter = new OutputStreamWriter(fileOut,Strings.UTF8);
		//CsvListWriter writer = new CsvListWriter(streamWriter,CsvPreference.EXCEL_PREFERENCE); 
		for (String[] tweetData : annotatedData) {
			String tweet = tweetData[TEXT_OFFSET];
			if (!langClassifier.classify(tweet).bestCategory().equals(ENGLISH)) {
				continue;
			}
			List<String> tokens = Arrays.asList(tokFactory.tokenizer(tweet.toCharArray(), 0, tweet.length()).tokenize());
			Tagging<String> tagging = decoder.tag(tokens);
			for (int i = 0; i < tagging.size(); ++i) {
				//writer.write(new String[] {tagging.token(i),tagging.tag(i)});
			}
			//tweetData[TEXT_OFFSET] = tagging.toString();
			//writer.write(new String[] {"","EOT"});
			//System.out.println(tagging);
		}
		//writer.close();
	}
}
package com.lingpipe.cookbook.chapter4;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Iterator;
import java.util.List;

import com.aliasi.classify.ConditionalClassification;
import com.aliasi.hmm.HiddenMarkovModel;
import com.aliasi.hmm.HmmDecoder;
import com.aliasi.tag.ScoredTagging;
import com.aliasi.tag.TagLattice;
import com.aliasi.tag.Tagging;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;


public class PosTagger {

	public static void main(String[] args) 
			throws ClassNotFoundException, IOException {
		TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		String hmmModelPath = args.length > 0 ? args[0] : "models/pos-en-general-brown.HiddenMarkovModel";
		HiddenMarkovModel hmm = (HiddenMarkovModel) AbstractExternalizable.readObject(new File(hmmModelPath));
		HmmDecoder decoder = new HmmDecoder(hmm);
		BufferedReader bufReader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.print("\n\nINPUT> ");
			System.out.flush();
			String input = bufReader.readLine();
			Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(),0,input.length());
			String[] tokens = tokenizer.tokenize();
			List<String> tokenList = Arrays.asList(tokens);
			firstBest(tokenList,decoder);
		}
	}

	static void firstBest(List<String> tokenList, HmmDecoder decoder) {
		Tagging<String> tagging = decoder.tag(tokenList);
		System.out.println("\nFIRST BEST");
		for (int i = 0; i < tagging.size(); ++i)
			System.out.print(tagging.token(i) + "_" + tagging.tag(i) + " ");
		System.out.println();
	}

	/*static void nBest(List<String> tokenList, HmmDecoder decoder, int maxNBest) {
		System.out.println("\nN BEST");
		System.out.println("#   JointLogProb         Analysis");
		Iterator<ScoredTagging<String>> nBestIt = decoder.tagNBest(tokenList,maxNBest);
		for (int n = 0; n < maxNBest && nBestIt.hasNext(); ++n) {
			ScoredTagging<String> scoredTagging = nBestIt.next();
			double score = scoredTagging.score();
			System.out.print(n + "   " + format(score) + "  ");
			for (int i = 0; i < tokenList.size(); ++i)
				System.out.print(scoredTagging.token(i) + "_" + pad(scoredTagging.tag(i),5));
			System.out.println();
		}        
	}

	static void confidence(List<String> tokenList, HmmDecoder decoder) {
		System.out.println("\nCONFIDENCE");
		System.out.println("#   Token          (Prob:Tag)*");
		TagLattice<String> lattice = decoder.tagMarginal(tokenList);
		for (int tokenIndex = 0; tokenIndex < tokenList.size(); ++tokenIndex) {
			ConditionalClassification tagScores = lattice.tokenClassification(tokenIndex);
			System.out.print(pad(Integer.toString(tokenIndex),4));
			System.out.print(pad(tokenList.get(tokenIndex),15));
			for (int i = 0; i < 5; ++i) {
				double conditionalProb = tagScores.score(i);
				String tag = tagScores.category(i);
				System.out.print(" " + format(conditionalProb) 
						+ ":" + pad(tag,4));
			}
			System.out.println();
		}
	}

	static String format(double x) {
		return String.format("%9.3f",x);
	}

	static String pad(String in, int length) {
		if (in.length() > length) return in.substring(0,length-3) + "...";
		if (in.length() == length) return in;
		StringBuilder sb = new StringBuilder(length);
		sb.append(in);
		while (sb.length() < length) sb.append(' ');
		return sb.toString();

	}
*/
}
package com.lingpipe.cookbook.chapter4;

import com.aliasi.crf.ChainCrfFeatureExtractor;
import com.aliasi.crf.ChainCrfFeatures;
import com.aliasi.tag.Tagging;
import com.aliasi.util.ObjectToCounterMap;
import com.aliasi.util.ObjectToDoubleMap;

import java.util.Arrays;
import java.util.Collections;
import java.util.List;
import java.util.Map;

public class SimpleCrfFeatureExtractor
    implements ChainCrfFeatureExtractor<String> {

    public ChainCrfFeatures<String> extract(List<String> tokens,
                                            List<String> tags) {
        return new SimpleChainCrfFeatures(tokens,tags);
    }

    static class SimpleChainCrfFeatures
        extends ChainCrfFeatures<String> {

        public SimpleChainCrfFeatures(List<String> tokens,
                                      List<String> tags) {
            super(tokens,tags);
        }
        
        public Map<String,Double> nodeFeatures(int n) {
        	ObjectToDoubleMap<String> features = new ObjectToDoubleMap<String>();
			features.increment("TOK_" + token(n),1.0);
			//System.out.println("node " + n);
			return features;
        }
        
        
        
        public Map<String,Double> edgeFeatures(int n, int k) {
        	ObjectToDoubleMap<String> features = new ObjectToDoubleMap<String>();
        	//System.out.println("edge " + n + " " + k);
        	features.increment("TAG_" + tag(k),1.0);
        	return features;
        }
    }
}package com.lingpipe.cookbook.chapter4;
import com.aliasi.classify.BaseClassifierEvaluator;
import com.aliasi.classify.ConfusionMatrix;
import com.aliasi.corpus.Corpus;
import com.aliasi.corpus.Parser;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.hmm.HiddenMarkovModel;
import com.aliasi.hmm.HmmCharLmEstimator;
import com.aliasi.hmm.HmmDecoder;
import com.aliasi.tag.MarginalTaggerEvaluator;
import com.aliasi.tag.NBestTaggerEvaluator;
import com.aliasi.tag.TaggerEvaluator;
import com.aliasi.tag.Tagging;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.Strings;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Random;
import java.util.Set;

import org.xml.sax.InputSource;

public class TagEvaluator {
	public static void main(String[] args) 
			throws ClassNotFoundException, IOException {
		HmmDecoder decoder = null;
		boolean storeTokens = true;
		TaggerEvaluator<String> evaluator
			= new TaggerEvaluator<String>(decoder,storeTokens);
		Corpus<ObjectHandler<Tagging<String>>> smallCorpus = new TinyPosCorpus();
		int numFolds = 10;
		XValidatingObjectCorpus<Tagging<String>> xValCorpus 
			= new XValidatingObjectCorpus<Tagging<String>>(numFolds);
		smallCorpus.visitCorpus(xValCorpus);
		xValCorpus.permuteCorpus(new Random(123234235));
		for (int i = 0; i < numFolds; ++i) {
			xValCorpus.setFold(i);
			HmmCharLmEstimator estimator = new HmmCharLmEstimator();
			xValCorpus.visitTrain(estimator);
			System.out.println("done training " + estimator.numTrainingTokens());
			decoder = new HmmDecoder(estimator);
			evaluator.setTagger(decoder);
			xValCorpus.visitTest(evaluator);
		}
		BaseClassifierEvaluator<String> classifierEval 
			= evaluator.tokenEval();
		System.out.println(classifierEval);
	}
}
package com.lingpipe.cookbook.chapter4;

import com.aliasi.corpus.Corpus;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.tag.Tagging;

import java.io.IOException;
import java.util.Arrays;

public class TinyPosCorpus
extends Corpus<ObjectHandler<Tagging<String>>> {

	public void visitTrain(ObjectHandler<Tagging<String>> handler) {
		for (String[][] wordsTags : WORDS_TAGSS) {
			String[] words = wordsTags[0];
			String[] tags = wordsTags[1];
			Tagging<String> tagging
			= new Tagging<String>(Arrays.asList(words),
					Arrays.asList(tags));
			handler.handle(tagging);
		}
	}

	public void visitTest(ObjectHandler<Tagging<String>> handler) {
		/* no op */
	}

	// legal starts: PN, DET
	// legal trans: DET-N, IV-EOS, N-EOS, N-IV, N-TV, PN-EOS, PN-IV, PN-TV, TV-DET, TV-PN,
	// legal ends: EOS
	static final String[][][] WORDS_TAGSS = new String[][][] {
		{ { "John", "ran", "." },                 { "PN", "IV", "EOS" } },
		{ { "Mary", "ran", "." },                 { "PN", "IV", "EOS" } },
		{ { "John", "jumped", "!" },              { "PN", "IV", "EOS" } },
		{ { "The", "dog", "jumped", "!" },        { "DET", "N", "IV", "EOS" } },
		{ { "The", "dog", "sat", "." },           { "DET", "N", "IV", "EOS" } },
		{ { "Mary", "sat", "!" },                 { "PN", "IV", "EOS" } },
		{ { "Mary", "likes", "John", "." },       { "PN", "TV", "PN", "EOS" } },
		{ { "The", "dog", "likes", "Mary", "." }, { "DET", "N", "TV", "PN", "EOS" } },
		{ { "John", "likes", "the", "dog", "." }, { "PN", "TV", "DET", "N", "EOS" } },
		{ { "The", "dog", "ran", "." },           { "DET", "N", "IV", "EOS", } },
		{ { "The", "dog", "ran", "." },           { "DET", "N", "IV", "EOS", } }
		
		//{ {"t1","t2","t3"}, {"c1","c2","c3"}}
	};

	public static void main(String[] args) throws IOException {
		Corpus<ObjectHandler<Tagging<String>>> corpus = new TinyPosCorpus();
		corpus.visitTrain(new ObjectHandler<Tagging<String>>() {

			@Override
			public void handle(Tagging<String> tagging) {
				System.out.println("Tagging: " + tagging);
				
			}
			
		});
	}
}



package com.lingpipe.cookbook.chapter4;

public class TrainWordTaggerHMM {

	
	//read tweets
}
package com.lingpipe.cookbook.chapter5;

import java.util.ArrayList;
import java.util.List;

import com.aliasi.chunk.BioTagChunkCodec;
import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunking;
import com.aliasi.tag.StringTagging;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;

public class BioCodec {
	public static void main(String[] args) {
		
		List<String> tokens = new ArrayList<String>();
		tokens.add("The");
		tokens.add("rain");
		tokens.add("in");
		tokens.add("Spain");
		tokens.add(".");
		List<String> tags = new ArrayList<String>();
		tags.add("B_Weather");
		tags.add("I_Weather");
		tags.add("O");
		tags.add("B_Place");
		tags.add("O");
		CharSequence cs = "The rain in Spain.";
						 //012345678901234567
		int[] tokenStarts = {0,4,9,12,17};
		int[] tokenEnds = {3,8,11,17,17};
		StringTagging tagging = new StringTagging(tokens, tags, cs, tokenStarts, tokenEnds);
		System.out.println("Tagging for :" + cs);
		for (int i = 0; i < tagging.size(); ++i) {
			System.out.println(tagging.token(i) + "/" + tagging.tag(i));
		}
		BioTagChunkCodec codec = new BioTagChunkCodec();
		Chunking chunking = codec.toChunking(tagging);
		System.out.println("Chunking from StringTagging");
		for (Chunk chunk : chunking.chunkSet()) {
			System.out.println(chunk);
		}
		boolean enforceConsistency = true;
		BioTagChunkCodec codec2 = new BioTagChunkCodec(IndoEuropeanTokenizerFactory.INSTANCE, enforceConsistency);
		StringTagging tagging2 = codec2.toStringTagging(chunking);
		System.out.println("StringTagging from Chunking");
		for (int i = 0; i < tagging2.size(); ++i) {
			System.out.println(tagging2.token(i) + "/" + tagging2.tag(i));
		}
	}
}
package com.lingpipe.cookbook.chapter5;

import com.aliasi.chunk.BioTagChunkCodec;
import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.ChunkFactory;
import com.aliasi.chunk.Chunking;
import com.aliasi.chunk.ChunkingImpl;
import com.aliasi.chunk.TagChunkCodec;
import com.aliasi.corpus.Corpus;
import com.aliasi.corpus.ListCorpus;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.corpus.XValidatingObjectCorpus;

import com.aliasi.crf.ChainCrf;
import com.aliasi.crf.ChainCrfChunker;
import com.aliasi.crf.ChainCrfFeatureExtractor;
import com.aliasi.crf.ForwardBackwardTagLattice;

import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;

import com.aliasi.sentences.SentenceChunker;
import com.aliasi.stats.AnnealingSchedule;
import com.aliasi.stats.RegressionPrior;

import com.aliasi.tag.ScoredTagging;
import com.aliasi.tag.TagLattice;
import com.aliasi.tag.Tagging;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.RegExTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;

import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.Files;
import com.aliasi.util.Strings;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;

import com.aliasi.crf.ChainCrfFeatureExtractor;
import com.aliasi.crf.ChainCrfFeatures;

import java.io.Serializable;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Set;

public class CRFSentenceChunking {
	
	
	static Chunking getChunking(TokenizerFactory tokenizerFactory, char[] text) {
		List<Chunk> chunks = new ArrayList<Chunk>();
		StringBuilder sb = new StringBuilder();
		Tokenizer tokenizer = tokenizerFactory.tokenizer(text, 0, text.length);
		String token = null;
		int sentStart = -1;
		int sentBody = 0;
		while ((token = tokenizer.nextToken()) != null) {
			String whiteSpace = tokenizer.nextWhitespace();
			if (token.equals("[")) {
				sentStart = sb.length();
				sentBody = 0;
			}
			else if (token.equals("]")) {
				chunks.add(ChunkFactory.createChunk(sentStart,sentStart + sentBody,"S"));
				sentStart = -1;
			}
			else {
				sb.append(token);
				sentBody += token.length();
				sentBody += whiteSpace.length();
			}
			sb.append(whiteSpace);
		}
		ChunkingImpl chunking = new ChunkingImpl(sb.toString());
		chunking.addAll(chunks);
		return chunking;
	}
	
    public static void main(String[] args) throws IOException {
    	TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
    	char[] text = Files.readCharsFromFile(new File(args[0]), Strings.UTF8);
		Chunking chunking = getChunking(tokenizerFactory,text);
		
		System.out.println("Training Chunking \n" + chunking);
		String t = (String) chunking.charSequence();
		for (Chunk chunk : chunking.chunkSet()) {
			System.out.println(t.substring(chunk.start(), chunk.end()) + "|");
		}
		ListCorpus<Chunking> corpus
			= new ListCorpus<Chunking> ();
		corpus.addTrain(chunking);
		boolean enforceConsistency = false;
        TagChunkCodec tagChunkCodec
            = new BioTagChunkCodec(tokenizerFactory,
                                   enforceConsistency);
      
       
        ChainCrfFeatureExtractor<String> featureExtractor
            = null;//new SimpleChainCrfFeatureExtractor();
        boolean addIntercept = true;
        int minFeatureCount = 1;
        boolean cacheFeatures = false;
        boolean allowUnseenTransitions = true;
        double priorVariance = 4.0;
        boolean uninformativeIntercept = true;
        RegressionPrior prior
            = RegressionPrior.gaussian(priorVariance,
                                       uninformativeIntercept);
        int priorBlockSize = 3;
        double initialLearningRate = 0.05;
        double learningRateDecay = 0.995;
        AnnealingSchedule annealingSchedule
            = AnnealingSchedule.exponential(initialLearningRate,
                                            learningRateDecay);
        double minImprovement = 0.00001;
        int minEpochs = 2;
        int maxEpochs = 2000;
        Reporter reporter
            = Reporters.stdOut().setLevel(LogLevel.DEBUG);
        System.out.println("\nEstimating");
        ChainCrfChunker crfChunker
        	= ChainCrfChunker.estimate(corpus,
                                   tagChunkCodec,
                                   tokenizerFactory,
                                   featureExtractor,
                                   addIntercept,
                                   minFeatureCount,
                                   cacheFeatures,
                                   prior,
                                   priorBlockSize,
                                   annealingSchedule,
                                   minImprovement,
                                   minEpochs,
                                   maxEpochs,
                                   reporter);

        
        BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.print("Enter text followed by new line\n>");
			System.out.flush();
			String evalText = reader.readLine();
			List<String> evalTokens = Arrays.asList(tokenizerFactory.tokenizer(evalText.toCharArray(),0,evalText.length()).tokenize());
			Chunking evalChunking = crfChunker.chunk(evalText);
			Tagging<String> tagging2 = crfChunker.crf().tag(evalTokens);
			System.out.println(tagging2);
			System.out.println(evalChunking);
			
            
		}
    }
}


package com.lingpipe.cookbook.chapter5;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;

import com.aliasi.chunk.BioTagChunkCodec;
import com.aliasi.chunk.Chunking;
import com.aliasi.chunk.TagChunkCodec;
import com.aliasi.chunk.TagChunkCodecAdapters;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.corpus.StringParser;
import com.aliasi.tag.LineTaggingParser;
import com.aliasi.tag.Tagging;

public class Conll2002ChunkTagParser extends StringParser<ObjectHandler<Chunking>> {

	static final String TOKEN_TAG_LINE_REGEX
	= "(\\S+)\\s(\\S+\\s)?(O|[B|I]-\\S+)"; // token ?posTag entityTag
	static final int TOKEN_GROUP = 1; // token
	static final int TAG_GROUP = 3;   // entityTag
	static final String IGNORE_LINE_REGEX
	= "-DOCSTART(.*)";  // lines that start with "-DOCSTART"
	static final String EOS_REGEX
	= "\\A\\Z";         // empty lines

	static final String BEGIN_TAG_PREFIX = "B-";
	static final String IN_TAG_PREFIX = "I-";
	static final String OUT_TAG = "O";

	private final LineTaggingParser mParser
	= new LineTaggingParser(TOKEN_TAG_LINE_REGEX, TOKEN_GROUP, TAG_GROUP,
			IGNORE_LINE_REGEX, EOS_REGEX);

	private final TagChunkCodec mCodec
	= new BioTagChunkCodec(null, // no tokenizer
			false,  // don't enforce consistency
			BEGIN_TAG_PREFIX, // custom BIO tag coding matches regex
			IN_TAG_PREFIX,
			OUT_TAG);

	public void parseString(char[] cs, int start, int end) {
		mParser.parseString(cs,start,end);
	}

	public void setHandler(ObjectHandler<Chunking> handler) {
		ObjectHandler<Tagging<String>> taggingHandler
		= TagChunkCodecAdapters.chunkingToTagging(mCodec,handler);
		mParser.setHandler(taggingHandler);
	}

	public TagChunkCodec getTagChunkCodec(){
		return mCodec;
	}

}
package com.lingpipe.cookbook.chapter5;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunking;
import com.aliasi.dict.DictionaryEntry;
import com.aliasi.dict.ExactDictionaryChunker;
import com.aliasi.dict.MapDictionary;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;

public class DictionaryChunker {

	/**
	 * @param args
	 */
	static final double CHUNK_SCORE = 1.0;
	public static void main(String[] args) throws IOException {
		MapDictionary<String> dictionary = new MapDictionary<String>();
		dictionary.addEntry(new DictionaryEntry<String>("Arthur","PERSON",CHUNK_SCORE));
		dictionary.addEntry(new DictionaryEntry<String>("Ford","PERSON",CHUNK_SCORE));
		dictionary.addEntry(new DictionaryEntry<String>("Trillian","PERSON",CHUNK_SCORE));
		dictionary.addEntry(new DictionaryEntry<String>("Zaphod","PERSON",CHUNK_SCORE));

		dictionary.addEntry(new DictionaryEntry<String>("Marvin","ROBOT",CHUNK_SCORE));
		dictionary.addEntry(new DictionaryEntry<String>("Heart of Gold","SPACECRAFT",CHUNK_SCORE));
		dictionary.addEntry(new DictionaryEntry<String>("Hitchhikers Guide","PRODUCT",CHUNK_SCORE));
		dictionary.addEntry(new DictionaryEntry<String>("Heart","ORGAN",CHUNK_SCORE));
		boolean returnAllMatches = true;
		boolean caseSensitive = true;
		ExactDictionaryChunker dictionaryChunker
		= new ExactDictionaryChunker(dictionary,
				IndoEuropeanTokenizerFactory.INSTANCE,
				returnAllMatches,caseSensitive);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
	    String text = "";
	    while (true) {
	    	System.out.println("Enter text, . to quit:");
	    	text = reader.readLine();
	    	if(text.equals(".")){
	    		break;
	    	}
	    	System.out.println("\nCHUNKER overlapping, case sensitive");
	    	Chunking chunking = dictionaryChunker.chunk(text);
		    for (Chunk chunk : chunking.chunkSet()) {
		        int start = chunk.start();
		        int end = chunk.end();
		        String type = chunk.type();
		        double score = chunk.score();
		        String phrase = text.substring(start,end);
		        System.out.println("     phrase=|" + phrase + "|"
		                           + " start=" + start
		                           + " end=" + end
		                           + " type=" + type
		                           + " score=" + score);
		    }
	    }

	}
	
	static void chunk(ExactDictionaryChunker chunker, String text) {
	    
	}

}
package com.lingpipe.cookbook.chapter5;

import java.io.File;
import java.io.IOException;
import java.util.HashSet;
import java.util.Set;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.ChunkFactory;
import com.aliasi.sentences.SentenceChunker;
import com.aliasi.util.Files;
import com.aliasi.util.Strings;

public class ErrorCheckingSimpleSentenceParser {

	public static void main(String[] args) throws IOException {
		char[] chars = Files.readCharsFromFile(new File(args[0]), Strings.UTF8);
		StringBuilder rawChars = new StringBuilder();
		int sentStart = -1;
		int sentEnd = -1;
		Set<Chunk> sentChunks = new HashSet<Chunk>();
		for (int i=0; i < chars.length; ++i) {
			if (chars[i] == '[') {
				if (sentStart != -1 || sentEnd != -1) {
					throw new RuntimeException("sentence start wrong. Got " + sentStart + " sentStart " + sentEnd + " sentEnd");
				}
				sentStart = rawChars.length();
			}
			else if (chars[i] == ']') {
				if (sentStart == -1 || sentEnd != -1) {
					throw new RuntimeException("sentence end wrong. Got " + sentStart + " sentStart " + sentEnd + " sentEnd");
				}
				sentEnd = rawChars.length();
				sentChunks.add(ChunkFactory.createChunk(sentStart,sentEnd,SentenceChunker.SENTENCE_CHUNK_TYPE));
				sentStart = -1;
				sentEnd = -1;
			}
			else {
				rawChars.append(chars[i]);
			}
		}
		String unannotatedText = rawChars.toString();
		for (Chunk sent : sentChunks) {
			System.out.println(sent);
		}
	}

}
package com.lingpipe.cookbook.chapter5;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import javax.management.RuntimeErrorException;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.ChunkAndCharSeq;
import com.aliasi.chunk.ChunkFactory;
import com.aliasi.chunk.Chunking;
import com.aliasi.chunk.ChunkingEvaluation;
import com.aliasi.chunk.ChunkingImpl;
import com.aliasi.sentences.IndoEuropeanSentenceModel;
import com.aliasi.sentences.SentenceChunker;
import com.aliasi.sentences.SentenceEvaluation;
import com.aliasi.sentences.SentenceEvaluator;
import com.aliasi.sentences.SentenceModel;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.Files;
import com.aliasi.util.Strings;

public class EvaluateAnnotatedSentences {

	public static void main(String[] args) throws IOException {
		String annotatedSentPath = args.length > 0 ? args[0] : "data/hitchHikersGuide.sentDetected";
		char[] chars = Files.readCharsFromFile(new File(annotatedSentPath), Strings.UTF8);
		StringBuilder rawChars = new StringBuilder();
		int start = -1;
		int end = -1;
		Set<Chunk> sentChunks = new HashSet<Chunk>();
		for (int i=0; i < chars.length; ++i) {
			if (chars[i] == '[') {
				start = rawChars.length();
			}
			else if (chars[i] == ']') {
				end = rawChars.length();
				Chunk chunk = ChunkFactory.createChunk(start,end,SentenceChunker.SENTENCE_CHUNK_TYPE);
				sentChunks.add(chunk);
			}
			else {
				rawChars.append(chars[i]);
			}
		}
		String originalText = rawChars.toString();
		ChunkingImpl sentChunking = new ChunkingImpl(originalText);
		for (Chunk chunk : sentChunks) {
			sentChunking.add(chunk);
		}
		boolean eosIsSentBoundary = false;
		boolean balanceParens = true;
		SentenceModel sentenceModel 
			= new MySentenceModel();
			//= new IndoEuropeanSentenceModel(eosIsSentBoundary,balanceParens);
		TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		SentenceChunker sentenceChunker 
			= new SentenceChunker(tokFactory,sentenceModel);	

		SentenceEvaluator evaluator = new SentenceEvaluator(sentenceChunker);
		evaluator.handle(sentChunking);
		SentenceEvaluation eval = evaluator.evaluation();
		ChunkingEvaluation chunkEval = eval.chunkingEvaluation();
		for (ChunkAndCharSeq truePos : chunkEval.truePositiveSet()) {
			System.out.println("TruePos: " + truePos);
		}
		for (ChunkAndCharSeq falsePos : chunkEval.falsePositiveSet()) {
			System.out.println("FalsePos: " + falsePos);
		}
		for (ChunkAndCharSeq falseNeg : chunkEval.falseNegativeSet()){
			System.out.println("FalseNeg: " + falseNeg);
		}		
	}
}
package com.lingpipe.cookbook.chapter5;

import com.aliasi.corpus.Corpus;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.crf.ChainCrfFeatureExtractor;
import com.aliasi.crf.ChainCrfFeatures;

import com.aliasi.tag.Tagger;
import com.aliasi.tag.Tagging;

import com.aliasi.tokenizer.IndoEuropeanTokenCategorizer;

import com.aliasi.hmm.HiddenMarkovModel;
import com.aliasi.hmm.HmmDecoder;

import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.FastCache;
import com.aliasi.util.ObjectToDoubleMap;
import com.lingpipe.cookbook.chapter4.ModifiedCrfFeatureExtractor;
import com.lingpipe.cookbook.chapter4.TinyPosCorpus;

import java.io.File;
import java.io.IOException;
import java.io.ObjectInput;
import java.io.ObjectOutput;
import java.io.Serializable;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Map;

public class FancyCrfFeatureExtractor
implements ChainCrfFeatureExtractor<String> {

	private final Tagger<String> mPosTagger;

	public FancyCrfFeatureExtractor()
			throws ClassNotFoundException, IOException {
		File posHmmFile = new File("models/pos-en-general-brown.HiddenMarkovModel");
		@SuppressWarnings("unchecked") 
		HiddenMarkovModel posHmm
		= (HiddenMarkovModel)
		AbstractExternalizable
		.readObject(posHmmFile);
		FastCache<String,double[]> emissionCache
			= new FastCache<String,double[]>(100000);
		mPosTagger = new HmmDecoder(posHmm,null,emissionCache);
	}

	public ChainCrfFeatures<String> extract(List<String> tokens,
			List<String> tags) {
		return new ChunkerFeatures(tokens,tags);
	}


	class ChunkerFeatures extends ChainCrfFeatures<String> {
		private final Tagging<String> mPosTagging;
		public ChunkerFeatures(List<String> tokens,
				List<String> tags) {
			super(tokens,tags);
			mPosTagging = mPosTagger.tag(tokens);
		}
		public Map<String,? extends Number> nodeFeatures(int n) {
			ObjectToDoubleMap<String> feats
			= new ObjectToDoubleMap<String>();

			boolean bos = n == 0;
			boolean eos = (n + 1) >= numTokens();

			String tokenCat = tokenCat(n);
			String prevTokenCat = bos ? null : tokenCat(n-1);
			String nextTokenCat = eos ? null : tokenCat(n+1);

			String token = normedToken(n);
			String prevToken = bos ? null : normedToken(n-1);
			String nextToken = eos ? null : normedToken(n+1);

			String posTag = mPosTagging.tag(n);
			String prevPosTag = bos ? null : mPosTagging.tag(n-1);
			String nextPosTag = eos ? null : mPosTagging.tag(n+1);

			if (bos)
				feats.set("BOS",1.0);
			if (eos)
				feats.set("EOS",1.0);
			if (!bos && !eos)
				feats.set("!BOS!EOS",1.0);

			feats.set("TOK_" + token, 1.0);
			if (!bos)
				feats.set("TOK_PREV_" + prevToken,1.0);
			if (!eos)
				feats.set("TOK_NEXT_" + nextToken,1.0);

			feats.set("TOK_CAT_" + tokenCat, 1.0);
			if (!bos)
				feats.set("TOK_CAT_PREV_" + prevTokenCat, 1.0);
			if (!eos)
				feats.set("TOK_CAT_NEXT_" + nextToken, 1.0);

			feats.set("POS_" + posTag,1.0);
			if (!bos)
				feats.set("POS_PREV_" + prevPosTag,1.0);
			if (!eos)
				feats.set("POS_NEXT_" + nextPosTag,1.0);

			for (String suffix : suffixes(token))
				feats.set("SUFF_" + suffix,1.0);
			if (!bos)
				for (String suffix : suffixes(prevToken))
					feats.set("SUFF_PREV_" + suffix,1.0);
			if (!eos)
				for (String suffix : suffixes(nextToken))
					feats.set("SUFF_NEXT_" + suffix,1.0);

			for (String prefix : prefixes(token))
				feats.set("PREF_" + prefix,1.0);
			if (!bos)
				for (String prefix : prefixes(prevToken))
					feats.set("PREF_PREV_" + prefix,1.0);
			if (!eos)
				for (String prefix : prefixes(nextToken))
					feats.set("PREF_NEXT_" + prefix,1.0);

			return feats;
		}

		public Map<String,? extends Number> edgeFeatures(int n, int k) {
			ObjectToDoubleMap<String> feats
			= new ObjectToDoubleMap<String>();
			feats.set("PREV_TAG_" + tag(k),
					1.0);
			feats.set("PREV_TAG_TOKEN_CAT_"  + tag(k)
					+ "_" + tokenCat(n-1),
					1.0);
			return feats;
		}

		// e.g. 12/3/08 to *DD*/*D*/*DD*
		public String normedToken(int n) {
			return token(n).replaceAll("\\d+","*$0*").replaceAll("\\d","D");
		}

		public String tokenCat(int n) {
			return IndoEuropeanTokenCategorizer.CATEGORIZER.categorize(token(n));
		}

	}

	// unfolding this would go faster with less GC
	static int MAX_PREFIX_LENGTH = 4;
	static List<String> prefixes(String s) {
		int numPrefixes = Math.min(MAX_PREFIX_LENGTH,s.length());
		if (numPrefixes == 0)
			return Collections.emptyList();
		if (numPrefixes == 1)
			return Collections.singletonList(s);
		List<String> result = new ArrayList<String>(numPrefixes);
		for (int i = 1; i <= Math.min(MAX_PREFIX_LENGTH,s.length()); ++i)
			result.add(s.substring(0,i));
		return result;
	}

	// unfolding this would go faster with less GC
	static int MAX_SUFFIX_LENGTH = 4;
	static List<String> suffixes(String s) {
		int numSuffixes = Math.min(s.length(), MAX_SUFFIX_LENGTH);
		if (numSuffixes <= 0)
			return Collections.emptyList();
		if (numSuffixes == 1)
			return Collections.singletonList(s);
		List<String> result = new ArrayList<String>(numSuffixes);
		for (int i = s.length() - numSuffixes; i < s.length(); ++i)
			result.add(s.substring(i));
		return result;
	}

	public static void main(String[] args) throws ClassNotFoundException, IOException {
		Corpus<ObjectHandler<Tagging<String>>> corpus = new TinyPosCorpus();
		final ChainCrfFeatureExtractor<String> featureExtractor = new FancyCrfFeatureExtractor();
		corpus.visitTrain(new ObjectHandler<Tagging<String>> () {
			@Override
			public void handle(Tagging<String> tagging) {
				ChainCrfFeatures<String> features = featureExtractor.extract(tagging.tokens(),tagging.tags());
				for (int i = 0; i < tagging.size(); ++i) {
					System.out.println("-------------------");
					System.out.println("Tagging:  " + tagging.token(i) + "/" + tagging.tag(i));
					System.out.print("Node Feats:" + features.nodeFeatures(i));
					if (i > 0) {
						System.out.println("Edge Feats:" + features.edgeFeatures(i, i -1));
					}
				}
			}

		});
	}
}


package com.lingpipe.cookbook.chapter5;


import com.aliasi.io.FileExtensionFilter;

import com.aliasi.chunk.Chunking;
import com.aliasi.chunk.HmmChunker;
import com.aliasi.corpus.ListCorpus;
import com.aliasi.corpus.Parser;
import com.aliasi.corpus.ObjectHandler;
import com.aliasi.corpus.XValidatingObjectCorpus;

import com.aliasi.hmm.HmmCharLmEstimator;
import com.aliasi.hmm.HmmDecoder;

import com.aliasi.tag.Tagging;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;

import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.Files;
import com.aliasi.util.Strings;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.ObjectOutputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

public class HmmChunkingSentenceDetector {

	static Tagging<String> getTagging(TokenizerFactory tokenizerFactory, char[] text) {
		Tokenizer tokenizer = tokenizerFactory.tokenizer(text, 0, text.length);
		List<String> tokens = new ArrayList<String>();
		List<String> tags = new ArrayList<String>();
		String token = null;
		boolean bosFound = false;
		while ((token = tokenizer.nextToken()) != null) {
			if (token.equals("[")) {
				bosFound = true;
			}
			else if (token.equals("]")) {
				tags.set(tags.size() - 1,"E_S");
			}
			else {
				tokens.add(token);
				if (bosFound) {
					tags.add("B_S");
					bosFound = false;
				}
				else {
					tags.add("M_S");
				}
			}
		}
		return new Tagging<String>(tokens,tags);
	}
	
	public static void main(String[] args) throws IOException {
		char[] text = Files.readCharsFromFile(new File(args[0]), Strings.UTF8);
		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		Tagging<String> tagging = getTagging(tokenizerFactory,text);
		System.out.println("Training Tagging " + tagging);		
		ListCorpus<Tagging<String>> corpus
			= new ListCorpus<Tagging<String>> ();
		corpus.addTrain(tagging);
		HmmCharLmEstimator estimator
			= new HmmCharLmEstimator();
		corpus.visitTrain(estimator);
		System.out.println("done training " + estimator.numTrainingTokens());
		HmmDecoder decoder = new HmmDecoder(estimator);
		HmmChunker chunker = new HmmChunker(tokenizerFactory, decoder);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.print("Enter text followed by new line\n>");
			String evalText = reader.readLine();
			List<String> evalTokens = Arrays.asList(tokenizerFactory.tokenizer(evalText.toCharArray(),0,evalText.length()).tokenize());
			Tagging<String> evalTagging = decoder.tag(evalTokens);
			System.out.println(evalTagging);
			
			Chunking chunking = chunker.chunk(evalText);
			System.out.println("Chunking: " + chunking);
		}
	}
}
package com.lingpipe.cookbook.chapter5;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.text.DecimalFormat;
import java.util.Iterator;

import com.aliasi.chunk.CharLmRescoringChunker;
import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.NBestChunker;
import com.aliasi.chunk.RescoringChunker;
import com.aliasi.hmm.HmmDecoder;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.Strings;


public class HmmNeChunker {

	static void trainHMMChunker(String modelFilename, String trainFilename) throws IOException{
		File modelFile = new File(modelFilename);
		File trainFile = new File(trainFilename);
		int numChunkingsRescored = 64;
		int maxNgram = 12;
		int numChars = 256;
		double lmInterpolation = maxNgram; 
		TokenizerFactory factory
			= IndoEuropeanTokenizerFactory.INSTANCE;
		CharLmRescoringChunker chunkerEstimator
			= new CharLmRescoringChunker(factory,numChunkingsRescored,
				maxNgram,numChars,
				lmInterpolation);
		Conll2002ChunkTagParser parser = new Conll2002ChunkTagParser();
		parser.setHandler(chunkerEstimator);
		parser.parse(trainFile);
		AbstractExternalizable.compileTo(chunkerEstimator,modelFile);
	}

	public static void main(String[] args) throws IOException, ClassNotFoundException {
		String modelFilename = "models/Conll2002_ESP.RescoringChunker";
		String trainFilename = "data/ner/data/esp.train";
		
		File modelFile = new File(modelFilename);
		if(!modelFile.exists()){
			System.out.println("Training HMM Chunker on data from: " + trainFilename);
			trainHMMChunker(modelFilename, trainFilename);
			System.out.println("Output written to : " + modelFilename);
		}
		@SuppressWarnings("unchecked")
		RescoringChunker<CharLmRescoringChunker> chunker 
			= (RescoringChunker<CharLmRescoringChunker>) AbstractExternalizable.readObject(modelFile);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
	    String text = "";
	    int MAX_N_BEST_CHUNKS = 10;
	    while (true) {
	    	System.out.println("Enter text, . to quit:");
	    	text = reader.readLine();
	    	
	    	if(text.equals(".")){
	    		break;
	    	}
	    	char[] cs = text.toCharArray();
	        Iterator<Chunk> it = chunker.nBestChunks(cs,0,cs.length,MAX_N_BEST_CHUNKS);
	        System.out.println(text);
	        System.out.println("Rank          Conf      Span    Type     Phrase");
	        DecimalFormat df = new DecimalFormat("0.0000");
	        for (int n = 0; it.hasNext(); ++n) {
	            Chunk chunk = it.next();
	            double conf = chunk.score();
	            int start = chunk.start();
	            int end = chunk.end();
	            String phrase = text.substring(start,end);
	            
	            System.out.println(n + " "
	            					+ "            " + df.format(conf)
	                               + "       (" + start
	                               + ", " + end
	                               + ")       " + chunk.type()
	                               + "         " + phrase);
	         }
	    	
	    }

	}






} 

package com.lingpipe.cookbook.chapter5;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import javax.management.RuntimeErrorException;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.ChunkAndCharSeq;
import com.aliasi.chunk.ChunkFactory;
import com.aliasi.chunk.ChunkingEvaluation;
import com.aliasi.chunk.ChunkingImpl;
import com.aliasi.sentences.IndoEuropeanSentenceModel;
import com.aliasi.sentences.SentenceChunker;
import com.aliasi.sentences.SentenceEvaluation;
import com.aliasi.sentences.SentenceEvaluator;
import com.aliasi.sentences.SentenceModel;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.util.Files;
import com.aliasi.util.Strings;

public class ModifiedSentenceDetector {

	public static void main(String[] args) throws IOException {

		char[] chars = Files.readCharsFromFile(new File(args[0]), Strings.UTF8);
		StringBuilder rawChars = new StringBuilder();
		int sentStart = -1;
		int sentEnd = -1;
		Set<Chunk> sentChunks = new HashSet<Chunk>();
		for (int i=0; i < chars.length; ++i) {
			if (chars[i] == '[') {
				sentStart = rawChars.length();
			}
			else if (chars[i] == ']') {
				sentEnd = rawChars.length();
				sentChunks.add(ChunkFactory.createChunk(sentStart,sentEnd,SentenceChunker.SENTENCE_CHUNK_TYPE));
			}
			else {
				rawChars.append(chars[i]);
			}
		}
		String unannotatedText = rawChars.toString();
		ChunkingImpl sentChunking = new ChunkingImpl(unannotatedText);
		for (Chunk chunk : sentChunks) {
			sentChunking.add(chunk);
		}
		boolean eosIsSentBoundary = false;
		boolean balanceParens = true;
		SentenceModel sentenceModel 
			//= new IndoEuropeanSentenceModel(eosIsSentBoundary,balanceParens);
			= new MySentenceModel();
		SentenceChunker sentenceChunker 
			= new SentenceChunker(IndoEuropeanTokenizerFactory.INSTANCE,sentenceModel);	

		SentenceEvaluator evaluator = new SentenceEvaluator(sentenceChunker);
		evaluator.handle(sentChunking);
		SentenceEvaluation eval = evaluator.evaluation();
		ChunkingEvaluation chunkEval = eval.chunkingEvaluation();
		for (ChunkAndCharSeq truePos : chunkEval.truePositiveSet()) {
			System.out.println("TruePos: " + truePos);
		}
		for (ChunkAndCharSeq falsePos : chunkEval.falsePositiveSet()) {
			System.out.println("FalsePos: " + falsePos);
		}
		for (ChunkAndCharSeq falseNeg : chunkEval.falseNegativeSet()){
			System.out.println("FalseNeg: " + falseNeg);
		}		
	}
}
package com.lingpipe.cookbook.chapter5;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.HashSet;
import java.util.Set;
import java.util.TreeSet;
import java.util.regex.Pattern;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.Chunking;
import com.aliasi.chunk.RegExChunker;
import com.aliasi.dict.DictionaryEntry;
import com.aliasi.dict.ExactDictionaryChunker;
import com.aliasi.dict.MapDictionary;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.util.AbstractExternalizable;


public class MultipleNer {

	/**
	 * @param args
	 */
	static final double CHUNK_SCORE = 1.0;
	public static void main(String[] args) throws IOException, ClassNotFoundException {
		Chunker pronounChunker = new RegExChunker(" He | he | Him | him ","MALE_PRONOUN",1.0);
		File MODEL_FILE
		= new File("models/ne-en-news-muc6.AbstractCharLmRescoringChunker");
		Chunker neChunker 
		= (Chunker) AbstractExternalizable.readObject(MODEL_FILE);
		
		MapDictionary<String> dictionary = new MapDictionary<String>();
		dictionary.addEntry(new DictionaryEntry<String>("Obama","PRESIDENT",CHUNK_SCORE));
		dictionary.addEntry(new DictionaryEntry<String>("Bush","PRESIDENT",CHUNK_SCORE));
		dictionary.addEntry(new DictionaryEntry<String>("Clinton","PRESIDENT",CHUNK_SCORE));
		dictionary.addEntry(new DictionaryEntry<String>("Reagan","PRESIDENT",CHUNK_SCORE));

		
		ExactDictionaryChunker dictionaryChunker
		= new ExactDictionaryChunker(dictionary,
				IndoEuropeanTokenizerFactory.INSTANCE,
				false,true);
		String text = "Arthur and Ford went up to Zaphod and asked him how the Heart of Gold works. As usual he didnt answer";
		//text = "Arthur and Ford met Jane";
		System.out.println(text);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
	    //String text = "";
		while (true) {
	    	System.out.println("Enter text, . to quit:");
	    	text = reader.readLine();
	    	if(text.equals(".")){
	    		break;
	    	}
	    	Set<Chunk> neChunking = neChunker.chunk(text).chunkSet();
			System.out.println("neChunking: " + neChunking);
			Set<Chunk> pChunking = pronounChunker.chunk(text).chunkSet();
			System.out.println("pChunking: " + pChunking);
			Set<Chunk> dChunking = dictionaryChunker.chunk(text).chunkSet();
			System.out.println("dChunking: " + dChunking);
			
			Set<Chunk> allChunks = new HashSet<Chunk>();
			allChunks.addAll(neChunking);
			allChunks.addAll(pChunking);
			allChunks.addAll(dChunking);
			System.out.println("----Overlaps Allowed");
			getCombinedChunks(allChunks,true);
			System.out.println("\n----Overlaps Not Allowed");
			getCombinedChunks(allChunks,false);
	    	
		}
		
	}

	static void getCombinedChunks(Set<Chunk> chunkSet, boolean allowOverlap){
		Set<Chunk> combinedChunks = new HashSet<Chunk>();
		Set<Chunk>overLappedChunks = new HashSet<Chunk>();
		for(Chunk c : chunkSet){
			combinedChunks.add(c);
			for(Chunk x : chunkSet){
				if(c.equals(x)){
					continue;
				}
				boolean debug = false;
				if(debug){
					System.out.println("C: " + c);
					System.out.println("X: " + x);
					System.out.println("Overlap: " + overlap(c,x));
				}
				if (overlap(c,x)) {
					if (allowOverlap){
						combinedChunks.add(x);
					} else {
						overLappedChunks.add(x);
						combinedChunks.remove(c);
					}
				} 
			}
		}
		if(allowOverlap){
			System.out.println("\n Combined Chunks:");
			System.out.println(combinedChunks);
		} else {
			System.out.println("\n Unique Chunks:");
			System.out.println(combinedChunks);
			System.out.println("\n OverLapped Chunks:");
			System.out.println(overLappedChunks);
		}
		
		
	}
		
		

	
	static boolean overlap(Chunk c1, Chunk c2){
		return overlap(c1.start(),c1.end(),c2.start(),c2.end());
	}
	
	static boolean overlap(int start1, int end1,
			int start2, int end2) {
		return java.lang.Math.max(start1,start2)
				< java.lang.Math.min(end1,end2);
	}
	
}
package com.lingpipe.cookbook.chapter5;

import com.aliasi.sentences.HeuristicSentenceModel;
import com.aliasi.util.AbstractExternalizable;

import java.io.IOException;
import java.io.ObjectInput;
import java.io.ObjectOutput;
import java.io.Serializable;

import java.util.HashSet;
import java.util.Set;


public class MySentenceModel 
    extends HeuristicSentenceModel 
    implements Serializable {

    static final long serialVersionUID = 7971853880414958463L;

    /**
     * Construct an Indo-European sentence model that does
     * not force the final token to be a stop and does not
     * balance parentheses.
     */
    public MySentenceModel() {
        this(false,false);
    }

    /**
     * Construct an Indo-European sentence model that forces final
     * tokens and balances parentheses according to the specified
     * flags.
     *
     * @param forceFinalToken Whether the final token is always a
     * sentence stop.
     * @param balanceParentheses Whether sentences can stop if not all
     * open parentheses have been closed.
     *
     */
    public MySentenceModel(boolean forceFinalToken,
                                     boolean balanceParentheses) {
        super(POSSIBLE_STOPS,
              IMPOSSIBLE_PENULTIMATES,
              IMPOSSIBLE_STARTS,
              forceFinalToken,balanceParentheses);
    }

    Object writeReplace() {
        return new Serializer(this);
    }

    private static final Set<String> POSSIBLE_STOPS = new HashSet<String>();
    static {
        POSSIBLE_STOPS.add(".");
        POSSIBLE_STOPS.add("..");  // abbrev + stop occurs
        POSSIBLE_STOPS.add("!");
        POSSIBLE_STOPS.add("?");
        POSSIBLE_STOPS.add("\"");
        POSSIBLE_STOPS.add("''");
        POSSIBLE_STOPS.add(").");
        POSSIBLE_STOPS.add("\u00BB"); // french close quote
        POSSIBLE_STOPS.add(">>"); // french close quote
    }

    private static final Set<String> IMPOSSIBLE_PENULTIMATES
        = new HashSet<String>();
    static {
        IMPOSSIBLE_PENULTIMATES.add(",");
        IMPOSSIBLE_PENULTIMATES.add(":");
        IMPOSSIBLE_PENULTIMATES.add("''");
        
        // Single letters; typically middle initials or parts of acronyms
        IMPOSSIBLE_PENULTIMATES.add("A");
        IMPOSSIBLE_PENULTIMATES.add("B");
        IMPOSSIBLE_PENULTIMATES.add("C");
        IMPOSSIBLE_PENULTIMATES.add("D");
        IMPOSSIBLE_PENULTIMATES.add("E");
        IMPOSSIBLE_PENULTIMATES.add("F");
        IMPOSSIBLE_PENULTIMATES.add("G");
        IMPOSSIBLE_PENULTIMATES.add("H");
        IMPOSSIBLE_PENULTIMATES.add("I");
        IMPOSSIBLE_PENULTIMATES.add("J");
        IMPOSSIBLE_PENULTIMATES.add("K");
        IMPOSSIBLE_PENULTIMATES.add("L");
        IMPOSSIBLE_PENULTIMATES.add("M");
        IMPOSSIBLE_PENULTIMATES.add("N");
        IMPOSSIBLE_PENULTIMATES.add("O");
        IMPOSSIBLE_PENULTIMATES.add("P");
        IMPOSSIBLE_PENULTIMATES.add("Q");
        IMPOSSIBLE_PENULTIMATES.add("R");
        //IMPOSSIBLE_PENULTIMATES.add("S"); breaks on "people's."
        //IMPOSSIBLE_PENULTIMATES.add("T"); breaks on "didn't."
        IMPOSSIBLE_PENULTIMATES.add("U");
        IMPOSSIBLE_PENULTIMATES.add("V");
        IMPOSSIBLE_PENULTIMATES.add("W");
        IMPOSSIBLE_PENULTIMATES.add("X");
        IMPOSSIBLE_PENULTIMATES.add("Y");
        IMPOSSIBLE_PENULTIMATES.add("Z");

        // Common Abbrevs
        IMPOSSIBLE_PENULTIMATES.add("Bros");
        IMPOSSIBLE_PENULTIMATES.add("No");  // too common ??
        IMPOSSIBLE_PENULTIMATES.add("vs");
        IMPOSSIBLE_PENULTIMATES.add("etc");
        IMPOSSIBLE_PENULTIMATES.add("Fig"); // thanks to MM

        // French Abbrevs:
        IMPOSSIBLE_PENULTIMATES.add("T\u00E9l");  // e + accent aigu
        IMPOSSIBLE_PENULTIMATES.add("t\u00E9l");

        // Directional Abbrevs
        IMPOSSIBLE_PENULTIMATES.add("NE");
        IMPOSSIBLE_PENULTIMATES.add("N.E");
        IMPOSSIBLE_PENULTIMATES.add("NW");
        IMPOSSIBLE_PENULTIMATES.add("N.W");
        IMPOSSIBLE_PENULTIMATES.add("SE");
        IMPOSSIBLE_PENULTIMATES.add("S.E");
        IMPOSSIBLE_PENULTIMATES.add("SW");
        IMPOSSIBLE_PENULTIMATES.add("S.W");


        // Personal Honorifics
        IMPOSSIBLE_PENULTIMATES.add("Mr");
        IMPOSSIBLE_PENULTIMATES.add("Mrs");
	IMPOSSIBLE_PENULTIMATES.add("Ms");
	IMPOSSIBLE_PENULTIMATES.add("MM");
        IMPOSSIBLE_PENULTIMATES.add("Mssrs");
        IMPOSSIBLE_PENULTIMATES.add("Messrs");

        // Professional Honorifics
        IMPOSSIBLE_PENULTIMATES.add("Dr");
        IMPOSSIBLE_PENULTIMATES.add("Gov");
        IMPOSSIBLE_PENULTIMATES.add("Hon");
        IMPOSSIBLE_PENULTIMATES.add("Rev");
        IMPOSSIBLE_PENULTIMATES.add("Pres");
        IMPOSSIBLE_PENULTIMATES.add("Prof");
        IMPOSSIBLE_PENULTIMATES.add("Ph.D");
        IMPOSSIBLE_PENULTIMATES.add("Ph");
        IMPOSSIBLE_PENULTIMATES.add("Rep");
        IMPOSSIBLE_PENULTIMATES.add("Reps");
        IMPOSSIBLE_PENULTIMATES.add("Rev");
        IMPOSSIBLE_PENULTIMATES.add("Sen");
        IMPOSSIBLE_PENULTIMATES.add("Sens");


        // Name Suffixes
        IMPOSSIBLE_PENULTIMATES.add("Jr");
        IMPOSSIBLE_PENULTIMATES.add("Sr");

        // Military Ranks
        IMPOSSIBLE_PENULTIMATES.add("PFC");
        IMPOSSIBLE_PENULTIMATES.add("Cpl");
        IMPOSSIBLE_PENULTIMATES.add("Sgt");
        IMPOSSIBLE_PENULTIMATES.add("Lt");
        IMPOSSIBLE_PENULTIMATES.add("Lieut");
        IMPOSSIBLE_PENULTIMATES.add("Capt");
        IMPOSSIBLE_PENULTIMATES.add("Cpt");
        IMPOSSIBLE_PENULTIMATES.add("Maj");
        IMPOSSIBLE_PENULTIMATES.add("Gen");
        IMPOSSIBLE_PENULTIMATES.add("Col");
        IMPOSSIBLE_PENULTIMATES.add("Cmdr");
        IMPOSSIBLE_PENULTIMATES.add("Adm");

        IMPOSSIBLE_PENULTIMATES.add("Col");

        // Corporate Designators
        IMPOSSIBLE_PENULTIMATES.add("Co");
        IMPOSSIBLE_PENULTIMATES.add("Corp");
        IMPOSSIBLE_PENULTIMATES.add("Inc");
        IMPOSSIBLE_PENULTIMATES.add("Ltd");

        // Month Abbrevs
        IMPOSSIBLE_PENULTIMATES.add("Jan");
        IMPOSSIBLE_PENULTIMATES.add("Feb");
        IMPOSSIBLE_PENULTIMATES.add("Mar");
        IMPOSSIBLE_PENULTIMATES.add("Apr");
        IMPOSSIBLE_PENULTIMATES.add("Jun");
        IMPOSSIBLE_PENULTIMATES.add("Jul");
        IMPOSSIBLE_PENULTIMATES.add("Aug");
        IMPOSSIBLE_PENULTIMATES.add("Sep");
        IMPOSSIBLE_PENULTIMATES.add("Sept");
        IMPOSSIBLE_PENULTIMATES.add("Oct");
        IMPOSSIBLE_PENULTIMATES.add("Nov");
        IMPOSSIBLE_PENULTIMATES.add("Dec");

        // Location Suffixes
        IMPOSSIBLE_PENULTIMATES.add("St");

        // Political Parties
        IMPOSSIBLE_PENULTIMATES.add("Rep");
        IMPOSSIBLE_PENULTIMATES.add("Dem");

        // Politicians
        IMPOSSIBLE_PENULTIMATES.add("Atty");

        // State Names - Post Office
        // Source: http://www.usps.com/ncsc/lookups/usps_abbreviations.html#states
        IMPOSSIBLE_PENULTIMATES.add("AL");
        IMPOSSIBLE_PENULTIMATES.add("AK");
        IMPOSSIBLE_PENULTIMATES.add("AS");
        IMPOSSIBLE_PENULTIMATES.add("AZ");
        IMPOSSIBLE_PENULTIMATES.add("AR");
        IMPOSSIBLE_PENULTIMATES.add("CA");
        IMPOSSIBLE_PENULTIMATES.add("CO");
        IMPOSSIBLE_PENULTIMATES.add("CT");
        IMPOSSIBLE_PENULTIMATES.add("DE");
        IMPOSSIBLE_PENULTIMATES.add("DC");
        IMPOSSIBLE_PENULTIMATES.add("FM");
        IMPOSSIBLE_PENULTIMATES.add("FL");
        IMPOSSIBLE_PENULTIMATES.add("GA");
        IMPOSSIBLE_PENULTIMATES.add("GU");
        IMPOSSIBLE_PENULTIMATES.add("HI");
        IMPOSSIBLE_PENULTIMATES.add("ID");
        IMPOSSIBLE_PENULTIMATES.add("IL");
        // IMPOSSIBLE_PENULTIMATES.add("IN"); too common
        IMPOSSIBLE_PENULTIMATES.add("IA");
        IMPOSSIBLE_PENULTIMATES.add("KS");
        IMPOSSIBLE_PENULTIMATES.add("KY");
        IMPOSSIBLE_PENULTIMATES.add("LA");
        // IMPOSSIBLE_PENULTIMATES.add("ME");  too common
        IMPOSSIBLE_PENULTIMATES.add("MH");
        IMPOSSIBLE_PENULTIMATES.add("MD");
        IMPOSSIBLE_PENULTIMATES.add("MA");
        IMPOSSIBLE_PENULTIMATES.add("MI");
        IMPOSSIBLE_PENULTIMATES.add("MN");
        IMPOSSIBLE_PENULTIMATES.add("MS");
        IMPOSSIBLE_PENULTIMATES.add("MO");
        IMPOSSIBLE_PENULTIMATES.add("MT");
        IMPOSSIBLE_PENULTIMATES.add("NE");
        IMPOSSIBLE_PENULTIMATES.add("NV");
        IMPOSSIBLE_PENULTIMATES.add("NH");
        IMPOSSIBLE_PENULTIMATES.add("NJ");
        IMPOSSIBLE_PENULTIMATES.add("NM");
        IMPOSSIBLE_PENULTIMATES.add("NY");
        IMPOSSIBLE_PENULTIMATES.add("NC");
        IMPOSSIBLE_PENULTIMATES.add("ND");
        IMPOSSIBLE_PENULTIMATES.add("MP");
        IMPOSSIBLE_PENULTIMATES.add("OH");
        IMPOSSIBLE_PENULTIMATES.add("OK");
        IMPOSSIBLE_PENULTIMATES.add("OR");
        IMPOSSIBLE_PENULTIMATES.add("PW");
        IMPOSSIBLE_PENULTIMATES.add("PA");
        IMPOSSIBLE_PENULTIMATES.add("PR");
        IMPOSSIBLE_PENULTIMATES.add("RI");
        IMPOSSIBLE_PENULTIMATES.add("SC");
        IMPOSSIBLE_PENULTIMATES.add("SD");
        IMPOSSIBLE_PENULTIMATES.add("TN");
        IMPOSSIBLE_PENULTIMATES.add("TX");
        IMPOSSIBLE_PENULTIMATES.add("UT");
        IMPOSSIBLE_PENULTIMATES.add("VT");
        IMPOSSIBLE_PENULTIMATES.add("VI");
        IMPOSSIBLE_PENULTIMATES.add("VA");
        IMPOSSIBLE_PENULTIMATES.add("WA");
        IMPOSSIBLE_PENULTIMATES.add("WV");
        IMPOSSIBLE_PENULTIMATES.add("WI");
        IMPOSSIBLE_PENULTIMATES.add("WY");

        // shipping terms
        IMPOSSIBLE_PENULTIMATES.add("f.o.b");
        IMPOSSIBLE_PENULTIMATES.add("c.i.f");
        IMPOSSIBLE_PENULTIMATES.add("fob");
        IMPOSSIBLE_PENULTIMATES.add("cif");

        // times
        IMPOSSIBLE_PENULTIMATES.add("A.M");
        IMPOSSIBLE_PENULTIMATES.add("P.M");


        // state names - Chicago Manual & AP
        // source: www.nyu.edu/classes/copyXediting/STABBREV.html
        IMPOSSIBLE_PENULTIMATES.add("Ala");
        IMPOSSIBLE_PENULTIMATES.add("Ariz");
        IMPOSSIBLE_PENULTIMATES.add("Ark");
        IMPOSSIBLE_PENULTIMATES.add("Calif");
        IMPOSSIBLE_PENULTIMATES.add("Colo");
        IMPOSSIBLE_PENULTIMATES.add("Conn");
        IMPOSSIBLE_PENULTIMATES.add("Del");
        IMPOSSIBLE_PENULTIMATES.add("D.C");
        IMPOSSIBLE_PENULTIMATES.add("Fla");
        IMPOSSIBLE_PENULTIMATES.add("Ga");
        IMPOSSIBLE_PENULTIMATES.add("Ill");
        IMPOSSIBLE_PENULTIMATES.add("Ind");
        IMPOSSIBLE_PENULTIMATES.add("Kan");
        IMPOSSIBLE_PENULTIMATES.add("Kans");
        IMPOSSIBLE_PENULTIMATES.add("Ky");
        IMPOSSIBLE_PENULTIMATES.add("Md");
        IMPOSSIBLE_PENULTIMATES.add("Mass");
        IMPOSSIBLE_PENULTIMATES.add("Mich");
        IMPOSSIBLE_PENULTIMATES.add("Minn");
        IMPOSSIBLE_PENULTIMATES.add("Miss");
        IMPOSSIBLE_PENULTIMATES.add("Mo");
        IMPOSSIBLE_PENULTIMATES.add("Mont");
        IMPOSSIBLE_PENULTIMATES.add("Neb");
        IMPOSSIBLE_PENULTIMATES.add("Nebr");
        IMPOSSIBLE_PENULTIMATES.add("Nev");
        IMPOSSIBLE_PENULTIMATES.add("N.H");
        IMPOSSIBLE_PENULTIMATES.add("N.J");
        IMPOSSIBLE_PENULTIMATES.add("N.M");
        IMPOSSIBLE_PENULTIMATES.add("N.Mex");
        IMPOSSIBLE_PENULTIMATES.add("N.Y");
        IMPOSSIBLE_PENULTIMATES.add("N.C");
        IMPOSSIBLE_PENULTIMATES.add("N.Dak");
        IMPOSSIBLE_PENULTIMATES.add("Okla");
        IMPOSSIBLE_PENULTIMATES.add("Ore");
        IMPOSSIBLE_PENULTIMATES.add("Oreg");
        IMPOSSIBLE_PENULTIMATES.add("Pa");
        IMPOSSIBLE_PENULTIMATES.add("R.I");
        IMPOSSIBLE_PENULTIMATES.add("S.C");
        IMPOSSIBLE_PENULTIMATES.add("S.Dak");
        IMPOSSIBLE_PENULTIMATES.add("Tenn");
        IMPOSSIBLE_PENULTIMATES.add("Tex");
        IMPOSSIBLE_PENULTIMATES.add("Tx");
        IMPOSSIBLE_PENULTIMATES.add("Vt");
        IMPOSSIBLE_PENULTIMATES.add("Va");
        IMPOSSIBLE_PENULTIMATES.add("Wash");
        IMPOSSIBLE_PENULTIMATES.add("W.Va");
        IMPOSSIBLE_PENULTIMATES.add("Wis");
        IMPOSSIBLE_PENULTIMATES.add("Wisc");
        IMPOSSIBLE_PENULTIMATES.add("Wyo");
        IMPOSSIBLE_PENULTIMATES.add("Wyom");
        IMPOSSIBLE_PENULTIMATES.add("Amer");
        IMPOSSIBLE_PENULTIMATES.add("C.Z");
        IMPOSSIBLE_PENULTIMATES.add("P.R");
        IMPOSSIBLE_PENULTIMATES.add("V.I");

        // Location suffixes
        // Source: http://www.usps.com/ncsc/lookups/usps_abbreviations.html#states
        IMPOSSIBLE_PENULTIMATES.add("ALY");
        IMPOSSIBLE_PENULTIMATES.add("ANEX");
        IMPOSSIBLE_PENULTIMATES.add("ANNX");
        IMPOSSIBLE_PENULTIMATES.add("ANX");
        IMPOSSIBLE_PENULTIMATES.add("ARC");
        IMPOSSIBLE_PENULTIMATES.add("AV");
        IMPOSSIBLE_PENULTIMATES.add("AVE");
        IMPOSSIBLE_PENULTIMATES.add("AVEN");
        IMPOSSIBLE_PENULTIMATES.add("AVN");
        IMPOSSIBLE_PENULTIMATES.add("BCH");
        IMPOSSIBLE_PENULTIMATES.add("BG");
        IMPOSSIBLE_PENULTIMATES.add("BGS");
        IMPOSSIBLE_PENULTIMATES.add("BLF");
        IMPOSSIBLE_PENULTIMATES.add("BLFS");
        IMPOSSIBLE_PENULTIMATES.add("BLVD");
        IMPOSSIBLE_PENULTIMATES.add("BND");
        IMPOSSIBLE_PENULTIMATES.add("BOT");
        IMPOSSIBLE_PENULTIMATES.add("BOUL");
        IMPOSSIBLE_PENULTIMATES.add("BR");
        IMPOSSIBLE_PENULTIMATES.add("BRG");
        IMPOSSIBLE_PENULTIMATES.add("BRK");
        IMPOSSIBLE_PENULTIMATES.add("BRKS");
        IMPOSSIBLE_PENULTIMATES.add("BRNCH");
        IMPOSSIBLE_PENULTIMATES.add("BURG");
        IMPOSSIBLE_PENULTIMATES.add("BURGS");
        IMPOSSIBLE_PENULTIMATES.add("BYP");
        IMPOSSIBLE_PENULTIMATES.add("BYPA");
        IMPOSSIBLE_PENULTIMATES.add("BYPS");
        IMPOSSIBLE_PENULTIMATES.add("BYU");
        IMPOSSIBLE_PENULTIMATES.add("CANYN");
        IMPOSSIBLE_PENULTIMATES.add("CEN");
        IMPOSSIBLE_PENULTIMATES.add("CENT");
        IMPOSSIBLE_PENULTIMATES.add("CIR");
        IMPOSSIBLE_PENULTIMATES.add("CIRC");
        IMPOSSIBLE_PENULTIMATES.add("CK");
        IMPOSSIBLE_PENULTIMATES.add("CLB");
        IMPOSSIBLE_PENULTIMATES.add("CLF");
        IMPOSSIBLE_PENULTIMATES.add("CLFS");
        IMPOSSIBLE_PENULTIMATES.add("CMN");
        IMPOSSIBLE_PENULTIMATES.add("CMP");
        IMPOSSIBLE_PENULTIMATES.add("CNTR");
        IMPOSSIBLE_PENULTIMATES.add("COR");
        IMPOSSIBLE_PENULTIMATES.add("CORS");
        IMPOSSIBLE_PENULTIMATES.add("CP");
        IMPOSSIBLE_PENULTIMATES.add("CPE");
        IMPOSSIBLE_PENULTIMATES.add("CR");
        IMPOSSIBLE_PENULTIMATES.add("CRCL");
        IMPOSSIBLE_PENULTIMATES.add("CRES");
        IMPOSSIBLE_PENULTIMATES.add("CRESCENT");
        IMPOSSIBLE_PENULTIMATES.add("CRK");
        IMPOSSIBLE_PENULTIMATES.add("CRSCNT");
        IMPOSSIBLE_PENULTIMATES.add("CRSE");
        IMPOSSIBLE_PENULTIMATES.add("CRSSNG");
        IMPOSSIBLE_PENULTIMATES.add("CRST");
        IMPOSSIBLE_PENULTIMATES.add("CRT");
        IMPOSSIBLE_PENULTIMATES.add("CSWY");
        IMPOSSIBLE_PENULTIMATES.add("CT");
        IMPOSSIBLE_PENULTIMATES.add("CTR");
        IMPOSSIBLE_PENULTIMATES.add("CTRS");
        IMPOSSIBLE_PENULTIMATES.add("CTS");
        IMPOSSIBLE_PENULTIMATES.add("CV");
        IMPOSSIBLE_PENULTIMATES.add("CVS");
        IMPOSSIBLE_PENULTIMATES.add("CYN");
        IMPOSSIBLE_PENULTIMATES.add("DIV");
        IMPOSSIBLE_PENULTIMATES.add("DL");
        IMPOSSIBLE_PENULTIMATES.add("DM");
        IMPOSSIBLE_PENULTIMATES.add("DR");
        IMPOSSIBLE_PENULTIMATES.add("DRS");
        IMPOSSIBLE_PENULTIMATES.add("DV");
        IMPOSSIBLE_PENULTIMATES.add("DVD");
        IMPOSSIBLE_PENULTIMATES.add("EST");
        IMPOSSIBLE_PENULTIMATES.add("ESTS");
        IMPOSSIBLE_PENULTIMATES.add("EXP");
        IMPOSSIBLE_PENULTIMATES.add("EXPR");
        IMPOSSIBLE_PENULTIMATES.add("EXPW");
        IMPOSSIBLE_PENULTIMATES.add("EXPY");
        IMPOSSIBLE_PENULTIMATES.add("EXT");
        IMPOSSIBLE_PENULTIMATES.add("EXTN");
        IMPOSSIBLE_PENULTIMATES.add("EXTNSN");
        IMPOSSIBLE_PENULTIMATES.add("EXTS");
        IMPOSSIBLE_PENULTIMATES.add("FLD");
        IMPOSSIBLE_PENULTIMATES.add("FLDS");
        IMPOSSIBLE_PENULTIMATES.add("FLS");
        IMPOSSIBLE_PENULTIMATES.add("FLT");
        IMPOSSIBLE_PENULTIMATES.add("FLTS");
        IMPOSSIBLE_PENULTIMATES.add("FRD");
        IMPOSSIBLE_PENULTIMATES.add("FRDS");
        IMPOSSIBLE_PENULTIMATES.add("FREEWY");
        IMPOSSIBLE_PENULTIMATES.add("FRG");
        IMPOSSIBLE_PENULTIMATES.add("FRGS");
        IMPOSSIBLE_PENULTIMATES.add("FRK");
        IMPOSSIBLE_PENULTIMATES.add("FRKS");
        IMPOSSIBLE_PENULTIMATES.add("FRRY");
        IMPOSSIBLE_PENULTIMATES.add("FRST");
        IMPOSSIBLE_PENULTIMATES.add("FRT");
        IMPOSSIBLE_PENULTIMATES.add("FRWAY");
        IMPOSSIBLE_PENULTIMATES.add("FRWY");
        IMPOSSIBLE_PENULTIMATES.add("FRY");
        IMPOSSIBLE_PENULTIMATES.add("FT");
        IMPOSSIBLE_PENULTIMATES.add("FWY");
        IMPOSSIBLE_PENULTIMATES.add("GARDN");
        IMPOSSIBLE_PENULTIMATES.add("GATEWY");
        IMPOSSIBLE_PENULTIMATES.add("GDN");
        IMPOSSIBLE_PENULTIMATES.add("GDNS");
        IMPOSSIBLE_PENULTIMATES.add("GLN");
        IMPOSSIBLE_PENULTIMATES.add("GLNS");
        IMPOSSIBLE_PENULTIMATES.add("GRDN");
        IMPOSSIBLE_PENULTIMATES.add("GRDNS");
        IMPOSSIBLE_PENULTIMATES.add("GRN");
        IMPOSSIBLE_PENULTIMATES.add("GRNS");
        IMPOSSIBLE_PENULTIMATES.add("GRV");
        IMPOSSIBLE_PENULTIMATES.add("GRVS");
        IMPOSSIBLE_PENULTIMATES.add("HARB");
        IMPOSSIBLE_PENULTIMATES.add("HBR");
        IMPOSSIBLE_PENULTIMATES.add("HGTS");
        IMPOSSIBLE_PENULTIMATES.add("HIWY");
        IMPOSSIBLE_PENULTIMATES.add("HL");
        IMPOSSIBLE_PENULTIMATES.add("HLLW");
        IMPOSSIBLE_PENULTIMATES.add("HLS");
        IMPOSSIBLE_PENULTIMATES.add("HT");
        IMPOSSIBLE_PENULTIMATES.add("HTS");
        IMPOSSIBLE_PENULTIMATES.add("HVN");
        IMPOSSIBLE_PENULTIMATES.add("HWAY");
        IMPOSSIBLE_PENULTIMATES.add("HWY");
        // IMPOSSIBLE_PENULTIMATES.add("IS"); // common
        IMPOSSIBLE_PENULTIMATES.add("ISLND");
        IMPOSSIBLE_PENULTIMATES.add("ISS");
        IMPOSSIBLE_PENULTIMATES.add("JCT");
        IMPOSSIBLE_PENULTIMATES.add("JCTN");
        IMPOSSIBLE_PENULTIMATES.add("JCTNS");
        IMPOSSIBLE_PENULTIMATES.add("JCTS");
        IMPOSSIBLE_PENULTIMATES.add("KNL");
        IMPOSSIBLE_PENULTIMATES.add("KY");
        IMPOSSIBLE_PENULTIMATES.add("KYS");
        IMPOSSIBLE_PENULTIMATES.add("LA");
        IMPOSSIBLE_PENULTIMATES.add("LCK");
        IMPOSSIBLE_PENULTIMATES.add("LCKS");
        IMPOSSIBLE_PENULTIMATES.add("LDG");
        IMPOSSIBLE_PENULTIMATES.add("LDGE");
        IMPOSSIBLE_PENULTIMATES.add("LF");
        IMPOSSIBLE_PENULTIMATES.add("LGT");
        IMPOSSIBLE_PENULTIMATES.add("LGTS");
        IMPOSSIBLE_PENULTIMATES.add("LK");
        IMPOSSIBLE_PENULTIMATES.add("LKS");
        IMPOSSIBLE_PENULTIMATES.add("LN");
        IMPOSSIBLE_PENULTIMATES.add("LNDG");
        IMPOSSIBLE_PENULTIMATES.add("LNDNG");
        IMPOSSIBLE_PENULTIMATES.add("MDW");
        IMPOSSIBLE_PENULTIMATES.add("MDWS");
        IMPOSSIBLE_PENULTIMATES.add("MISSN");
        IMPOSSIBLE_PENULTIMATES.add("ML");
        IMPOSSIBLE_PENULTIMATES.add("MLS");
        IMPOSSIBLE_PENULTIMATES.add("MNR");
        IMPOSSIBLE_PENULTIMATES.add("MNRS");
        IMPOSSIBLE_PENULTIMATES.add("MNT");
        IMPOSSIBLE_PENULTIMATES.add("MNTN");
        IMPOSSIBLE_PENULTIMATES.add("MNTNS");
        IMPOSSIBLE_PENULTIMATES.add("MSN");
        IMPOSSIBLE_PENULTIMATES.add("MSSN");
        IMPOSSIBLE_PENULTIMATES.add("MT");
        IMPOSSIBLE_PENULTIMATES.add("MTN");
        IMPOSSIBLE_PENULTIMATES.add("MTNS");
        IMPOSSIBLE_PENULTIMATES.add("MTWY");
        IMPOSSIBLE_PENULTIMATES.add("NCK");
        IMPOSSIBLE_PENULTIMATES.add("OPAS");
        IMPOSSIBLE_PENULTIMATES.add("ORCH");
        IMPOSSIBLE_PENULTIMATES.add("OVL");
        IMPOSSIBLE_PENULTIMATES.add("PK");
        IMPOSSIBLE_PENULTIMATES.add("PKWAY");
        IMPOSSIBLE_PENULTIMATES.add("PKWY");
        IMPOSSIBLE_PENULTIMATES.add("PKWYS");
        IMPOSSIBLE_PENULTIMATES.add("PKY");
        IMPOSSIBLE_PENULTIMATES.add("PL");
        IMPOSSIBLE_PENULTIMATES.add("PLN");
        IMPOSSIBLE_PENULTIMATES.add("PLNS");
        IMPOSSIBLE_PENULTIMATES.add("PLZ");
        IMPOSSIBLE_PENULTIMATES.add("PLZA");
        IMPOSSIBLE_PENULTIMATES.add("PNE");
        IMPOSSIBLE_PENULTIMATES.add("PNES");
        IMPOSSIBLE_PENULTIMATES.add("PR");
        IMPOSSIBLE_PENULTIMATES.add("PRK");
        IMPOSSIBLE_PENULTIMATES.add("PRR");
        IMPOSSIBLE_PENULTIMATES.add("PRT");
        IMPOSSIBLE_PENULTIMATES.add("PRTS");
        IMPOSSIBLE_PENULTIMATES.add("PSGE");
        IMPOSSIBLE_PENULTIMATES.add("PT");
        IMPOSSIBLE_PENULTIMATES.add("PTS");
        IMPOSSIBLE_PENULTIMATES.add("RAD");
        IMPOSSIBLE_PENULTIMATES.add("RADL");
        IMPOSSIBLE_PENULTIMATES.add("RAMP");
        IMPOSSIBLE_PENULTIMATES.add("RD");
        IMPOSSIBLE_PENULTIMATES.add("RDG");
        IMPOSSIBLE_PENULTIMATES.add("RDGE");
        IMPOSSIBLE_PENULTIMATES.add("RDGS");
        IMPOSSIBLE_PENULTIMATES.add("RDS");
        IMPOSSIBLE_PENULTIMATES.add("RIV");
        IMPOSSIBLE_PENULTIMATES.add("RIVR");
        IMPOSSIBLE_PENULTIMATES.add("RNCH");
        IMPOSSIBLE_PENULTIMATES.add("RNCHS");
        IMPOSSIBLE_PENULTIMATES.add("RPD");
        IMPOSSIBLE_PENULTIMATES.add("RPDS");
        IMPOSSIBLE_PENULTIMATES.add("RST");
        IMPOSSIBLE_PENULTIMATES.add("RTE");
        IMPOSSIBLE_PENULTIMATES.add("RVR");
        IMPOSSIBLE_PENULTIMATES.add("SHL");
        IMPOSSIBLE_PENULTIMATES.add("SHLS");
        IMPOSSIBLE_PENULTIMATES.add("SHR");
        IMPOSSIBLE_PENULTIMATES.add("SHRS");
        IMPOSSIBLE_PENULTIMATES.add("SKWY");
        IMPOSSIBLE_PENULTIMATES.add("SMT");
        IMPOSSIBLE_PENULTIMATES.add("SPG");
        IMPOSSIBLE_PENULTIMATES.add("SPGS");
        IMPOSSIBLE_PENULTIMATES.add("SPNG");
        IMPOSSIBLE_PENULTIMATES.add("SQ");
        IMPOSSIBLE_PENULTIMATES.add("SQR");
        IMPOSSIBLE_PENULTIMATES.add("SQRE");
        IMPOSSIBLE_PENULTIMATES.add("SQRS");
        IMPOSSIBLE_PENULTIMATES.add("SQS");
        IMPOSSIBLE_PENULTIMATES.add("SQU");
        IMPOSSIBLE_PENULTIMATES.add("ST");
        IMPOSSIBLE_PENULTIMATES.add("STA");
        IMPOSSIBLE_PENULTIMATES.add("STATN");
        IMPOSSIBLE_PENULTIMATES.add("STN");
        IMPOSSIBLE_PENULTIMATES.add("STR");
        IMPOSSIBLE_PENULTIMATES.add("STRA");
        IMPOSSIBLE_PENULTIMATES.add("STRM");
        IMPOSSIBLE_PENULTIMATES.add("STRT");
        IMPOSSIBLE_PENULTIMATES.add("STRVN");
        IMPOSSIBLE_PENULTIMATES.add("STS");
        IMPOSSIBLE_PENULTIMATES.add("TER");
        IMPOSSIBLE_PENULTIMATES.add("TERR");
        IMPOSSIBLE_PENULTIMATES.add("TPK");
        IMPOSSIBLE_PENULTIMATES.add("TPKE");
        IMPOSSIBLE_PENULTIMATES.add("TR");
        IMPOSSIBLE_PENULTIMATES.add("TRAK");
        IMPOSSIBLE_PENULTIMATES.add("TRCE");
        IMPOSSIBLE_PENULTIMATES.add("TRFY");
        IMPOSSIBLE_PENULTIMATES.add("TRK");
        IMPOSSIBLE_PENULTIMATES.add("TRKS");
        IMPOSSIBLE_PENULTIMATES.add("TRL");
        IMPOSSIBLE_PENULTIMATES.add("TRLS");
        IMPOSSIBLE_PENULTIMATES.add("TRNPK");
        IMPOSSIBLE_PENULTIMATES.add("TRPK");
        IMPOSSIBLE_PENULTIMATES.add("TRWY");
        IMPOSSIBLE_PENULTIMATES.add("TUNL");
        IMPOSSIBLE_PENULTIMATES.add("TUNLS");
        IMPOSSIBLE_PENULTIMATES.add("TUNNL");
        IMPOSSIBLE_PENULTIMATES.add("TURNPK");
        IMPOSSIBLE_PENULTIMATES.add("UN");
        IMPOSSIBLE_PENULTIMATES.add("UNS");
        IMPOSSIBLE_PENULTIMATES.add("UPAS");
        IMPOSSIBLE_PENULTIMATES.add("VDCT");
        IMPOSSIBLE_PENULTIMATES.add("VIA");
        IMPOSSIBLE_PENULTIMATES.add("VILL");
        IMPOSSIBLE_PENULTIMATES.add("VILLE");
        IMPOSSIBLE_PENULTIMATES.add("VILLG");
        IMPOSSIBLE_PENULTIMATES.add("VIS");
        IMPOSSIBLE_PENULTIMATES.add("VIST");
        IMPOSSIBLE_PENULTIMATES.add("VISTA");
        IMPOSSIBLE_PENULTIMATES.add("VL");
        IMPOSSIBLE_PENULTIMATES.add("VLG");
        IMPOSSIBLE_PENULTIMATES.add("VLGS");
        IMPOSSIBLE_PENULTIMATES.add("VLLY");
        IMPOSSIBLE_PENULTIMATES.add("VLY");
        IMPOSSIBLE_PENULTIMATES.add("VLYS");
        IMPOSSIBLE_PENULTIMATES.add("VST");
        IMPOSSIBLE_PENULTIMATES.add("VSTA");
        IMPOSSIBLE_PENULTIMATES.add("VW");
        IMPOSSIBLE_PENULTIMATES.add("VWS");
        IMPOSSIBLE_PENULTIMATES.add("WL");
        IMPOSSIBLE_PENULTIMATES.add("WLS");
        IMPOSSIBLE_PENULTIMATES.add("WY");
        IMPOSSIBLE_PENULTIMATES.add("XING");
        IMPOSSIBLE_PENULTIMATES.add("XRD");

        // corporate designators
        // source: http://www.ldc.upenn.edu/Catalog/CatalogList/LDC2001T02/resources.corp-desigs-ref-list
        IMPOSSIBLE_PENULTIMATES.add("AB");
        IMPOSSIBLE_PENULTIMATES.add("A.B");
        IMPOSSIBLE_PENULTIMATES.add("AE");
        IMPOSSIBLE_PENULTIMATES.add("A.E.");
        IMPOSSIBLE_PENULTIMATES.add("AENP");
        IMPOSSIBLE_PENULTIMATES.add("AG");
        IMPOSSIBLE_PENULTIMATES.add("AG&COKG");
        IMPOSSIBLE_PENULTIMATES.add("AG");
        IMPOSSIBLE_PENULTIMATES.add("Co");
        IMPOSSIBLE_PENULTIMATES.add("KG");
        IMPOSSIBLE_PENULTIMATES.add("AL");
        IMPOSSIBLE_PENULTIMATES.add("A/L");
        IMPOSSIBLE_PENULTIMATES.add("AMBA");
        IMPOSSIBLE_PENULTIMATES.add("A.M.B.A.");
        IMPOSSIBLE_PENULTIMATES.add("AO");
        IMPOSSIBLE_PENULTIMATES.add("A.O");
        IMPOSSIBLE_PENULTIMATES.add("APS");
        IMPOSSIBLE_PENULTIMATES.add("A&P");
        IMPOSSIBLE_PENULTIMATES.add("AS");
        IMPOSSIBLE_PENULTIMATES.add("AS");
        IMPOSSIBLE_PENULTIMATES.add("A.S");
        IMPOSSIBLE_PENULTIMATES.add("AS");
        IMPOSSIBLE_PENULTIMATES.add("A/S");
        IMPOSSIBLE_PENULTIMATES.add("A.S");
        IMPOSSIBLE_PENULTIMATES.add("AY");
        IMPOSSIBLE_PENULTIMATES.add("BA");
        IMPOSSIBLE_PENULTIMATES.add("B.A");
        IMPOSSIBLE_PENULTIMATES.add("BHD");
        IMPOSSIBLE_PENULTIMATES.add("BM");
        IMPOSSIBLE_PENULTIMATES.add("B.M");
        IMPOSSIBLE_PENULTIMATES.add("BSC");
        IMPOSSIBLE_PENULTIMATES.add("BV");
        IMPOSSIBLE_PENULTIMATES.add("B.V");
        IMPOSSIBLE_PENULTIMATES.add("BVBA");
        IMPOSSIBLE_PENULTIMATES.add("B.V.B.A");
        IMPOSSIBLE_PENULTIMATES.add("BVCV");
        IMPOSSIBLE_PENULTIMATES.add("B.V");
        IMPOSSIBLE_PENULTIMATES.add("C.V");
        IMPOSSIBLE_PENULTIMATES.add("B.V./C.V");
        IMPOSSIBLE_PENULTIMATES.add("CA");
        IMPOSSIBLE_PENULTIMATES.add("C.A");
        IMPOSSIBLE_PENULTIMATES.add("CA");
        IMPOSSIBLE_PENULTIMATES.add("Cia.");
        IMPOSSIBLE_PENULTIMATES.add("CDERL");
        IMPOSSIBLE_PENULTIMATES.add("CDERL");
        IMPOSSIBLE_PENULTIMATES.add("CV");
        IMPOSSIBLE_PENULTIMATES.add("C.V");
        IMPOSSIBLE_PENULTIMATES.add("CO");
        IMPOSSIBLE_PENULTIMATES.add("CORP");
        IMPOSSIBLE_PENULTIMATES.add("CPORA");
        IMPOSSIBLE_PENULTIMATES.add("CPT");
        IMPOSSIBLE_PENULTIMATES.add("EC");
        IMPOSSIBLE_PENULTIMATES.add("E.C");
        IMPOSSIBLE_PENULTIMATES.add("EG");
        IMPOSSIBLE_PENULTIMATES.add("EGMBH");
        IMPOSSIBLE_PENULTIMATES.add("EPE");
        IMPOSSIBLE_PENULTIMATES.add("E.P.E");
        IMPOSSIBLE_PENULTIMATES.add("GMBH");
        IMPOSSIBLE_PENULTIMATES.add("Ges.m.b.H");
        IMPOSSIBLE_PENULTIMATES.add("GBR");
        IMPOSSIBLE_PENULTIMATES.add("GGMBH");
        IMPOSSIBLE_PENULTIMATES.add("GGMBH");
        IMPOSSIBLE_PENULTIMATES.add("GMK");
        IMPOSSIBLE_PENULTIMATES.add("GM.K");
        IMPOSSIBLE_PENULTIMATES.add("G.M.B.H");
        IMPOSSIBLE_PENULTIMATES.add("CO,KG");
        IMPOSSIBLE_PENULTIMATES.add("GP");
        IMPOSSIBLE_PENULTIMATES.add("G.P");
        IMPOSSIBLE_PENULTIMATES.add("GSK");
        IMPOSSIBLE_PENULTIMATES.add("HF");
        IMPOSSIBLE_PENULTIMATES.add("H.F");
        IMPOSSIBLE_PENULTIMATES.add("HMIG");
        IMPOSSIBLE_PENULTIMATES.add("H.MIJ");
        IMPOSSIBLE_PENULTIMATES.add("H.MIG");
        IMPOSSIBLE_PENULTIMATES.add("HVER");
        IMPOSSIBLE_PENULTIMATES.add("H.VER");
        IMPOSSIBLE_PENULTIMATES.add("INC");
        // IMPOSSIBLE_PENULTIMATES.add("IS"); // too common
        IMPOSSIBLE_PENULTIMATES.add("I/S");
        IMPOSSIBLE_PENULTIMATES.add("KB");
        IMPOSSIBLE_PENULTIMATES.add("KG");
        IMPOSSIBLE_PENULTIMATES.add("KGAA");
        IMPOSSIBLE_PENULTIMATES.add("K.G.A.A");
        IMPOSSIBLE_PENULTIMATES.add("KK");
        IMPOSSIBLE_PENULTIMATES.add("KS");
        IMPOSSIBLE_PENULTIMATES.add("K/S");
        IMPOSSIBLE_PENULTIMATES.add("KY");
        IMPOSSIBLE_PENULTIMATES.add("LDA");
        IMPOSSIBLE_PENULTIMATES.add("LTD");
        IMPOSSIBLE_PENULTIMATES.add("LTDAPS");
        IMPOSSIBLE_PENULTIMATES.add("LLC");
        IMPOSSIBLE_PENULTIMATES.add("L.L.C");
        IMPOSSIBLE_PENULTIMATES.add("LP");
        IMPOSSIBLE_PENULTIMATES.add("L.P");
        IMPOSSIBLE_PENULTIMATES.add("LTDA");
        IMPOSSIBLE_PENULTIMATES.add("MIJ");
        IMPOSSIBLE_PENULTIMATES.add("NL");
        IMPOSSIBLE_PENULTIMATES.add("N.L");
        IMPOSSIBLE_PENULTIMATES.add("NPL");
        IMPOSSIBLE_PENULTIMATES.add("N.P.L");
        IMPOSSIBLE_PENULTIMATES.add("NV");
        IMPOSSIBLE_PENULTIMATES.add("N.V");
        IMPOSSIBLE_PENULTIMATES.add("OHG");
        IMPOSSIBLE_PENULTIMATES.add("OE");
        IMPOSSIBLE_PENULTIMATES.add("O.E");
        IMPOSSIBLE_PENULTIMATES.add("OY");
        IMPOSSIBLE_PENULTIMATES.add("OYAB");
        IMPOSSIBLE_PENULTIMATES.add("PERJAN");
        IMPOSSIBLE_PENULTIMATES.add("PERSERO");
        IMPOSSIBLE_PENULTIMATES.add("PERUM");
        IMPOSSIBLE_PENULTIMATES.add("PLC");
        IMPOSSIBLE_PENULTIMATES.add("PN");
        IMPOSSIBLE_PENULTIMATES.add("PP");
        IMPOSSIBLE_PENULTIMATES.add("PT");
        IMPOSSIBLE_PENULTIMATES.add("PTY");
        IMPOSSIBLE_PENULTIMATES.add("PTE");
        IMPOSSIBLE_PENULTIMATES.add("PVBA");
        IMPOSSIBLE_PENULTIMATES.add("P.V.B.A");
        IMPOSSIBLE_PENULTIMATES.add("SA");
        IMPOSSIBLE_PENULTIMATES.add("S.A");
        IMPOSSIBLE_PENULTIMATES.add("SA");
        IMPOSSIBLE_PENULTIMATES.add("SA");
        IMPOSSIBLE_PENULTIMATES.add("SAC");
        IMPOSSIBLE_PENULTIMATES.add("S.A.C");
        IMPOSSIBLE_PENULTIMATES.add("SACA");
        IMPOSSIBLE_PENULTIMATES.add("S.A.C.A");
        IMPOSSIBLE_PENULTIMATES.add("SACC");
        IMPOSSIBLE_PENULTIMATES.add("S.Acc");
        IMPOSSIBLE_PENULTIMATES.add("ACC");
        IMPOSSIBLE_PENULTIMATES.add("SACCPA");
        IMPOSSIBLE_PENULTIMATES.add("S.ACC.P.A");
        IMPOSSIBLE_PENULTIMATES.add("ACC.P.A");
        IMPOSSIBLE_PENULTIMATES.add("P.A");
        IMPOSSIBLE_PENULTIMATES.add("SACEI");
        IMPOSSIBLE_PENULTIMATES.add("S.A.");
        IMPOSSIBLE_PENULTIMATES.add("C.E.I");
        IMPOSSIBLE_PENULTIMATES.add("SACIF");
        IMPOSSIBLE_PENULTIMATES.add("S.A.C.I.F");
        IMPOSSIBLE_PENULTIMATES.add("SADECV");
        IMPOSSIBLE_PENULTIMATES.add("S.A.");
        IMPOSSIBLE_PENULTIMATES.add("C.V");
        IMPOSSIBLE_PENULTIMATES.add("SAIC");
        IMPOSSIBLE_PENULTIMATES.add("S.A.I.C");
        IMPOSSIBLE_PENULTIMATES.add("SAICA");
        IMPOSSIBLE_PENULTIMATES.add("S.A.I.C.A");
        IMPOSSIBLE_PENULTIMATES.add("SALC");
        IMPOSSIBLE_PENULTIMATES.add("S.A.L.C");
        IMPOSSIBLE_PENULTIMATES.add("SALC");
        IMPOSSIBLE_PENULTIMATES.add("SANV");
        IMPOSSIBLE_PENULTIMATES.add("S.A.N.V");
        IMPOSSIBLE_PENULTIMATES.add("SARL");
        IMPOSSIBLE_PENULTIMATES.add("S.A.R.L");
        IMPOSSIBLE_PENULTIMATES.add("SAS");
        IMPOSSIBLE_PENULTIMATES.add("S.A.S");
        IMPOSSIBLE_PENULTIMATES.add("SCI");
        IMPOSSIBLE_PENULTIMATES.add("S.C.I");
        IMPOSSIBLE_PENULTIMATES.add("SCl");
        IMPOSSIBLE_PENULTIMATES.add("S.C.L");
        IMPOSSIBLE_PENULTIMATES.add("SCP");
        IMPOSSIBLE_PENULTIMATES.add("S.C.P");
        IMPOSSIBLE_PENULTIMATES.add("SCPA");
        IMPOSSIBLE_PENULTIMATES.add("S.C.p.A");
        IMPOSSIBLE_PENULTIMATES.add("SCPDEG");
        IMPOSSIBLE_PENULTIMATES.add("S.C.P");
        IMPOSSIBLE_PENULTIMATES.add("SCRL");
        IMPOSSIBLE_PENULTIMATES.add("SDERL");
        IMPOSSIBLE_PENULTIMATES.add("R.L");
        IMPOSSIBLE_PENULTIMATES.add("R.L");
        IMPOSSIBLE_PENULTIMATES.add("SDERLDECV");
        IMPOSSIBLE_PENULTIMATES.add("C.V");
        IMPOSSIBLE_PENULTIMATES.add("SENC");
        IMPOSSIBLE_PENULTIMATES.add("SENCPORA");
        IMPOSSIBLE_PENULTIMATES.add("SEND");
        IMPOSSIBLE_PENULTIMATES.add("SICI");
        IMPOSSIBLE_PENULTIMATES.add("S.I.C.I");
        IMPOSSIBLE_PENULTIMATES.add("SL");
        IMPOSSIBLE_PENULTIMATES.add("S.L.");
        IMPOSSIBLE_PENULTIMATES.add("SMA");
        IMPOSSIBLE_PENULTIMATES.add("S.M.A");
        IMPOSSIBLE_PENULTIMATES.add("SMCP");
        IMPOSSIBLE_PENULTIMATES.add("S.M.C.P");
        IMPOSSIBLE_PENULTIMATES.add("SNC");
        IMPOSSIBLE_PENULTIMATES.add("S.N.C");
        IMPOSSIBLE_PENULTIMATES.add("SPA");
        IMPOSSIBLE_PENULTIMATES.add("S.P.A");
        IMPOSSIBLE_PENULTIMATES.add("SPRL");
        IMPOSSIBLE_PENULTIMATES.add("S.P.R.L");
        IMPOSSIBLE_PENULTIMATES.add("SRL");
        IMPOSSIBLE_PENULTIMATES.add("S.R.L");
        IMPOSSIBLE_PENULTIMATES.add("SV");
        IMPOSSIBLE_PENULTIMATES.add("SZRL");
        IMPOSSIBLE_PENULTIMATES.add("S.Z.R.L");
        IMPOSSIBLE_PENULTIMATES.add("TAS");
        IMPOSSIBLE_PENULTIMATES.add("T.A.S");
        IMPOSSIBLE_PENULTIMATES.add("UPA");
        IMPOSSIBLE_PENULTIMATES.add("U.p.a");
        IMPOSSIBLE_PENULTIMATES.add("UNIV");
        IMPOSSIBLE_PENULTIMATES.add("VN");
        IMPOSSIBLE_PENULTIMATES.add("WLL");
        IMPOSSIBLE_PENULTIMATES.add("W.L.L");
    }

    private static final Set<String> IMPOSSIBLE_STARTS
        = new HashSet<String>();
    static {
        IMPOSSIBLE_STARTS.add(",");
        IMPOSSIBLE_STARTS.add(")");
        IMPOSSIBLE_STARTS.add("]");
        IMPOSSIBLE_STARTS.add("}");
        IMPOSSIBLE_STARTS.add(">");
        IMPOSSIBLE_STARTS.add("<");
        IMPOSSIBLE_STARTS.add(".");
        IMPOSSIBLE_STARTS.add("!");
        IMPOSSIBLE_STARTS.add("?");
        IMPOSSIBLE_STARTS.add(":");
        IMPOSSIBLE_STARTS.add(";");
        IMPOSSIBLE_STARTS.add("-");
        IMPOSSIBLE_STARTS.add("--");
        IMPOSSIBLE_STARTS.add("---");
        IMPOSSIBLE_STARTS.add("%");
    }

    static class Serializer extends AbstractExternalizable {
        static final long serialVersionUID = 6764068662877106091L;
        final MySentenceModel mModel;
        public Serializer() {
            this(null);
        }
        public Serializer(MySentenceModel model) {
            mModel = model;
        }
        public void writeExternal(ObjectOutput out) throws IOException {
            out.writeBoolean(mModel.forceFinalStop());
            out.writeBoolean(mModel.balanceParens());
        }
        public Object read(ObjectInput in) 
            throws IOException {
            
            boolean forceFinalStop = in.readBoolean();
            boolean balanceParens = in.readBoolean();
            return new MySentenceModel(forceFinalStop,
                                                 balanceParens);
        }
    }

}
package com.lingpipe.cookbook.chapter5;

import java.io.File;
import java.io.IOException;
import java.util.Arrays;
import java.util.Set;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.ChunkFactory;
import com.aliasi.chunk.ChunkingImpl;
import com.aliasi.util.Files;
import com.aliasi.util.Strings;

public class ParagraphDetection {

	public static void main(String[] args) throws IOException {
		String document = Files.readFromFile(new File(args[0]), Strings.UTF8);
		ChunkingImpl chunking = new ChunkingImpl(document.toCharArray(),0,document.length());
		String[] paragraphs = document.split("\n\n");
		int start = 0;
		int end = 0;
		for (String paragraph : paragraphs) {
			end = start + paragraph.length();
			chunking.add(ChunkFactory.createChunk(start,end,"p"));
			start = end + 2;
		}
		String underlyingString = chunking.charSequence().toString();
		Set<Chunk> chunkSet = chunking.chunkSet();
		Chunk[] chunkArray = chunkSet.toArray(new Chunk[0]);
		Arrays.sort(chunkArray,Chunk.TEXT_ORDER_COMPARATOR);
		StringBuilder output = new StringBuilder(underlyingString);
		for (int i = chunkArray.length -1; i > 0; --i) {
			Chunk chunk = chunkArray[i];
			output.insert(chunk.end(),"}");
			output.insert(chunk.start(),"{");
		}
		System.out.println(output.toString());
	}
}
package com.lingpipe.cookbook.chapter5;

import java.io.File;
import java.io.IOException;
import java.util.Arrays;
import java.util.Comparator;
import java.util.Set;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.ChunkFactory;
import com.aliasi.chunk.ChunkingImpl;
import com.aliasi.sentences.IndoEuropeanSentenceModel;
import com.aliasi.sentences.SentenceChunker;
import com.aliasi.sentences.SentenceModel;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.util.Files;
import com.aliasi.util.Strings;

public class ParagraphSentenceDetection {

	public static void main(String[] args) throws IOException {
		String filePath = args.length > 0 ? args[0] : "data/paragraphExample.txt";
		String document = Files.readFromFile(new File(filePath), Strings.UTF8);
		String[] paragraphs = document.split("\n\n");
		int paraSeparatorLength = 2;
		ChunkingImpl paraChunking = new ChunkingImpl(document.toCharArray(),0,document.length());
		ChunkingImpl sentChunking = new ChunkingImpl(paraChunking.charSequence());
		boolean eosIsSentBoundary = true; 
		boolean balanceParens = false;
		SentenceModel sentenceModel 
			= new IndoEuropeanSentenceModel(eosIsSentBoundary,balanceParens);
		SentenceChunker sentenceChunker 
			= new SentenceChunker(IndoEuropeanTokenizerFactory.INSTANCE,sentenceModel);	
		int paraStart = 0;
		for (String paragraph : paragraphs) {
			for (Chunk sentChunk : sentenceChunker.chunk(paragraph).chunkSet()) {
				Chunk adjustedSentChunk 
					= ChunkFactory.createChunk(sentChunk.start() + paraStart,sentChunk.end() + paraStart, "S");
				sentChunking.add(adjustedSentChunk);
			}
			paraChunking.add(ChunkFactory.createChunk(paraStart, paraStart + paragraph.length(),"P"));
			paraStart += paragraph.length() + paraSeparatorLength;
		}
		String underlyingString = paraChunking.charSequence().toString();
		ChunkingImpl displayChunking = new ChunkingImpl(paraChunking.charSequence());
		displayChunking.addAll(sentChunking.chunkSet());
		displayChunking.addAll(paraChunking.chunkSet());
		Set<Chunk> chunkSet = displayChunking.chunkSet();
		Chunk[] chunkArray = chunkSet.toArray(new Chunk[0]);
		Arrays.sort(chunkArray,Chunk.LONGEST_MATCH_ORDER_COMPARATOR);
		StringBuilder output = new StringBuilder(underlyingString);
		int sentBoundOffset = 0;
		for (int i = chunkArray.length -1; i >= 0; --i) {
			Chunk chunk = chunkArray[i];
			System.out.println(chunk);
			if (chunk.type().equals("P")) {
				output.insert(chunk.end() + sentBoundOffset,"}");
				output.insert(chunk.start(),"{");
				sentBoundOffset = 0;
			}
			if (chunk.type().equals("S")) {
				output.insert(chunk.end(),"]");
				output.insert(chunk.start(),"[");
				sentBoundOffset += 2;
			}
		}
		System.out.println(output.toString());
	}
}

package com.lingpipe.cookbook.chapter5;
import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.ChunkFactory;
import com.aliasi.chunk.Chunking;
import com.aliasi.chunk.ChunkingImpl;
import com.aliasi.hmm.HiddenMarkovModel;
import com.aliasi.hmm.HmmDecoder;
import com.aliasi.tag.Tagging;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.Strings;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.Arrays;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

/**
 * Expects to chunk 1 sentence at a time.
 */
public class PhraseChunker implements Chunker {

	// Determiners & Numerals
	// ABN, ABX, AP, AP$, AT, CD, CD$, DT, DT$, DTI, DTS, DTX, OD

	// Adjectives
	// JJ, JJ$, JJR, JJS, JJT

	// Nouns
	// NN, NN$, NNS, NNS$, NP, NP$, NPS, NPS$

	// Adverbs
	// RB, RB$, RBR, RBT, RN (not RP, the particle adverb)

	// Pronoun
	// PN, PN$, PP$, PP$$, PPL, PPLS, PPO, PPS, PPSS

	// Verbs
	// VB, VBD, VBG, VBN, VBZ

	// Auxiliaries
	// MD, BE, BED, BEDZ, BEG, BEM, BEN, BER, BEZ

	// Adverbs
	// RB, RB$, RBR, RBT, RN (not RP, the particle adverb)


	// Punctuation
	// ', ``, '', ., (, ), *, --, :, ,

	static final Set<String> DETERMINER_TAGS = new HashSet<String>();
	static final Set<String> ADJECTIVE_TAGS = new HashSet<String>();
	static final Set<String> NOUN_TAGS = new HashSet<String>();
	static final Set<String> PRONOUN_TAGS = new HashSet<String>();

	static final Set<String> ADVERB_TAGS = new HashSet<String>();

	static final Set<String> VERB_TAGS = new HashSet<String>();
	static final Set<String> AUXILIARY_VERB_TAGS = new HashSet<String>();

	static final Set<String> PUNCTUATION_TAGS = new HashSet<String>();

	static final Set<String> START_VERB_TAGS = new HashSet<String>();
	static final Set<String> CONTINUE_VERB_TAGS = new HashSet<String>();

	static final Set<String> START_NOUN_TAGS = new HashSet<String>();
	static final Set<String> CONTINUE_NOUN_TAGS = new HashSet<String>();

	static {
		DETERMINER_TAGS.add("abn");
		DETERMINER_TAGS.add("abx");
		DETERMINER_TAGS.add("ap");
		DETERMINER_TAGS.add("ap$");
		DETERMINER_TAGS.add("at");
		DETERMINER_TAGS.add("cd");
		DETERMINER_TAGS.add("cd$");
		DETERMINER_TAGS.add("dt");
		DETERMINER_TAGS.add("dt$");
		DETERMINER_TAGS.add("dti");
		DETERMINER_TAGS.add("dts");
		DETERMINER_TAGS.add("dtx");
		DETERMINER_TAGS.add("od");

		ADJECTIVE_TAGS.add("jj");
		ADJECTIVE_TAGS.add("jj$");
		ADJECTIVE_TAGS.add("jjr");
		ADJECTIVE_TAGS.add("jjs");
		ADJECTIVE_TAGS.add("jjt");
		ADJECTIVE_TAGS.add("*");
		ADJECTIVE_TAGS.add("ql");

		NOUN_TAGS.add("nn");
		NOUN_TAGS.add("nn$");
		NOUN_TAGS.add("nns");
		NOUN_TAGS.add("nns$");
		NOUN_TAGS.add("np");
		NOUN_TAGS.add("np$");
		NOUN_TAGS.add("nps");
		NOUN_TAGS.add("nps$");
		NOUN_TAGS.add("nr");
		NOUN_TAGS.add("nr$");
		NOUN_TAGS.add("nrs");

		PRONOUN_TAGS.add("pn");
		PRONOUN_TAGS.add("pn$");
		PRONOUN_TAGS.add("pp$");
		PRONOUN_TAGS.add("pp$$");
		PRONOUN_TAGS.add("ppl");
		PRONOUN_TAGS.add("ppls");
		PRONOUN_TAGS.add("ppo");
		PRONOUN_TAGS.add("pps");
		PRONOUN_TAGS.add("ppss");

		VERB_TAGS.add("vb");
		VERB_TAGS.add("vbd");
		VERB_TAGS.add("vbg");
		VERB_TAGS.add("vbn");
		VERB_TAGS.add("vbz");

		AUXILIARY_VERB_TAGS.add("to");
		AUXILIARY_VERB_TAGS.add("md");
		AUXILIARY_VERB_TAGS.add("be");
		AUXILIARY_VERB_TAGS.add("bed");
		AUXILIARY_VERB_TAGS.add("bedz");
		AUXILIARY_VERB_TAGS.add("beg");
		AUXILIARY_VERB_TAGS.add("bem");
		AUXILIARY_VERB_TAGS.add("ben");
		AUXILIARY_VERB_TAGS.add("ber");
		AUXILIARY_VERB_TAGS.add("bez");

		ADVERB_TAGS.add("rb");
		ADVERB_TAGS.add("rb$");
		ADVERB_TAGS.add("rbr");
		ADVERB_TAGS.add("rbt");
		ADVERB_TAGS.add("rn");
		ADVERB_TAGS.add("ql");
		ADVERB_TAGS.add("*");  // negation

		PUNCTUATION_TAGS.add("'");
		// PUNCTUATION_TAGS.add("``");
		// PUNCTUATION_TAGS.add("''");
		PUNCTUATION_TAGS.add(".");
		PUNCTUATION_TAGS.add("*");
		// PUNCTUATION_TAGS.add(","); // miss comma-separated phrases
		// PUNCTUATION_TAGS.add("(");
		// PUNCTUATION_TAGS.add(")");
		// PUNCTUATION_TAGS.add("*"); // negation "not"
		// PUNCTUATION_TAGS.add("--");
		// PUNCTUATION_TAGS.add(":");
	}

	static {

		START_NOUN_TAGS.addAll(DETERMINER_TAGS);
		START_NOUN_TAGS.addAll(ADJECTIVE_TAGS);
		START_NOUN_TAGS.addAll(NOUN_TAGS);
		START_NOUN_TAGS.addAll(PRONOUN_TAGS);

		CONTINUE_NOUN_TAGS.addAll(START_NOUN_TAGS);
		CONTINUE_NOUN_TAGS.addAll(ADVERB_TAGS);
		CONTINUE_NOUN_TAGS.addAll(PUNCTUATION_TAGS);

		START_VERB_TAGS.addAll(VERB_TAGS);
		START_VERB_TAGS.addAll(AUXILIARY_VERB_TAGS);
		START_VERB_TAGS.addAll(ADVERB_TAGS);

		CONTINUE_VERB_TAGS.addAll(START_VERB_TAGS);
		CONTINUE_VERB_TAGS.addAll(PUNCTUATION_TAGS);
	}

	private final HmmDecoder mPosTagger;
	private final TokenizerFactory mTokenizerFactory;

	public PhraseChunker(HmmDecoder posTagger,
			TokenizerFactory tokenizerFactory) {
		mPosTagger = posTagger;
		mTokenizerFactory = tokenizerFactory;
	}

	public Chunking chunk(CharSequence cSeq) {
		char[] cs = Strings.toCharArray(cSeq);
		return chunk(cs,0,cs.length);
	}

	public Chunking chunk(char[] cs, int start, int end) {

		// tokenize
		List<String> tokenList = new ArrayList<String>();
		List<String> whiteList = new ArrayList<String>();
		Tokenizer tokenizer = mTokenizerFactory.tokenizer(cs,start,end-start);
		tokenizer.tokenize(tokenList,whiteList);
		String[] tokens
		= tokenList.<String>toArray(new String[tokenList.size()]);
		String[] whites
		= whiteList.<String>toArray(new String[whiteList.size()]);

		// part-of-speech tag
		Tagging<String> tagging = mPosTagger.tag(tokenList);

		ChunkingImpl chunking = new ChunkingImpl(cs,start,end);
		int startChunk = 0;
		for (int i = 0; i < tagging.size(); ) {
			startChunk += whites[i].length();

			if (START_NOUN_TAGS.contains(tagging.tag(i))) {
				int endChunk = startChunk + tokens[i].length();
				++i;
				while (i < tokens.length && CONTINUE_NOUN_TAGS.contains(tagging.tag(i))) {
					endChunk += whites[i].length() + tokens[i].length();
					++i;
				}
				// this separation allows internal punctuation, but not final punctuation
				int trimmedEndChunk = endChunk;
				for (int k = i;
						--k >= 0 && PUNCTUATION_TAGS.contains(tagging.tag(k)); ) {
					trimmedEndChunk -= (whites[k].length() + tokens[k].length());
				}
				if (startChunk >= trimmedEndChunk) {
					startChunk = endChunk;
					continue;
				}
				Chunk chunk
				= ChunkFactory.createChunk(startChunk,trimmedEndChunk,"noun");
				chunking.add(chunk);
				startChunk = endChunk;

			} else if (START_VERB_TAGS.contains(tagging.tag(i))) {
				int endChunk = startChunk + tokens[i].length();
				++i;
				while (i < tokens.length && CONTINUE_VERB_TAGS.contains(tagging.tag(i))) {
					endChunk += whites[i].length() + tokens[i].length();
					++i;
				}
				int trimmedEndChunk = endChunk;
				for (int k = i;
						--k >= 0 && PUNCTUATION_TAGS.contains(tagging.tag(k)); ) {
					trimmedEndChunk -= (whites[k].length() + tokens[k].length());
				}
				if (startChunk >= trimmedEndChunk) {
					startChunk = endChunk;
					continue;
				}
				Chunk chunk = ChunkFactory.createChunk(startChunk,trimmedEndChunk,"verb");
				chunking.add(chunk);
				startChunk = endChunk;

			} else {
				startChunk += tokens[i].length();
				++i;
			}
		}
		return chunking;
	}

	public static void main(String[] args) throws IOException, ClassNotFoundException {
		File hmmFile = new File("models/pos-en-general-brown.HiddenMarkovModel");
		HiddenMarkovModel posHmm = (HiddenMarkovModel)
				AbstractExternalizable.readObject(hmmFile);
		HmmDecoder posTagger  = new HmmDecoder(posHmm);
		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		PhraseChunker chunker = new PhraseChunker(posTagger,tokenizerFactory);
		BufferedReader bufReader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.print("\n\nINPUT> ");
			System.out.flush();
			String input = bufReader.readLine();
			Tokenizer tokenizer 
				= tokenizerFactory.tokenizer(input.toCharArray(),0,input.length());
			String[] tokens = tokenizer.tokenize();
			List<String> tokenList = Arrays.asList(tokens);
			Tagging<String> tagging = posTagger.tag(tokenList);
			for (int j = 0; j < tokenList.size(); ++j) {
				System.out.print(tokens[j] + "/" + tagging.tag(j) + " ");
			}
			System.out.println();
			Chunking chunking = chunker.chunk(input);
			CharSequence cs = chunking.charSequence();
			for (Chunk chunk : chunking.chunkSet()) {
				String type = chunk.type();
				int start = chunk.start();
				int end = chunk.end();
				CharSequence text = cs.subSequence(start,end);
				System.out.println("  " + type + "(" + start + "," + end + ") " + text);
			}
		}

	}

}package com.lingpipe.cookbook.chapter5;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.Iterator;
import java.util.Set;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.Chunking;
import com.aliasi.chunk.RegExChunker;

public class RegexNer {

	public static void main(String[] args) throws IOException {
		String emailRegex
    		= "[A-Za-z0-9](([_\\.\\-]?[a-zA-Z0-9]+)*)@([A-Za-z0-9]+)"
    				+ "(([\\.\\-]?[a-zA-Z0-9]+)*)\\.([A-Za-z]{2,})";
		String chunkType = "email";
		double score = 1.0;
		Chunker chunker = new RegExChunker(emailRegex,chunkType,score);   	  
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
	    String input = "";
	    while (true) {
			System.out.println("Enter text, . to quit:");
			input = reader.readLine();
			if(input.equals(".")){
				break;
			}
			Chunking chunking = chunker.chunk(input);
	        System.out.println("input=" + input);
	        System.out.println("chunking=" + chunking);
	        Set<Chunk> chunkSet = chunking.chunkSet();
	        Iterator<Chunk> it = chunkSet.iterator();
	        while (it.hasNext()) {
	            Chunk chunk = it.next();
	            int start = chunk.start();
	            int end = chunk.end();
	            String text = input.substring(start,end);
	            System.out.println("     chunk=" + chunk + "  text=" + text);
	        }
	    }
	}
}
package com.lingpipe.cookbook.chapter5;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.Chunking;
import com.aliasi.sentences.IndoEuropeanSentenceModel;
import com.aliasi.sentences.SentenceChunker;
import com.aliasi.sentences.SentenceModel;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.Set;


public class SentenceDetection {

	public static void main(String[] args) throws IOException {
		boolean endSent = true;
		boolean parenS = true;
		SentenceModel sentenceModel 
			= new IndoEuropeanSentenceModel(endSent,parenS);
		TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		Chunker sentenceChunker 
			= new SentenceChunker(tokFactory,sentenceModel);	
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.print("Enter text followed by new line\n>");
			String text = reader.readLine();
			Chunking chunking 
			= sentenceChunker.chunk(text);
			Set<Chunk> sentences = chunking.chunkSet();
			if (sentences.size() < 1) {
				System.out.println("No sentence chunks found.");
				return;
			}
			String textStored = chunking.charSequence().toString();
			for (Chunk sentence : sentences) {
				int start = sentence.start();
				int end = sentence.end();
				System.out.println("SENTENCE :" + textStored.substring(start,end));
			}
		}
	}
}


package com.lingpipe.cookbook.chapter5;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.ChunkFactory;
import com.aliasi.chunk.Chunking;
import com.aliasi.chunk.ChunkingImpl;

import com.aliasi.corpus.Corpus;
import com.aliasi.corpus.ObjectHandler;

public class TinyEntityCorpus extends Corpus<ObjectHandler<Chunking>> {

    public void visitTrain(ObjectHandler<Chunking> handler) {
        for (Chunking chunking : CHUNKINGS)
            handler.handle(chunking);
    }

    public void visitTest(ObjectHandler<Chunking> handler) {
        /* no op */
    }

    static final Chunking[] CHUNKINGS
        = new Chunking[] {
        chunking(""),
        chunking("The"),
        chunking("John ran.",
                 chunk(0,4,"PER")),
        chunking("Mary ran.",
                 chunk(0,4,"PER")),
        chunking("The kid ran."),
        chunking("John likes Mary.",
                 chunk(0,4,"PER"),
                 chunk(11,15,"PER")),
        chunking("Tim lives in Washington",
                 chunk(0,3,"PER"),
                 chunk(13,23,"LOC")),
        chunking("Mary Smith is in New York City",
                 chunk(0,10,"PER"),                     
                 chunk(17,30,"LOC")),
        chunking("New York City is fun",
                 chunk(0,13,"LOC")),
        chunking("Chicago is not like Washington",
                 chunk(0,7,"LOC"),
                 chunk(20,30,"LOC"))
    };

    static Chunking chunking(String s, Chunk... chunks) {
        ChunkingImpl chunking = new ChunkingImpl(s);
        for (Chunk chunk : chunks) 
            chunking.add(chunk);
        return chunking;
    }
    
    static Chunk chunk(int start, int end, String type) {
        return ChunkFactory.createChunk(start,end,type);
    }

}package com.lingpipe.cookbook.chapter5;

import com.aliasi.chunk.BioTagChunkCodec;
import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunking;
import com.aliasi.chunk.TagChunkCodec;

import com.aliasi.corpus.Corpus;
import com.aliasi.corpus.ObjectHandler;

import com.aliasi.crf.ChainCrf;
import com.aliasi.crf.ChainCrfChunker;
import com.aliasi.crf.ChainCrfFeatureExtractor;

import com.aliasi.io.LogLevel;
import com.aliasi.io.Reporter;
import com.aliasi.io.Reporters;

import com.aliasi.stats.AnnealingSchedule;
import com.aliasi.stats.RegressionPrior;

import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;

import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.FeatureExtractor;
import com.aliasi.util.ScoredObject;
import com.lingpipe.cookbook.chapter4.SimpleCrfFeatureExtractor;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.Iterator;

public class TrainAndRunSimpleCrf {

	public static void main(String[] args) throws IOException {
		Corpus<ObjectHandler<Chunking>> corpus
		= new TinyEntityCorpus();

		TokenizerFactory tokenizerFactory
		= IndoEuropeanTokenizerFactory.INSTANCE;
		boolean enforceConsistency = true;
		TagChunkCodec tagChunkCodec
		= new BioTagChunkCodec(tokenizerFactory,
				enforceConsistency);

		ChainCrfFeatureExtractor<String> featureExtractor
		= new SimpleCrfFeatureExtractor();

		int minFeatureCount = 1;

		boolean cacheFeatures = true;

		boolean addIntercept = true;

		double priorVariance = 4.0;
		boolean uninformativeIntercept = true;
		RegressionPrior prior
		= RegressionPrior.gaussian(priorVariance,
				uninformativeIntercept);
		int priorBlockSize = 3;

		double initialLearningRate = 0.05;
		double learningRateDecay = 0.995;
		AnnealingSchedule annealingSchedule
		= AnnealingSchedule.exponential(initialLearningRate,
				learningRateDecay);

		double minImprovement = 0.00001;
		int minEpochs = 10;
		int maxEpochs = 5000;

		Reporter reporter
		= Reporters.stdOut().setLevel(LogLevel.DEBUG);

		System.out.println("\nEstimating");
		ChainCrfChunker crfChunker
		= ChainCrfChunker.estimate(corpus,
				tagChunkCodec,
				tokenizerFactory,
				featureExtractor,
				addIntercept,
				minFeatureCount,
				cacheFeatures,
				prior,
				priorBlockSize,
				annealingSchedule,
				minImprovement,
				minEpochs,
				maxEpochs,
				reporter);

		System.out.println("Done Training");
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.print("Enter text followed by new line\n>");
			String evalText = reader.readLine();
			char[] evalTextChars = evalText.toCharArray();
			System.out.println("\nFIRST BEST");
			Chunking chunking = crfChunker.chunk(evalText);
			System.out.println(chunking);

			int maxNBest = 10;
			System.out.println("\n" + maxNBest + " BEST CONDITIONAL");
			System.out.println("Rank log p(tags|tokens)  Tagging");
			Iterator<ScoredObject<Chunking>> it
			= crfChunker.nBestConditional(evalTextChars,0,evalTextChars.length,maxNBest);
			for (int rank = 0; rank < maxNBest && it.hasNext(); ++rank) {
				ScoredObject<Chunking> scoredChunking = it.next();
				System.out.println(rank + "    " + scoredChunking.score() + " " + scoredChunking.getObject().chunkSet());
			}

			System.out.println("\nMARGINAL CHUNK PROBABILITIES");
			System.out.println("Rank Chunk Phrase");
			int maxNBestChunks = 10;
			Iterator<Chunk> nBestChunkIt = crfChunker.nBestChunks(evalTextChars,0,evalTextChars.length,maxNBestChunks);
			for (int n = 0; n < maxNBestChunks && nBestChunkIt.hasNext(); ++n) {
				Chunk chunk = nBestChunkIt.next();
				System.out.println(n + " " + chunk + " " + evalText.substring(chunk.start(),chunk.end()));
			}
		}
	}

}package com.lingpipe.cookbook.chapter5;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.Chunking;
import com.aliasi.sentences.IndoEuropeanSentenceModel;
import com.aliasi.sentences.SentenceChunker;
import com.aliasi.sentences.SentenceModel;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;

import java.util.Arrays;
import java.util.Iterator;
import java.util.Set;

/** Use SentenceModel to find sentence boundaries in text */
public class WriteSentDetectedChunks {

	public static void main(String[] args) throws IOException {
		boolean eosIsSentBoundary = true;
		boolean balanceParens = true;
		SentenceModel sentenceModel 
		= new IndoEuropeanSentenceModel(eosIsSentBoundary,balanceParens);
		Chunker sentenceChunker 
		= new SentenceChunker(IndoEuropeanTokenizerFactory.INSTANCE,sentenceModel);	
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.print("Enter text followed by new line\n>");
			System.out.flush();
			String text = reader.readLine();
			Chunking chunking 
			= sentenceChunker.chunk(text);
			Set<Chunk> sentences = chunking.chunkSet();
			if (sentences.size() < 1) {
				System.out.println("No sentence chunks found.");
				continue;
			}
			String textStored = chunking.charSequence().toString();
			Set<Chunk> chunkSet = chunking.chunkSet();
			System.out.println("size: " + chunkSet.size());
			Chunk[] chunkArray = chunkSet.toArray(new Chunk[0]);
			Arrays.sort(chunkArray,Chunk.LONGEST_MATCH_ORDER_COMPARATOR);
			StringBuilder output = new StringBuilder(textStored);
			int sentBoundOffset = 0;
			for (int i = chunkArray.length -1; i >= 0; --i) {
				Chunk chunk = chunkArray[i];
				String sentence = textStored.substring(chunk.start(), chunk.end());
				if (sentence.contains("like")) {
					output.insert(chunk.end() + sentBoundOffset,"}");
					output.insert(chunk.start(),"{");
				}
			}
			System.out.println(output.toString());
		}
	}
}


package com.lingpipe.cookbook.chapter6;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.UnsupportedEncodingException;
import java.util.SortedSet;

import com.aliasi.io.FileLineReader;
import com.aliasi.spell.AutoCompleter;
import com.aliasi.spell.FixedWeightEditDistance;
import com.aliasi.util.ObjectToCounterMap;
import com.aliasi.util.ScoredObject;

public class AutoComplete {

	/**
	 * @param args
	 * @throws IOException 
	 * @throws UnsupportedEncodingException 
	 */
	public static void main(String[] args) throws UnsupportedEncodingException, IOException {
		File wordsFile = new File("data/city_populations_2012.csv");
	    String[] lines = FileLineReader.readLineArray(wordsFile,"ISO-8859-1");
	    ObjectToCounterMap<String> cityPopMap = new ObjectToCounterMap<String>();
	    int lineCount = 0;
	    for (String line : lines) {
	    	if(lineCount++ <1) continue;
	        int i = line.lastIndexOf(',');
	        if (i < 0) continue;
	        String phrase = line.substring(0,i);
	        String countString = line.substring(i+1);
	        Integer count = Integer.valueOf(countString);
	        System.out.println("Phrase: " + phrase + " Count: " + count);
	        cityPopMap.set(phrase,count);
	    }
	    double matchWeight = 0.0;
	    double insertWeight = -10.0;
	    double substituteWeight = -10.0;
	    double deleteWeight = -10.0;
	    double transposeWeight = Double.NEGATIVE_INFINITY;
	    FixedWeightEditDistance editDistance
	        = new FixedWeightEditDistance(matchWeight,
	                                      deleteWeight,
	                                      insertWeight,
	                                      substituteWeight,
	                                      transposeWeight);
	    int maxResults = 5;
	    int maxQueueSize = 10000;
	    double minScore = -25.0;
	    AutoCompleter completer
	        = new AutoCompleter(cityPopMap, editDistance,
	                            maxResults, maxQueueSize, minScore);
	    
	    BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
	    String query = "";
	    while (true) {
			System.out.println("Enter word, . to quit:");
			query = reader.readLine();
			if(query.equals(".")){
				break;
			}
			SortedSet<ScoredObject<String>> completions = completer.complete(query);
			System.out.println("\n|" + query + "|");
	        for (ScoredObject<String> so : completions)
	            System.out.printf("%6.2f %s\n", so.score(), so.getObject());
	    }
	}

}
package com.lingpipe.cookbook.chapter6;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;

import com.aliasi.lm.NGramProcessLM;
import com.aliasi.spell.CompiledSpellChecker;
import com.aliasi.spell.TrainSpellChecker;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.Files;
import com.aliasi.util.Strings;

public class CaseRestore {

	
	public static void main(String[] args) throws IOException, ClassNotFoundException {
		int NGRAM_LENGTH = 5;
		NGramProcessLM lm = new NGramProcessLM(NGRAM_LENGTH);
		
	    TrainSpellChecker sc = new TrainSpellChecker(lm,CompiledSpellChecker.CASE_RESTORING);
	    
	    String bigEnglish = Files.readFromFile(new File("data/project_gutenberg_books.txt"), Strings.UTF8);
	    sc.handle(bigEnglish);
	    CompiledSpellChecker csc = (CompiledSpellChecker) AbstractExternalizable.compile(sc);
	    
	    BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
	    String query = "";
	    while (true) {
			System.out.println("Enter input, . to quit:");
			query = reader.readLine();
			if(query.equals(".")){
				break;
			}
			
            String bestAlternative = csc.didYouMean(query);
            System.out.println("Query Text: " + query);
            System.out.println("Best Alternative: " + bestAlternative);
	    }
	}

}
package com.lingpipe.cookbook.chapter6;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.UnsupportedEncodingException;
import java.util.Arrays;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import au.com.bytecode.opencsv.CSVReader;

import com.aliasi.cluster.AbstractHierarchicalClusterer;
import com.aliasi.cluster.CompleteLinkClusterer;
import com.aliasi.cluster.Dendrogram;
import com.aliasi.cluster.HierarchicalClusterer;
import com.aliasi.cluster.SingleLinkClusterer;
import com.aliasi.io.FileLineReader;
import com.aliasi.spell.EditDistance;
import com.aliasi.spell.JaccardDistance;
import com.aliasi.spell.TfIdfDistance;
import com.aliasi.tokenizer.CharacterTokenizerFactory;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.util.Distance;
import com.aliasi.util.ObjectToCounterMap;
import com.aliasi.util.Strings;

public class CompleteLinkClusteringEditDistance {

	/**
	 * @param args
	 * @throws IOException 
	 * @throws UnsupportedEncodingException 
	 */
	public static void main(String[] args) throws UnsupportedEncodingException, IOException {
		int TEXT_INDEX = 0;
		Distance<CharSequence> EDIT_DISTANCE = new EditDistance(false);
		
		//Distance<CharSequence> EDIT_DISTANCE = new JaccardDistance(CharacterTokenizerFactory.INSTANCE);
		Distance<CharSequence> TFIDF_DISTANCE = new TfIdfDistance(IndoEuropeanTokenizerFactory.INSTANCE);
		CSVReader csvReader = new CSVReader(new InputStreamReader(new FileInputStream("data/city_populations_2012.csv"),Strings.UTF8));
		List<String[]> lines = csvReader.readAll();
		int lineCount = 0;
		Set<String> inputSet = new HashSet<String>();
		for(String[] line : lines){
			if(lineCount++ < 1) continue; //Skip header
			//if(lineCount > 100) continue;
			String text = line[TEXT_INDEX];
			inputSet.add(text);
			//System.out.println(text);
			
			
		}
		/*
		inputSet = new HashSet<String>();
		//String [] input = {"aa","aaaa","bbbb","bb","abc","aabbcc"};
		String [] input = { "aa", "aaa", "aaaaa", "bbb", "bbbb" };
		inputSet.addAll(Arrays.asList(input));
		*/
		// Complete-Link Clusterer
		double maxDistance = 4;
        HierarchicalClusterer<String> clClusterer 
            = new CompleteLinkClusterer<String>(maxDistance,
                                              EDIT_DISTANCE);
        
        
        
     // Hierarchical Clustering
        Dendrogram<String> slDendrogram
            = clClusterer.hierarchicalCluster(inputSet);
        //System.out.println("\nSingle Link Dendrogram");
        //System.out.println(slDendrogram.prettyPrint());
        
        System.out.println("\nComplete Link Clusterings with k Clusters");
        //int [] kValues = {2,10,50,100};
        int [] kValues = {1,2,3,4,5};
        for (int k:kValues) {
            Set<Set<String>> slKClustering = slDendrogram.partitionK(k);
            System.out.println(k + "  " + slKClustering);
        }
        
        Set<Set<String>> slClustering
        = clClusterer.cluster(inputSet);
        System.out.println("\nComplete Link Clustering");
        System.out.println(slClustering);
        System.out.println("\n");
        
        for(int k: kValues){
        	((AbstractHierarchicalClusterer<String>) clClusterer).setMaxDistance(k);
        	System.out.println("Complete Link Clustering at Max Distance= "+k);
        	Set<Set<String>> slClusteringMd= clClusterer.cluster(inputSet);
            //System.out.println("\nSingle Link Clustering");
            System.out.println(slClusteringMd);
            //Dendrogram<String> slDendrogramMd = clClusterer.hierarchicalCluster(inputSet);
            //System.out.println(slDendrogramMd.prettyPrint());
        }
        
        ObjectToCounterMap<Set<String>> clusterCounterMap = new ObjectToCounterMap<Set<String>>();
        for(Set<String> ss : slClustering){
        	//System.out.println("Cluster #: " + clusterCount + " ItemsInCluster: " + ss.size() + ":" + ss.toString());
        	
        	clusterCounterMap.set(ss, ss.size());
        }
        int clusterCount = 0;
        System.out.println("\nCompleteLink Clustering with more than two items in cluster");
		for(Set<String> ss : clusterCounterMap.keysOrderedByCountList()){
			if (ss.size() == 1) continue;
			System.out.println("Cluster #: " + clusterCount + " ItemsInCluster: " + ss.size() + ":" + ss.toString());
			clusterCount++;
		}
		
		
	}

}
package com.lingpipe.cookbook.chapter6;

import java.io.File;
import java.io.IOException;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import com.aliasi.io.FileLineReader;
import com.aliasi.spell.EditDistance;
import com.aliasi.spell.FixedWeightEditDistance;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.Files;
import com.aliasi.util.ObjectToDoubleMap;
import com.aliasi.util.Strings;

public class DictionaryBasedSpellCheck {

	/**
	 * @param args
	 * @throws IOException 
	 */
	public static void main(String[] args) throws IOException {
		double MATCH_WEIGHT = -0.0;
	    double DELETE_WEIGHT = -3.0;
	    double INSERT_WEIGHT = -3.0;
	    double SUBSTITUTE_WEIGHT = -2.0;
	    double TRANSPOSE_WEIGHT = -1.0;
	    
	    FixedWeightEditDistance fixedEdit =
	            new FixedWeightEditDistance(MATCH_WEIGHT,
	                                        DELETE_WEIGHT,
	                                        INSERT_WEIGHT,
	                                        SUBSTITUTE_WEIGHT,
	                                        TRANSPOSE_WEIGHT);
	    EditDistance simpleEdit = new EditDistance(true);
	    TokenizerFactory tokenizerFactory = new com.aliasi.tokenizer.LowerCaseTokenizerFactory(IndoEuropeanTokenizerFactory.INSTANCE);
	    List<String> dictLines = FileLineReader.readLines(new File("data/websters_words.txt"), Strings.UTF8);
	    Set<String> dictSet = new HashSet<String>(dictLines);
	    
	    //ObjectToDoubleMap<String> closestDictionaryWord = new ObjectToDoubleMap<String>();
	    
	    String input = "i want to acheive good skillz in lingpipe";
	    
	    String [] tokens = tokenizerFactory.tokenizer(input.toCharArray(), 0, input.length()).tokenize();
	    
	    for(String tok: tokens){
	    	ObjectToDoubleMap<String> closestDictionaryWord = new ObjectToDoubleMap<String>();
		    for(String word: dictSet){
		    	double proximity = fixedEdit.proximity(tok, word);
		    	closestDictionaryWord.put(word, proximity);
		    }
		    
		    int count = 0;
		    for(String word: closestDictionaryWord.keysOrderedByValueList()){
		    	if(count++ >= 1) break;
		    	System.out.println("Tok: " + tok + " Replacement: " + word + " Proximity: " + closestDictionaryWord.getValue(word) );
		    	//System.out.println("Distance: " + fixedEdit.distance(input, word));
		    }
	    }
	    

	}

}
package com.lingpipe.cookbook.chapter6;


import java.io.IOException;
import java.io.UnsupportedEncodingException;
import java.util.Arrays;
import java.util.HashSet;
import java.util.Set;

import com.aliasi.cluster.AbstractHierarchicalClusterer;
import com.aliasi.cluster.CompleteLinkClusterer;
import com.aliasi.cluster.Dendrogram;
import com.aliasi.cluster.HierarchicalClusterer;
import com.aliasi.cluster.SingleLinkClusterer;
import com.aliasi.spell.EditDistance;
import com.aliasi.util.Distance;

public class HierarchicalClustering {

	public static void main(String[] args) throws UnsupportedEncodingException, IOException {
		boolean allowTranspositions = false;
		Distance<CharSequence> editDistance = new EditDistance(allowTranspositions);

		Set<String> inputSet = new HashSet<String>();
		String [] input = { "aa", "aaa", "aaaaa", "bbb", "bbbb" };
		inputSet.addAll(Arrays.asList(input));

		AbstractHierarchicalClusterer<String> slClusterer 
			= new SingleLinkClusterer<String>(editDistance);

		Dendrogram<String> slDendrogram
			= slClusterer.hierarchicalCluster(inputSet);


		System.out.println("\nSingle Link Dendrogram");
		System.out.println(slDendrogram.prettyPrint());

		AbstractHierarchicalClusterer<String> clClusterer 
			= new CompleteLinkClusterer<String>(editDistance); 

		Dendrogram<String> clDendrogram 
				= clClusterer.hierarchicalCluster(inputSet);
	
		System.out.println("\nComplete Link Dendrogram");
		System.out.println(clDendrogram.prettyPrint());

		System.out.println("\nSingle Link Clusterings with k Clusters");		
		for (int k = 1; k < 6; ++k ) {
			Set<Set<String>> slKClustering = slDendrogram.partitionK(k);
			System.out.println(k + "  " + slKClustering);
		}
		
		Set<Set<String>> slClustering= slClusterer.cluster(inputSet);
		System.out.println("\nComplete Link Clustering No Max Distance");
		System.out.println(slClustering);
		System.out.println("\n");

		for (int k = 1; k < 6; ++k ) {
			clClusterer.setMaxDistance(k);
			System.out.println("Complete Link Clustering at Max Distance= " + k);
			Set<Set<String>> slClusteringMd= clClusterer.cluster(inputSet);
			System.out.println(slClusteringMd);
		}
	}

}
package com.lingpipe.cookbook.chapter6;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;






import com.aliasi.spell.JaccardDistance;
import com.aliasi.tokenizer.CharacterTokenizerFactory;
import com.aliasi.tokenizer.EnglishStopTokenizerFactory;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;

public class JaccardDistanceSample {

	
	public static void main(String[] args) throws IOException {
		
		TokenizerFactory indoEuropeanTf = IndoEuropeanTokenizerFactory.INSTANCE;
		TokenizerFactory characterTf = CharacterTokenizerFactory.INSTANCE;
		TokenizerFactory englishStopWordTf = new EnglishStopTokenizerFactory(indoEuropeanTf);
		
		JaccardDistance jaccardIndoEuropean = new JaccardDistance(indoEuropeanTf);
		JaccardDistance jaccardCharacter = new JaccardDistance(characterTf);
		JaccardDistance jaccardEnglishStopWord = new JaccardDistance(englishStopWordTf);
		
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.println("Enter the first string:");
			String text1 = reader.readLine();
			System.out.println("Enter the second string:");
			String text2 = reader.readLine();
			
			
			double jaccardIndoEuropeanDistance = jaccardIndoEuropean.distance(text1,text2);
			double jaccardCharacterDistance = jaccardCharacter.distance(text1,text2);
			double jaccardEnglishStopWordDistance = jaccardEnglishStopWord.distance(text1, text2);
			
			String indoEuropeanTokensT1 = join(indoEuropeanTf.tokenizer(text1.toCharArray(), 0, text1.length()).tokenize(),",");
			String indoEuropeanTokensT2 = join(indoEuropeanTf.tokenizer(text2.toCharArray(), 0, text2.length()).tokenize(),",");
			
			String characterTokensT1 = join(characterTf.tokenizer(text1.toCharArray(), 0, text1.length()).tokenize(),",");
			String characterTokensT2 = join(characterTf.tokenizer(text2.toCharArray(), 0, text2.length()).tokenize(),",");
			
			String englishStopWordTokensT1 = join(englishStopWordTf.tokenizer(text1.toCharArray(), 0, text1.length()).tokenize(),",");
			String englishStopWordTokensT2 = join(englishStopWordTf.tokenizer(text2.toCharArray(), 0, text2.length()).tokenize(),",");
			
			
			System.out.println("\nIndoEuropean Tokenizer");
			System.out.println("Text1 Tokens: {" + indoEuropeanTokensT1+"}");
			System.out.println("Text2 Tokens: {" + indoEuropeanTokensT2+"}");
			System.out.println("IndoEuropean Jaccard Distance is " + jaccardIndoEuropeanDistance);
			
			System.out.println("\nCharacter Tokenizer");
			System.out.println("Text1 Tokens: {" + characterTokensT1+"}");
			System.out.println("Text2 Tokens: {" + characterTokensT2+"}");
			System.out.println("Character Jaccard Distance between is " + jaccardCharacterDistance);
			
			System.out.println("\nEnglishStopWord Tokenizer");
			System.out.println("Text1 Tokens: {" + englishStopWordTokensT1+"}");
			System.out.println("Text2 Tokens: {" + englishStopWordTokensT2+"}");
			System.out.println("English Stopword Jaccard Distance between is " + jaccardEnglishStopWordDistance);
			System.out.println();
		}

	}

	private static String join(String[] tokens, String delimiter) {
		StringBuilder sb = new StringBuilder();
		for (int i = 0; i < tokens.length -1; ++ i) {
			sb.append("'" + tokens[i] +  "'");
		}
		return sb.toString();	
	}

}
package com.lingpipe.cookbook.chapter6;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Random;
import java.util.Set;

import au.com.bytecode.opencsv.CSVReader;

import com.aliasi.classify.Classification;
import com.aliasi.classify.Classified;
import com.aliasi.cluster.LatentDirichletAllocation;
import com.aliasi.corpus.XValidatingObjectCorpus;
import com.aliasi.symbol.MapSymbolTable;
import com.aliasi.symbol.SymbolTable;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.LowerCaseTokenizerFactory;
import com.aliasi.tokenizer.RegExTokenizerFactory;
import com.aliasi.tokenizer.StopTokenizerFactory;
import com.aliasi.tokenizer.TokenLengthTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.Strings;
import com.lingpipe.cookbook.Util;

public class LDA {
	
	static int ANNOTATION_OFFSET = 2;
	static int TEXT_OFFSET = 3;
	static int NUM_FOLDS = 4;

	static String[] CATEGORIES;

	static List<String> getCsvData(File file,String encoding) throws IOException {
		InputStreamReader fileReader = new InputStreamReader(new FileInputStream(file),encoding);
		CSVReader csvReader = new CSVReader(fileReader);
		List<String[]> inputData = csvReader.readAll();
		csvReader.close();
		List<String> lines = new ArrayList<String>();
		for (String[] row : inputData) {
			if (row[TEXT_OFFSET].equals("Text") ) {
				continue;
			}
			lines.add(row[TEXT_OFFSET]);
		}
		return lines;
	}

	public static void main(String[] args) throws Exception {
		String inFile = args.length > 0 ? args[0] : "data/gravity_tweets.csv";
        File corpusFile = new File(inFile);
        List<String[]> tweets = Util.readCsvRemoveHeader(corpusFile);
        int minTokenCount = 5;
        short numTopics = 25;
        double documentTopicPrior = .1;
        double wordPrior = 0.01;
        int burninEpochs = 0;
        int sampleLag = 1;
        int numSamples = 2000;
        long randomSeed = 6474835;
        SymbolTable symbolTable = new MapSymbolTable();
        TokenizerFactory tokFactory = new RegExTokenizerFactory("[^\\s]+");
        	//= IndoEuropeanTokenizerFactory.INSTANCE;
        
        tweets = Util.filterJaccard(tweets, tokFactory, .5);
        System.out.println("Input file=" + corpusFile);
        System.out.println("Minimum token count=" + minTokenCount);
        System.out.println("Number of topics=" + numTopics);
        System.out.println("Topic prior in docs=" + documentTopicPrior);
        System.out.println("Word prior in topics=" + wordPrior);
        System.out.println("Burnin epochs=" + burninEpochs);
        System.out.println("Sample lag=" + sampleLag);
        System.out.println("Number of samples=" + numSamples);
        // reportCorpus(articleTexts);
        String[] ldaTexts = new String[tweets.size()];
        for (int i = 0; i < tweets.size(); ++i) {
        	ldaTexts[i] = tweets.get(i)[Util.TEXT_OFFSET];
        }
        System.out.println("##########Got " + ldaTexts.length);
        int[][] docTokens
            = LatentDirichletAllocation
            .tokenizeDocuments(ldaTexts,tokFactory,symbolTable,minTokenCount);
        System.out.println("Number of unique words above count threshold=" + symbolTable.numSymbols());

        int numTokens = 0;
        for (int[] tokens : docTokens) {
            numTokens += tokens.length;
        }
        System.out.println("Tokenized.  #Tokens After Pruning=" + numTokens);

        LdaReportingHandler handler
            = new LdaReportingHandler(symbolTable);

        LatentDirichletAllocation.GibbsSample sample
            = LatentDirichletAllocation
            .gibbsSampler(docTokens,
                          numTopics,
                          documentTopicPrior,
                          wordPrior,
                          burninEpochs,
                          sampleLag,
                          numSamples,
                          new Random(randomSeed),
                          handler);

        int maxWordsPerTopic = 20;
        int maxTopicsPerDoc = 10;
        boolean reportTokens = true;
        handler.reportTopics(sample,maxWordsPerTopic,maxTopicsPerDoc,reportTokens);
        handler.reportDocuments(sample, maxWordsPerTopic, maxTopicsPerDoc, reportTokens);
    }
}
package com.lingpipe.cookbook.chapter6;

import com.aliasi.cluster.LatentDirichletAllocation;

import com.aliasi.corpus.ObjectHandler;

import com.aliasi.symbol.SymbolTable;

import com.aliasi.util.ObjectToCounterMap;
import com.aliasi.util.Strings;

import java.util.List;


public class LdaReportingHandler implements ObjectHandler<LatentDirichletAllocation.GibbsSample> {

	private final SymbolTable mSymbolTable;
	private final long mStartTime;




	LdaReportingHandler(SymbolTable symbolTable) {
		mSymbolTable = symbolTable;
		mStartTime = System.currentTimeMillis();
	}

	public void handle(LatentDirichletAllocation.GibbsSample sample) {

		System.out.printf("Epoch=%3d   elapsed time=%s\n",
				sample.epoch(),
				Strings.msToString(System.currentTimeMillis() - mStartTime));

		if ((sample.epoch() % 10) == 0) {
			double corpusLog2Prob = sample.corpusLog2Probability();
			System.out.println("      log2 p(corpus|phi,theta)=" + corpusLog2Prob
					+ "     token cross-entropy rate=" + (-corpusLog2Prob/sample.numTokens()));
		}
	}

	void reportTopics(LatentDirichletAllocation.GibbsSample sample,
			int maxWordsPerTopic,
			int maxTopicsPerDoc,
			boolean reportTokens) {

		System.out.println("\nFull Report");

		int numTopics = sample.numTopics();
		int numWords = sample.numWords();
		int numDocs = sample.numDocuments();
		int numTokens = sample.numTokens();

		System.out.println("epoch=" + sample.epoch());
		System.out.println("numDocs=" + numDocs);
		System.out.println("numTokens=" + numTokens);
		System.out.println("numWords=" + numWords);
		System.out.println("numTopics=" + numTopics);

		for (int topic = 0; topic < numTopics; ++topic) {
			int topicCount = sample.topicCount(topic);
			ObjectToCounterMap<Integer> counter = new ObjectToCounterMap<Integer>();
			for (int word = 0; word < numWords; ++word)
				counter.set(Integer.valueOf(word),sample.topicWordCount(topic,word));
			List<Integer> topWords = counter.keysOrderedByCountList();
			System.out.println("\nTOPIC " + topic  + "  (total count=" + topicCount + ")");
			//System.out.println("SYMBOL             WORD    COUNT   PROB          Z");
			System.out.println("           WORD    COUNT        Z");
			System.out.println("--------------------------------------------------");
			for (int rank = 0; rank < maxWordsPerTopic && rank < topWords.size(); ++rank) {
				int wordId = topWords.get(rank);
				String word = mSymbolTable.idToSymbol(wordId);
				int wordCount = sample.wordCount(wordId);
				int topicWordCount = sample.topicWordCount(topic,wordId);
				double topicWordProb = sample.topicWordProb(topic,wordId);
				double z = binomialZ(topicWordCount,
						topicCount,
						wordCount,
						numTokens);

				/*System.out.printf("%6d  %15s  %7d   %4.3f  %8.1f\n",
						wordId,
						word,
						topicWordCount,
						topicWordProb,
						z);
						*/
				System.out.printf("%15s  %7d  %8.1f\n",
						word,
						topicWordCount,
						z);
			}
		}
	}

	void reportDocuments(LatentDirichletAllocation.GibbsSample sample,
			int maxWordsPerTopic,
			int maxTopicsPerDoc,
			boolean reportTokens) {
		int numTopics = sample.numTopics();
		int numDocs = sample.numDocuments();
		for (int doc = 0; doc < numDocs; ++doc) {
			int docCount = 0;
			for (int topic = 0; topic < numTopics; ++topic)
				docCount += sample.documentTopicCount(doc,topic);
			ObjectToCounterMap<Integer> counter = new ObjectToCounterMap<Integer>();
			for (int topic = 0; topic < numTopics; ++topic)
				counter.set(Integer.valueOf(topic),sample.documentTopicCount(doc,topic));
			List<Integer> topTopics = counter.keysOrderedByCountList();
			System.out.println("\nDOC " + doc);
			System.out.println("TOPIC    COUNT    PROB");
			System.out.println("----------------------");
			for (int rank = 0; rank < topTopics.size() && rank < maxTopicsPerDoc; ++rank) {
				int topic = topTopics.get(rank);
				int docTopicCount = sample.documentTopicCount(doc,topic);
				double docTopicPrior = sample.documentTopicPrior();
				double docTopicProb = (sample.documentTopicCount(doc,topic) + docTopicPrior)
						/ (docCount + numTopics * docTopicPrior);
				System.out.printf("%5d  %7d   %4.3f\n",
						topic,
						docTopicCount,
						docTopicProb);
			}
			System.out.println();
			if (!reportTokens) continue;
			int numDocTokens = sample.documentLength(doc);
			for (int tok = 0; tok < numDocTokens; ++tok) {
				int symbol = sample.word(doc,tok);
				short topic = sample.topicSample(doc,tok);
				String word = mSymbolTable.idToSymbol(symbol);
				System.out.print(word + "(" + topic + ") ");
			}
			System.out.println();
		}
	}

	static double binomialZ(double wordCountInDoc, double wordsInDoc,
			double wordCountinCorpus, double wordsInCorpus) {
		double pCorpus = wordCountinCorpus / wordsInCorpus;
		double var = wordsInCorpus * pCorpus * (1 - pCorpus);
		double dev = Math.sqrt(var);
		double expected = wordsInDoc * pCorpus;
		double z = (wordCountInDoc - expected) / dev;
		return z;
	}
}package com.lingpipe.cookbook.chapter6;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;

import com.aliasi.spell.EditDistance;

public class SimpleEditDistance {

	/**
	 * @param args
	 * @throws IOException 
	 */
	public static void main(String[] args) throws IOException {
		
	
		EditDistance dmAllowTrans = new EditDistance(true);
		EditDistance dmNoTrans = new EditDistance(false);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.println("Enter the first string:");
			String text1 = reader.readLine();
			System.out.println("Enter the second string:");
			String text2 = reader.readLine();
			double allowTransDist = dmAllowTrans.distance(text1, text2);
			double noTransDist = dmNoTrans.distance(text1, text2);
			System.out.println("Allowing Transposition Distance between: " + text1 + " and " + text2 + " is " + allowTransDist);
			System.out.println("No Transposition Distance between: " + text1 + " and " + text2 + " is " + noTransDist);
		}
	}

}
package com.lingpipe.cookbook.chapter6;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;

import com.aliasi.spell.EditDistance;
import com.aliasi.spell.FixedWeightEditDistance;
import com.aliasi.spell.WeightedEditDistance;

public class SimpleWeightedEditDistance {

	/**
	 * @param args
	 * @throws IOException 
	 */
	public static void main(String[] args) throws IOException {
		String text1 = "";
		String text2 = "";
		EditDistance dmAllowTrans = new EditDistance(true);
		EditDistance dmNoTrans = new EditDistance(false);
		
		double matchWeight = 0;
		double deleteWeight = -2;
		double insertWeight = -2;
		double substituteWeight = -2;
		double transposeWeight = Double.NEGATIVE_INFINITY;
		WeightedEditDistance wed = new FixedWeightEditDistance(matchWeight,deleteWeight,insertWeight,substituteWeight,transposeWeight);
		System.out.println("Fixed Weight Edit Distance: "+ wed.toString());
		CustomWeightedEditDistance cwed = new CustomWeightedEditDistance();
		System.out.println("Custom Weight Edit Distance: "+ cwed.toString());
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		//reader.markSupported();
		//reader.mark(0);
		while (true) {
			System.out.println("Enter the first string:");
			text1 = reader.readLine();
			//reader.reset();
			System.out.println("Enter the second string:");
			text2 = reader.readLine();
			//reader.reset();
			double allowTransDistance = dmAllowTrans.distance(text1, text2);
			double noTransDistance = dmNoTrans.distance(text1, text2);
			double wedDistance = wed.distance(text1, text2);
			double cwedDistance = cwed.distance(text1, text2);
			
			
			System.out.println("Allowing Transposition Distance between: " + text1 + " and " + text2 + " is " + allowTransDistance);
			System.out.println("No Transposition Distance between: " + text1 + " and " + text2 + " is " + noTransDistance);
			System.out.println("Fixed Weight Edit Distance between: " + text1 + " and " + text2 + " is " + wedDistance);
			System.out.println("Custom Weight Edit Distance between: " + text1 + " and " + text2 + " is " + cwedDistance);
		}
	}
	
	public static class CustomWeightedEditDistance extends WeightedEditDistance{

		@Override
		public double deleteWeight(char arg0) {
			return (Character.isDigit(arg0)||Character.isLetter(arg0)) ? -1 : 0;
			
		}

		@Override
		public double insertWeight(char arg0) {
			return deleteWeight(arg0);
		}

		@Override
		public double matchWeight(char arg0) {
			return 0;
		}

		@Override
		public double substituteWeight(char cDeleted, char cInserted) {
			return Character.toLowerCase(cDeleted) == Character.toLowerCase(cInserted) ? 0 :-1;
			
		}

		@Override
		public double transposeWeight(char arg0, char arg1) {
			return Double.NEGATIVE_INFINITY;
		}
		
		
	}

}
package com.lingpipe.cookbook.chapter6;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.UnsupportedEncodingException;
import java.util.Arrays;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import au.com.bytecode.opencsv.CSVReader;

import com.aliasi.cluster.AbstractHierarchicalClusterer;
import com.aliasi.cluster.CompleteLinkClusterer;
import com.aliasi.cluster.Dendrogram;
import com.aliasi.cluster.HierarchicalClusterer;
import com.aliasi.cluster.SingleLinkClusterer;
import com.aliasi.io.FileLineReader;
import com.aliasi.spell.EditDistance;
import com.aliasi.spell.JaccardDistance;
import com.aliasi.spell.TfIdfDistance;
import com.aliasi.tokenizer.CharacterTokenizerFactory;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.util.Distance;
import com.aliasi.util.ObjectToCounterMap;
import com.aliasi.util.Strings;

public class SingleLinkClusteringEditDistance {

	/**
	 * @param args
	 * @throws IOException 
	 * @throws UnsupportedEncodingException 
	 */
	public static void main(String[] args) throws UnsupportedEncodingException, IOException {
		int TEXT_INDEX = 0;
		Distance<CharSequence> EDIT_DISTANCE = new EditDistance(false);
		
		//Distance<CharSequence> EDIT_DISTANCE = new JaccardDistance(CharacterTokenizerFactory.INSTANCE);
		Distance<CharSequence> TFIDF_DISTANCE = new TfIdfDistance(IndoEuropeanTokenizerFactory.INSTANCE);
		CSVReader csvReader = new CSVReader(new InputStreamReader(new FileInputStream("data/city_populations_2012.csv"),Strings.UTF8));
		List<String[]> lines = csvReader.readAll();
		int lineCount = 0;
		Set<String> inputSet = new HashSet<String>();
		for(String[] line : lines){
			if(lineCount++ < 1) continue; //Skip header
			//if(lineCount > 100) continue;
			String text = line[TEXT_INDEX];
			inputSet.add(text);
			//System.out.println(text);
			
			
		}
		/*
		inputSet = new HashSet<String>();
		//String [] input = {"aa","aaaa","bbbb","bb","abc","aabbcc"};
		String [] input = { "aa", "aaa", "aaaaa", "bbb", "bbbb" };
		//String [] input = {"a","aaa","aaaaaa","aaaaaaaa","aaaaaaaaaa"};
		inputSet.addAll(Arrays.asList(input));
		*/
		// Single-Link Clusterer
		double maxDistance = 4;
        HierarchicalClusterer<String> slClusterer 
            = new SingleLinkClusterer<String>(maxDistance,
                                              EDIT_DISTANCE);
        
        
        
     // Hierarchical Clustering
        Dendrogram<String> slDendrogram
            = slClusterer.hierarchicalCluster(inputSet);
        //System.out.println("\nSingle Link Dendrogram");
        //System.out.println(slDendrogram.prettyPrint());
       
        
        System.out.println("\nSingle Link Clusterings with k Clusters");
        //int [] kValues = {2,10,50,100};
        int [] kValues = {1,2,3,4,5};
        for (int k:kValues) {
            Set<Set<String>> slKClustering = slDendrogram.partitionK(k);
            System.out.println(k + "  " + slKClustering);
        }
        
        Set<Set<String>> slClustering= slClusterer.cluster(inputSet);
        System.out.println("\nSingle Link Clustering");
        System.out.println(slClustering);
        System.out.println("\n");
        
        for(int k: kValues){
        	((AbstractHierarchicalClusterer<String>) slClusterer).setMaxDistance(k);
        	System.out.println("Single Link Clustering at Max Distance= "+k);
        	Set<Set<String>> slClusteringMd= slClusterer.cluster(inputSet);
            //System.out.println("\nSingle Link Clustering");
            System.out.println(slClusteringMd);
            //Dendrogram<String> slDendrogramMd = slClusterer.hierarchicalCluster(inputSet);
            //System.out.println(slDendrogramMd.prettyPrint());
        }
        
        ObjectToCounterMap<Set<String>> clusterCounterMap = new ObjectToCounterMap<Set<String>>();
        for(Set<String> ss : slClustering){
        	//System.out.println("Cluster #: " + clusterCount + " ItemsInCluster: " + ss.size() + ":" + ss.toString());
        	
        	clusterCounterMap.set(ss, ss.size());
        }
        
        int clusterCount = 0;
        System.out.println("\nSingle Link Clustering with more than two items in cluster");
		for(Set<String> ss : clusterCounterMap.keysOrderedByCountList()){
			if (ss.size() == 1) continue;
			System.out.println("Cluster #: " + clusterCount + " ItemsInCluster: " + ss.size() + ":" + ss.toString());
			clusterCount++;
		}
		
		
	}

}
package com.lingpipe.cookbook.chapter6;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.UnsupportedEncodingException;
import java.util.Arrays;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import au.com.bytecode.opencsv.CSVReader;

import com.aliasi.cluster.Dendrogram;
import com.aliasi.cluster.HierarchicalClusterer;
import com.aliasi.cluster.SingleLinkClusterer;
import com.aliasi.io.FileLineReader;
import com.aliasi.spell.EditDistance;
import com.aliasi.spell.JaccardDistance;
import com.aliasi.spell.TfIdfDistance;
import com.aliasi.tokenizer.CharacterTokenizerFactory;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.util.Distance;
import com.aliasi.util.ObjectToCounterMap;
import com.aliasi.util.Strings;

public class SingleLinkClusteringTfIdf {

	/**
	 * @param args
	 * @throws IOException 
	 * @throws UnsupportedEncodingException 
	 */
	public static void main(String[] args) throws UnsupportedEncodingException, IOException {
		int TEXT_INDEX = 3;
		Distance<CharSequence> EDIT_DISTANCE = new EditDistance(false);
		
		//Distance<CharSequence> EDIT_DISTANCE = new JaccardDistance(CharacterTokenizerFactory.INSTANCE);
		TfIdfDistance TFIDF_DISTANCE = new TfIdfDistance(IndoEuropeanTokenizerFactory.INSTANCE);
		CSVReader csvReader = new CSVReader(new InputStreamReader(new FileInputStream("data/gravity_tweets.csv"),Strings.UTF8));
		List<String[]> lines = csvReader.readAll();
		int lineCount = 0;
		Set<String> inputSet = new HashSet<String>();
		for(String[] line : lines){
			if(lineCount++ < 1) continue; //Skip header
			//if(lineCount > 100) continue;
			String text = line[TEXT_INDEX];
			inputSet.add(text);
			TFIDF_DISTANCE.handle(text);
			//System.out.println(text);
			
			
		}
		System.out.println("Looked at :" + lineCount + " tweets");
		/*
		inputSet = new HashSet<String>();
		//String [] input = {"aa","aaaa","bbbb","bb","abc","aabbcc"};
		String [] input = { "aa", "aaa", "aaaaa", "bbb", "bbbb" };
		inputSet.addAll(Arrays.asList(input));
		*/
		// Single-Link Clusterer
		double maxDistance = 0.3;
        HierarchicalClusterer<String> slClusterer 
            = new SingleLinkClusterer<String>(maxDistance,
                                              TFIDF_DISTANCE);
        
     // Hierarchical Clustering
        Dendrogram<String> slDendrogram
            = slClusterer.hierarchicalCluster(inputSet);
        //System.out.println("\nSingle Link Dendrogram");
        //System.out.println(slDendrogram.prettyPrint());
        /*
        System.out.println("\nSingle Link Clusterings");
        int [] kValues = {2,10,50,100};
        for (int k:kValues) {
            Set<Set<String>> slKClustering = slDendrogram.partitionK(k);
            System.out.println(k + "  " + slKClustering);
        }
        */
        Set<Set<String>> slClustering
        = slClusterer.cluster(inputSet);
        System.out.println("\nSingle Link Clustering");
        System.out.println(slClustering);
        
        
        ObjectToCounterMap<Set<String>> clusterCounterMap = new ObjectToCounterMap<Set<String>>();
        for(Set<String> ss : slClustering){
        	//System.out.println("Cluster #: " + clusterCount + " ItemsInCluster: " + ss.size() + ":" + ss.toString());
        	
        	clusterCounterMap.set(ss, ss.size());
        }
        int clusterCount = 0;
		for(Set<String> ss : clusterCounterMap.keysOrderedByCountList()){
			if (ss.size() == 1) continue;
			System.out.println("Cluster #: " + clusterCount + " ItemsInCluster: " + ss.size() + ":" + ss.toString());
			clusterCount++;
		}
	}

}
package com.lingpipe.cookbook.chapter6;

import java.io.BufferedReader;
import java.io.File;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.ObjectOutput;
import java.io.ObjectOutputStream;
import java.util.HashSet;
import java.util.Iterator;
import java.util.NoSuchElementException;
import java.util.Set;

import com.aliasi.lm.NGramProcessLM;
import com.aliasi.spell.CompiledSpellChecker;
import com.aliasi.spell.FixedWeightEditDistance;
import com.aliasi.spell.SpellChecker;
import com.aliasi.spell.TrainSpellChecker;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.Files;
import com.aliasi.util.ObjectToCounterMap;
import com.aliasi.util.ScoredObject;
import com.aliasi.util.Strings;

public class SpellCheck {

	public static void main(String[] args) throws IOException, ClassNotFoundException {
		double matchWeight = -0.0;
	    double deleteWeight = -4.0;
	    double insertWeight = -2.5;
	    double substituteWeight = -2.5;
	    double transposeWeight = -1.0;
	    
	    FixedWeightEditDistance fixedEdit =
	            new FixedWeightEditDistance(matchWeight,
	                                        deleteWeight,
	                                        insertWeight,
	                                        substituteWeight,
	                                        transposeWeight);
	    int NGRAM_LENGTH = 6;
	    NGramProcessLM lm = new NGramProcessLM(NGRAM_LENGTH);
	    
	    TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
	    tokenizerFactory
	    	= new com.aliasi.tokenizer.LowerCaseTokenizerFactory(tokenizerFactory);
	    
	    TrainSpellChecker sc = new TrainSpellChecker(lm,fixedEdit,tokenizerFactory);
	    File inFile = new File("data/project_gutenberg_books.txt");
	    String bigEnglish = Files.readFromFile(inFile, Strings.UTF8);
	    sc.handle(bigEnglish);
	    File dict = new File("data/websters_words.txt");
	    String webster = Files.readFromFile(dict, Strings.UTF8);
	    sc.handle(webster);
	    CompiledSpellChecker csc = (CompiledSpellChecker) AbstractExternalizable.compile(sc);
	    Set<String> dontEdit = new HashSet<String>();
	    dontEdit.add("lingpipe");
	    csc.setDoNotEditTokens(dontEdit);
	    
	    csc.setTokenizerFactory(tokenizerFactory);
	    
	    int nBest = 3;
	    csc.setNBest(64);
	    BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
	    String query = "";
	    while (true) {
			System.out.println("Enter word, . to quit:");
			query = reader.readLine();
			if(query.equals(".")){
				break;
			}
            String bestAlternative = csc.didYouMean(query);
            System.out.println("Best Alternative: " + bestAlternative);
            int i = 0;
            Iterator<ScoredObject<String>> iterator = csc.didYouMeanNBest(query);
            while (i < nBest) {
            		ScoredObject<String> so = iterator.next();
            		System.out.println("Nbest: " + i + ": " + so.getObject() + " Score:" + so.score());
            		i++;
            }
	    }
	}
}
package com.lingpipe.cookbook.chapter6;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.List;

import com.aliasi.spell.TfIdfDistance;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.NGramTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.Files;
import com.aliasi.util.ObjectToDoubleMap;
import com.aliasi.util.Strings;
import com.lingpipe.cookbook.Util;

import au.com.bytecode.opencsv.CSVReader;


public class TfIdfSearch {

	public static void main(String[] args) throws IOException {
		String searchableDocs = args.length > 0 ? args[0] : "data/disneyWorld.csv";
		System.out.println("Reading search index from " + searchableDocs);
		String idfFile = args.length > 1 ? args[1] : "data/connecticut_yankee_king_arthur.txt";
		System.out.println("Getting IDF data from " + idfFile);

		TokenizerFactory tokFact = IndoEuropeanTokenizerFactory.INSTANCE;
		TfIdfDistance tfIdfDist = new TfIdfDistance(tokFact);
		
		String training = Files.readFromFile(new File(idfFile), Strings.UTF8);
		for (String line: training.split("\\.")) {
			tfIdfDist.handle(line);
		}

		List<String[]> docsToSearch = Util.readCsvRemoveHeader(new File(searchableDocs));
		
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.println("enter a query: ");
			String query = reader.readLine();
			ObjectToDoubleMap<String> scoredMatches = new ObjectToDoubleMap<String>();
			for (String [] line : docsToSearch) {
				scoredMatches.put(line[Util.TEXT_OFFSET], tfIdfDist.proximity(line[Util.TEXT_OFFSET],query));
			}
			List<String> rankedDocs = scoredMatches.keysOrderedByValueList();
			for (int i = 0; i < 10; ++i) {
				System.out.printf("%.2f : ",scoredMatches.get(rankedDocs.get(i)));
				System.out.println(rankedDocs.get(i));
			}
		}
	}
}
package com.lingpipe.cookbook.chapter7;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.ChunkFactory;
import com.aliasi.chunk.Chunking;
import com.aliasi.chunk.ChunkingImpl;

import com.aliasi.hmm.HiddenMarkovModel;
import com.aliasi.hmm.HmmDecoder;

import com.aliasi.tag.Tagging;

import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;

import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.FastCache;
import com.aliasi.util.Strings;

import java.io.File;
import java.io.IOException;

import java.util.Arrays;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

/**
 * Expects to chunk 1 sentence at a time.
 */
public class BasalNPVPChunker implements Chunker {

    // Determiners & Numerals
    // ABN, ABX, AP, AP$, AT, CD, CD$, DT, DT$, DTI, DTS, DTX, OD

    // Adjectives
    // JJ, JJ$, JJR, JJS, JJT

    // Nouns
    // NN, NN$, NNS, NNS$, NP, NP$, NPS, NPS$

    // Adverbs
    // RB, RB$, RBR, RBT, RN (not RP, the particle adverb)

    // Pronoun
    // PN, PN$, PP$, PP$$, PPL, PPLS, PPO, PPS, PPSS

    // Verbs
    // VB, VBD, VBG, VBN, VBZ

    // Auxiliaries
    // MD, BE, BED, BEDZ, BEG, BEM, BEN, BER, BEZ

    // Adverbs
    // RB, RB$, RBR, RBT, RN (not RP, the particle adverb)


    // Punctuation
    // ', ``, '', ., (, ), *, --, :, ,

    static final Set<String> DETERMINER_TAGS = new HashSet<String>();
    static final Set<String> ADJECTIVE_TAGS = new HashSet<String>();
    static final Set<String> NOUN_TAGS = new HashSet<String>();
    static final Set<String> PRONOUN_TAGS = new HashSet<String>();

    static final Set<String> ADVERB_TAGS = new HashSet<String>();

    static final Set<String> VERB_TAGS = new HashSet<String>();
    static final Set<String> AUXILIARY_VERB_TAGS = new HashSet<String>();

    static final Set<String> PUNCTUATION_TAGS = new HashSet<String>();

    static final Set<String> START_VERB_TAGS = new HashSet<String>();
    static final Set<String> CONTINUE_VERB_TAGS = new HashSet<String>();

    static final Set<String> START_NOUN_TAGS = new HashSet<String>();
    static final Set<String> CONTINUE_NOUN_TAGS = new HashSet<String>();

    static {
        DETERMINER_TAGS.add("abn");
        DETERMINER_TAGS.add("abx");
        DETERMINER_TAGS.add("ap");
        DETERMINER_TAGS.add("ap$");
        DETERMINER_TAGS.add("at");
        DETERMINER_TAGS.add("cd");
        DETERMINER_TAGS.add("cd$");
        DETERMINER_TAGS.add("dt");
        DETERMINER_TAGS.add("dt$");
        DETERMINER_TAGS.add("dti");
        DETERMINER_TAGS.add("dts");
        DETERMINER_TAGS.add("dtx");
        DETERMINER_TAGS.add("od");

        ADJECTIVE_TAGS.add("jj");
        ADJECTIVE_TAGS.add("jj$");
        ADJECTIVE_TAGS.add("jjr");
        ADJECTIVE_TAGS.add("jjs");
        ADJECTIVE_TAGS.add("jjt");
        ADJECTIVE_TAGS.add("*");
        ADJECTIVE_TAGS.add("ql");

        NOUN_TAGS.add("nn");
        NOUN_TAGS.add("nn$");
        NOUN_TAGS.add("nns");
        NOUN_TAGS.add("nns$");
        NOUN_TAGS.add("np");
        NOUN_TAGS.add("np$");
        NOUN_TAGS.add("nps");
        NOUN_TAGS.add("nps$");
        NOUN_TAGS.add("nr");
        NOUN_TAGS.add("nr$");
        NOUN_TAGS.add("nrs");

        PRONOUN_TAGS.add("pn");
        PRONOUN_TAGS.add("pn$");
        PRONOUN_TAGS.add("pp$");
        PRONOUN_TAGS.add("pp$$");
        PRONOUN_TAGS.add("ppl");
        PRONOUN_TAGS.add("ppls");
        PRONOUN_TAGS.add("ppo");
        PRONOUN_TAGS.add("pps");
        PRONOUN_TAGS.add("ppss");

        VERB_TAGS.add("vb");
        VERB_TAGS.add("vbd");
        VERB_TAGS.add("vbg");
        VERB_TAGS.add("vbn");
        VERB_TAGS.add("vbz");

        AUXILIARY_VERB_TAGS.add("to");
        AUXILIARY_VERB_TAGS.add("md");
        AUXILIARY_VERB_TAGS.add("be");
        AUXILIARY_VERB_TAGS.add("bed");
        AUXILIARY_VERB_TAGS.add("bedz");
        AUXILIARY_VERB_TAGS.add("beg");
        AUXILIARY_VERB_TAGS.add("bem");
        AUXILIARY_VERB_TAGS.add("ben");
        AUXILIARY_VERB_TAGS.add("ber");
        AUXILIARY_VERB_TAGS.add("bez");

        ADVERB_TAGS.add("rb");
        ADVERB_TAGS.add("rb$");
        ADVERB_TAGS.add("rbr");
        ADVERB_TAGS.add("rbt");
        ADVERB_TAGS.add("rn");
        ADVERB_TAGS.add("ql");
        ADVERB_TAGS.add("*");  // negation

        PUNCTUATION_TAGS.add("'");
        // PUNCTUATION_TAGS.add("``");
        // PUNCTUATION_TAGS.add("''");
        PUNCTUATION_TAGS.add(".");
        PUNCTUATION_TAGS.add("*");
        // PUNCTUATION_TAGS.add(","); // miss comma-separated phrases
        // PUNCTUATION_TAGS.add("(");
        // PUNCTUATION_TAGS.add(")");
        // PUNCTUATION_TAGS.add("*"); // negation "not"
        // PUNCTUATION_TAGS.add("--");
        // PUNCTUATION_TAGS.add(":");
    }

    static {

        START_NOUN_TAGS.addAll(DETERMINER_TAGS);
        START_NOUN_TAGS.addAll(ADJECTIVE_TAGS);
        START_NOUN_TAGS.addAll(NOUN_TAGS);
        START_NOUN_TAGS.addAll(PRONOUN_TAGS);

        CONTINUE_NOUN_TAGS.addAll(START_NOUN_TAGS);
        CONTINUE_NOUN_TAGS.addAll(ADVERB_TAGS);
        CONTINUE_NOUN_TAGS.addAll(PUNCTUATION_TAGS);

        START_VERB_TAGS.addAll(VERB_TAGS);
        START_VERB_TAGS.addAll(AUXILIARY_VERB_TAGS);
        START_VERB_TAGS.addAll(ADVERB_TAGS);

        CONTINUE_VERB_TAGS.addAll(START_VERB_TAGS);
        CONTINUE_VERB_TAGS.addAll(PUNCTUATION_TAGS);
    }

    private final HmmDecoder mPosTagger;
    private final TokenizerFactory mTokenizerFactory;

    public BasalNPVPChunker(HmmDecoder posTagger,
                         TokenizerFactory tokenizerFactory) {
        mPosTagger = posTagger;
        mTokenizerFactory = tokenizerFactory;
    }

    public Chunking chunk(CharSequence cSeq) {
        char[] cs = Strings.toCharArray(cSeq);
        return chunk(cs,0,cs.length);
    }

    public Chunking chunk(char[] cs, int start, int end) {

        // tokenize
        List<String> tokenList = new ArrayList<String>();
        List<String> whiteList = new ArrayList<String>();
        Tokenizer tokenizer = mTokenizerFactory.tokenizer(cs,start,end-start);
        tokenizer.tokenize(tokenList,whiteList);
        String[] tokens
            = tokenList.<String>toArray(new String[tokenList.size()]);
        String[] whites
            = whiteList.<String>toArray(new String[whiteList.size()]);

        // part-of-speech tag
        Tagging<String> tagging = mPosTagger.tag(tokenList);

        ChunkingImpl chunking = new ChunkingImpl(cs,start,end);
        int startChunk = 0;
        for (int i = 0; i < tagging.size(); ) {
            startChunk += whites[i].length();

            if (START_NOUN_TAGS.contains(tagging.tag(i))) {
                int endChunk = startChunk + tokens[i].length();
                ++i;
                while (i < tokens.length && CONTINUE_NOUN_TAGS.contains(tagging.tag(i))) {
                    endChunk += whites[i].length() + tokens[i].length();
                    ++i;
                }
                // this separation allows internal punctuation, but not final punctuation
                int trimmedEndChunk = endChunk;
                for (int k = i;
                     --k >= 0 && PUNCTUATION_TAGS.contains(tagging.tag(k)); ) {
                    trimmedEndChunk -= (whites[k].length() + tokens[k].length());
                }
                if (startChunk >= trimmedEndChunk) {
                    startChunk = endChunk;
                    continue;
                }
                Chunk chunk
                    = ChunkFactory.createChunk(startChunk,trimmedEndChunk,"noun");
                chunking.add(chunk);
                startChunk = endChunk;

            } else if (START_VERB_TAGS.contains(tagging.tag(i))) {
                int endChunk = startChunk + tokens[i].length();
                ++i;
                while (i < tokens.length && CONTINUE_VERB_TAGS.contains(tagging.tag(i))) {
                    endChunk += whites[i].length() + tokens[i].length();
                    ++i;
                }
                int trimmedEndChunk = endChunk;
                for (int k = i;
                     --k >= 0 && PUNCTUATION_TAGS.contains(tagging.tag(k)); ) {
                    trimmedEndChunk -= (whites[k].length() + tokens[k].length());
                }
                if (startChunk >= trimmedEndChunk) {
                    startChunk = endChunk;
                    continue;
                }
                Chunk chunk = ChunkFactory.createChunk(startChunk,trimmedEndChunk,"verb");
                chunking.add(chunk);
                startChunk = endChunk;

            } else {
                startChunk += tokens[i].length();
                ++i;
            }
        }
        return chunking;
    }

    public static void main(String[] args) {

        // parse input params
        File hmmFile = new File("models/pos-en-general-brown.HiddenMarkovModel");
        int cacheSize = Integer.valueOf(1000);
        FastCache<String,double[]> cache = new FastCache<String,double[]>(cacheSize);

        // read HMM for pos tagging
        HiddenMarkovModel posHmm;
        try {
            posHmm
                = (HiddenMarkovModel)
                AbstractExternalizable.readObject(hmmFile);
        } catch (IOException e) {
            System.out.println("Exception reading model=" + e);
            e.printStackTrace(System.out);
            return;
        } catch (ClassNotFoundException e) {
            System.out.println("Exception reading model=" + e);
            e.printStackTrace(System.out);
            return;
        }

        // construct chunker
        HmmDecoder posTagger  = new HmmDecoder(posHmm,null,cache);
        TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
        BasalNPVPChunker chunker = new BasalNPVPChunker(posTagger,tokenizerFactory);


        // apply chunker & pos tagger
        for (int i = 2; i < args.length; ++i) {
            System.out.println("\n" + args[i]);
            String[] tokens 
                = tokenizerFactory
                .tokenizer(args[i].toCharArray(),0,args[i].length())
                .tokenize();
            List<String> tokenList = Arrays.asList(tokens);
            Tagging<String> tagging = posTagger.tag(tokenList);
            for (int j = 0; j < tokenList.size(); ++j)
                System.out.print(tokens[j] + "/" + tagging.tag(j) + " ");
            System.out.println();

            Chunking chunking = chunker.chunk(args[i]);
            CharSequence cs = chunking.charSequence();
            for (Chunk chunk : chunking.chunkSet()) {
                String type = chunk.type();
                int start = chunk.start();
                int end = chunk.end();
                CharSequence text = cs.subSequence(start,end);
                System.out.println("  " + type + "(" + start + "," + end + ") " + text);
            }
        }
    }
}package com.lingpipe.cookbook.chapter7;

import com.aliasi.chunk.Chunk;

class CorefChunk implements Chunk {
	String mType;
	int mId;
	int mStart;
	int mEnd;

	public CorefChunk(int id, String type, int start, int end) {
		mType = type;
		mId = id;
		mStart = start;
		mEnd = end;
	}
	@Override
	public int start() {
		// TODO Auto-generated method stub
		return mStart;
	}
	@Override
	public int end() {
		// TODO Auto-generated method stub
		return mEnd;
	}
	@Override
	public String type() {
		// TODO Auto-generated method stub
		return mType;
	}
	@Override
	public double score() {
		// TODO Auto-generated method stub
		return 0.0d;				
	}
	@Override
	public String toString() {
		return mType + " " + mId + " " + mStart + " " + mEnd + " ";
	}
}package com.lingpipe.cookbook.chapter7;

import com.aliasi.chunk.*;
import com.aliasi.coref.*;
import com.aliasi.sentences.*;
import com.aliasi.tokenizer.*;
import com.aliasi.util.*;


import java.io.*;
import java.util.*;
import java.util.regex.*;

public class Coreference {

	static Pattern MALE_EN_PRONOUNS = Pattern.compile("\\b(He|he|Him|him)\\b");
	static Pattern FEMALE_EN_PRONOUNS = Pattern.compile("\\b(She|she|Her|her)\\b");

	static File MODEL_FILE
	= new File("models/ne-en-news-muc6.AbstractCharLmRescoringChunker");

	static TokenizerFactory TOK_FACTORY
	= new IndoEuropeanTokenizerFactory();

	static SentenceModel SENT_MODEL
	   = new IndoEuropeanSentenceModel();
	
	public static void main(String[] args) 
			throws ClassNotFoundException, IOException {
		Chunker neChunker 
		= (Chunker) AbstractExternalizable.readObject(MODEL_FILE);
		Chunker sentenceChunker 
		= new SentenceChunker(TOK_FACTORY,SENT_MODEL);
		BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
		while (true) {
			System.out.print("Enter text followed by new line\n>");
			String text = reader.readLine();
			MentionFactory mf = new EnglishMentionFactory();  
			WithinDocCoref coref = new WithinDocCoref(mf);
			Chunking sentenceChunking
			= sentenceChunker.chunk(text);
			Iterator<Chunk> sentenceIt 
			= sentenceChunking.chunkSet().iterator();
			for (int sentenceNum = 0; sentenceIt.hasNext(); ++sentenceNum) {
				Chunk sentenceChunk = sentenceIt.next();
				String sentenceText 
				= text.substring(sentenceChunk.start(),
						sentenceChunk.end());
				System.out.println("Sentence Text=" + sentenceText);
				Chunking mentionChunking
				= neChunker.chunk(sentenceText);
				Set<Chunk> chunkSet = new TreeSet<Chunk>(Chunk.TEXT_ORDER_COMPARATOR);
				chunkSet.addAll(mentionChunking.chunkSet());
				addRegexMatchingChunks(MALE_EN_PRONOUNS,"MALE_PRONOUN",sentenceText,chunkSet);
				addRegexMatchingChunks(FEMALE_EN_PRONOUNS,"FEMALE_PRONOUN",sentenceText,chunkSet);
				Iterator<Chunk> mentionIt = chunkSet.iterator();
				while (mentionIt.hasNext()) {
					Chunk mentionChunk = (Chunk) mentionIt.next();
					String mentionText
					= sentenceText.substring(mentionChunk.start(),
							mentionChunk.end());
					String mentionType = mentionChunk.type();
					Mention mention = mf.create(mentionText,mentionType);
					int mentionId = coref.resolveMention(mention,sentenceNum);
					System.out.println("     mention text=" + mentionText
							+ " type=" + mentionType
							+ " id=" + mentionId);
				}	
			}
		}
	}

	static void addRegexMatchingChunks(Pattern pattern, String type, String text, Set<Chunk> chunkSet) {
		java.util.regex.Matcher matcher = pattern.matcher(text);
		while (matcher.find()) {
			Chunk regexChunk = ChunkFactory.createChunk(matcher.start(),
					matcher.end(),
					type);
			for (Chunk chunk : chunkSet) {
				if (ChunkingImpl.overlap(chunk,regexChunk)) {
					chunkSet.remove(chunk);
				}
			}
			chunkSet.add(regexChunk);
		}
	}
	
	
}
package com.lingpipe.cookbook.chapter7;
import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.Chunking;
import com.aliasi.classify.PrecisionRecallEvaluation;

import com.aliasi.cluster.HierarchicalClusterer;
import com.aliasi.cluster.ClusterScore;
import com.aliasi.cluster.CompleteLinkClusterer;
import com.aliasi.cluster.SingleLinkClusterer;
import com.aliasi.cluster.Dendrogram;
import com.aliasi.coref.EnglishMentionFactory;
import com.aliasi.coref.Mention;
import com.aliasi.coref.MentionFactory;
import com.aliasi.coref.WithinDocCoref;
import com.aliasi.corpus.ObjectHandler;

import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.Counter;
import com.aliasi.util.Distance;
import com.aliasi.util.Files;
import com.aliasi.util.ObjectToCounterMap;
import com.aliasi.util.Strings;

import com.aliasi.sentences.IndoEuropeanSentenceModel;
import com.aliasi.sentences.SentenceChunker;
import com.aliasi.sentences.SentenceModel;
import com.aliasi.spell.TfIdfDistance;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;


import java.io.*;
import java.util.*;
import java.util.regex.Pattern;

public class JohnSmith {


	static Chunker SENTENCE_CHUNKER;
	static Chunker NAMED_ENTITY_CHUNKER;

	public static void main(String[] args) 
			throws ClassNotFoundException, IOException {
		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		SentenceModel sentenceModel
		= new IndoEuropeanSentenceModel();
		SENTENCE_CHUNKER 
		= new SentenceChunker(tokenizerFactory,sentenceModel);
		File modelFile
		= new File("models/ne-en-news-muc6.AbstractCharLmRescoringChunker");
		NAMED_ENTITY_CHUNKER 
		= (Chunker) AbstractExternalizable.readObject(modelFile);
		TfIdfDocumentDistance tfIdfDist = new TfIdfDocumentDistance(tokenizerFactory);
		File dir = new File("data/johnSmith");
		Set<Set<Document>> referencePartition
		= new HashSet<Set<Document>>();
		for (File catDir : dir.listFiles()) {
			System.out.println("Category from file=" + catDir);
			Set<Document> docsForCat = new HashSet<Document>();
			referencePartition.add(docsForCat);
			for (File file : catDir.listFiles()) {
				Document doc = new Document(file);
				tfIdfDist.train(doc.mText);
				docsForCat.add(doc);
			}
		}
		Set<Document> docSet = new HashSet<Document>();
		for (Set<Document> cluster : referencePartition) {
			docSet.addAll(cluster);
		}
		// eval clusterers
		HierarchicalClusterer<Document> clClusterer
		= new CompleteLinkClusterer<Document>(tfIdfDist);
		Dendrogram<Document> completeLinkDendrogram
		= clClusterer.hierarchicalCluster(docSet);

		HierarchicalClusterer<Document> slClusterer
		= new SingleLinkClusterer<Document>(tfIdfDist);
		Dendrogram<Document> singleLinkDendrogram
		= slClusterer.hierarchicalCluster(docSet);
		System.out.println();
		System.out.println(" --------------------------------------------------------");
		System.out.println("|  K  |  Complete      |  Single        |  Cross         |");
		System.out.println("|     |  P    R    F   |  P    R    F   |  P    R    F   |");
		System.out.println(" --------------------------------------------------------");
		for (int k = 1; k <= docSet.size(); ++k) {
			Set<Set<Document>> clResponsePartition
			= completeLinkDendrogram.partitionK(k);
			Set<Set<Document>> slResponsePartition
			= singleLinkDendrogram.partitionK(k);

			ClusterScore<Document> scoreCL
			= new ClusterScore<Document>(referencePartition,
					clResponsePartition);
			PrecisionRecallEvaluation clPrEval = scoreCL.equivalenceEvaluation();


			ClusterScore<Document> scoreSL
			= new ClusterScore<Document>(referencePartition,
					slResponsePartition);
			PrecisionRecallEvaluation slPrEval = scoreSL.equivalenceEvaluation();

			System.out.printf("| %3d | %3.2f %3.2f %3.2f | %3.2f %3.2f %3.2f \n",
					k,
					clPrEval.precision(),
					clPrEval.recall(),
					clPrEval.fMeasure(),
					slPrEval.precision(),
					slPrEval.recall(),
					slPrEval.fMeasure()
					);
		}
		System.out.println(" --------------------------------------------------------");
		System.out.println("B-cubed eval");
		for (double maxDist = 0.0; maxDist < 1.01; maxDist += .05) {
			HierarchicalClusterer<Document> slClustererThresholded
			= new SingleLinkClusterer<Document>(maxDist,tfIdfDist);
			Set<Set<Document>> thresholdedCluster
			= slClustererThresholded.cluster(docSet);
			ClusterScore<Document> score 
			= new ClusterScore<Document>(referencePartition,thresholdedCluster);
			System.out.printf("Dist: %.2f P: %.2f R: %.2f size:%3d\n", maxDist, score.b3ClusterPrecision(),score.b3ClusterRecall(),thresholdedCluster.size());
		}
	}

	static final Set<String> getCoreferentSents(String targetPhrase, String text) {
		Chunking sentenceChunking
		= SENTENCE_CHUNKER.chunk(text);
		Iterator<Chunk> sentenceIt 
		= sentenceChunking.chunkSet().iterator();
		int targetId = -2;
		MentionFactory mentionFactory = new EnglishMentionFactory();
		WithinDocCoref coref = new WithinDocCoref(mentionFactory);
		Set<String> matchingSentenceAccumulator = new HashSet<String>();
		for (int sentenceNum = 0; sentenceIt.hasNext(); ++sentenceNum) {
			Chunk sentenceChunk = sentenceIt.next();
			String sentenceText 
			= text.substring(sentenceChunk.start(),
					sentenceChunk.end());
			Chunking neChunking
			= NAMED_ENTITY_CHUNKER.chunk(sentenceText);
			Set<Chunk> chunkSet = new TreeSet<Chunk>(Chunk.TEXT_ORDER_COMPARATOR);
			chunkSet.addAll(neChunking.chunkSet());
			Coreference.addRegexMatchingChunks(Pattern.compile("\\bJohn Smith\\b"),"PERSON",sentenceText,chunkSet);
			Coreference.addRegexMatchingChunks(Pattern.compile("\\b(He|he|him|his|His)\\b"),"MALE_PRONOUN",sentenceText,chunkSet);
			Iterator<Chunk> neChunkIt = chunkSet.iterator();
			while (neChunkIt.hasNext()) {
				Chunk neChunk = neChunkIt.next();
				String mentionText
				= sentenceText.substring(neChunk.start(),
						neChunk.end());
				String mentionType = neChunk.type();
				Mention mention = mentionFactory.create(mentionText,mentionType);
				int mentionId = coref.resolveMention(mention,sentenceNum);
				if (targetId == -2 && mentionText.matches(targetPhrase)) {
					targetId = mentionId;
				}
				if (mentionId == targetId) {
					matchingSentenceAccumulator.add(sentenceText);
					System.out.println("Adding " + sentenceText);

					System.out.println("     mention text=" + mentionText
							+ " type=" + mentionType
							+ " id=" + mentionId);
				}
			}
		}
		if (targetId == -2) {
			System.out.println("!!!Missed target doc " + text);
		}
		return matchingSentenceAccumulator;
	}

	static class Document {
		final File mFile;
		final CharSequence mText; 
		final CharSequence mCoreferentText;
		Document(File file) throws IOException {
			mFile = file; // includes name
			mText = Files.readFromFile(file,Strings.UTF8);
			Set<String> coreferentSents = getCoreferentSents(".*John Smith.*",mText.toString());
			StringBuilder sb = new StringBuilder();
			for (String sentence : coreferentSents) {
				sb.append(sentence);
			}
			mCoreferentText = sb.toString();
		}

		public String toString() {
			return mFile.getParentFile().getName() + "/"  + mFile.getName();
		}
	}
}
package com.lingpipe.cookbook.chapter7;

import com.aliasi.chunk.*;
import com.aliasi.classify.PrecisionRecallEvaluation;
import com.aliasi.coref.*;
import com.aliasi.hmm.HiddenMarkovModel;
import com.aliasi.hmm.HmmDecoder;
import com.aliasi.sentences.*;
import com.aliasi.tokenizer.*;
import com.aliasi.util.*;

import java.io.*;
import java.util.*;
import java.util.regex.*;
import java.util.regex.Matcher;

public class ModifiedCoref {

	static Pattern MALE_EN_PRONOUNS = Pattern.compile("\\b(He|he|Him|him)\\b");
	static Pattern FEMALE_EN_PRONOUNS = Pattern.compile("\\b(She|she|Her|her)\\b");

	static File MODEL_FILE
	= new File("models/ne-en-news-muc6.AbstractCharLmRescoringChunker");

	static TokenizerFactory TOK_FACTORY
	= new IndoEuropeanTokenizerFactory();

	static SentenceModel SENT_MODEL
	= new IndoEuropeanSentenceModel();

	static Chunker mNeChunker;

	static Pattern COREF_ANNOT_PATTERN = Pattern.compile("(<(\\d+):(\\w+)>)([\\w\\s]+)(<>)");

	Chunker mBasalNpVpChunker; 

	MentionFactory mMentionFactory;
	WithinDocCoref mCoref;

	public ModifiedCoref() throws IOException, ClassNotFoundException {
		mNeChunker
		= (Chunker) AbstractExternalizable.readObject(MODEL_FILE);
		mMentionFactory = new EnglishMentionFactory();  
		mCoref = new WithinDocCoref(mMentionFactory);
		HiddenMarkovModel posHmm
		= (HiddenMarkovModel) AbstractExternalizable.readObject(new File("models/pos-en-general-brown.HiddenMarkovModel"));

		// construct chunker
		HmmDecoder posTagger  = new HmmDecoder(posHmm,null,null);
		TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
		mBasalNpVpChunker = new BasalNPVPChunker(posTagger,TOK_FACTORY);
	}


	static void test1() throws IOException, ClassNotFoundException {
		ModifiedCoref coref = new ModifiedCoref();

		String truth1 = "<0:PERSON>John<> went to <1:THING>the farm<>.";
		Pair<String,Set<CorefChunk>> pair = createChunks(truth1);
		String rawSent1 = pair.a();
		Set<CorefChunk> truthChunks1 = pair.b();
		Set<CorefChunk> responseChunks1 = coref.resolve(rawSent1,0);
		judge(rawSent1,truthChunks1,responseChunks1);
		new HashSet<CorefChunk>();
		String sent2 = "He wanted to see some cows.";
		String sent3 = "They mooed alot.";
	}

	static boolean matches(CorefChunk chunk1, CorefChunk chunk2) {
		if (chunk1.start() == chunk2.start()
				&& chunk1.end() == chunk2.end()
				&& chunk1.type().equals(chunk2.type())) {
			return true;
		}
		return false;

	}
	static void judge(String text,Set<CorefChunk> truth, Set<CorefChunk> response) {
		for (CorefChunk truthChunk : truth) {
			boolean foundMatch = false;
			for (CorefChunk responseChunk : response) {
				if (matches(truthChunk,responseChunk)) {
					foundMatch = true;
				}
			}
			if (!foundMatch) {
				System.out.println("Failed to find truth chunk in response '" 
						+ text.substring(truthChunk.start(),truthChunk.end()) + "' " + truthChunk);
			}
			else {

			}
		}
		for (CorefChunk responseChunk : response) {
			boolean foundMatch = false;
			for (CorefChunk truthChunk : truth) {
				if (matches(truthChunk,responseChunk)) {
					foundMatch = true;
				}
			}
			if (!foundMatch) {
				System.out.println("Failed to find response chunk in truth '" 
						+ text.substring(responseChunk.start(),responseChunk.end()) + "' " + responseChunk);
			}
		}
	}

	static Pair<String,Set<CorefChunk>> createChunks(String annotatedSent) {
		Set<CorefChunk> retChunks = new HashSet<CorefChunk>();
		Matcher matcher = COREF_ANNOT_PATTERN.matcher(annotatedSent);
		int annotOffset = 0;
		while (matcher.find()) {
			int mentionId = Integer.valueOf(matcher.group(2));
			String type = matcher.group(3);
			int annotLength = matcher.end(1) - matcher.start(1);
			annotOffset += annotLength;
			int mentionStart = matcher.start(4) - annotOffset;
			int mentionEnd = matcher.end(4) - annotOffset;		
			CorefChunk corefChunk = new CorefChunk(mentionId,type,mentionStart,mentionEnd);
			annotOffset += 2;
			retChunks.add(corefChunk);
		}
		String returnString = annotatedSent.replaceAll("<[^<>]*>", "");
		return new Pair<String,Set<CorefChunk>>(returnString,retChunks);
	}

	Set<CorefChunk> resolve(String sentenceText,int sentenceNum){
		Set<CorefChunk> ents = new HashSet<CorefChunk>();
		System.out.println("Sentence Text=" + sentenceText);
		Chunking mentionChunking
		= mNeChunker.chunk(sentenceText);

		//basal NPs
		Chunking basalNpVpChunking = mBasalNpVpChunker.chunk(sentenceText);
		//plurals
		//first person

		Set<Chunk> chunkSet = new TreeSet<Chunk>(Chunk.TEXT_ORDER_COMPARATOR);
		chunkSet.addAll(mentionChunking.chunkSet());
		addRegexMatchingChunks(MALE_EN_PRONOUNS,"MALE_PRONOUN",sentenceText,chunkSet);
		addRegexMatchingChunks(FEMALE_EN_PRONOUNS,"FEMALE_PRONOUN",sentenceText,chunkSet);
		addNonConflictingNpChunks(basalNpVpChunking.chunkSet(),chunkSet);
		Iterator<Chunk> mentionIt = chunkSet.iterator();
		while (mentionIt.hasNext()) {
			Chunk mentionChunk = (Chunk) mentionIt.next();
			String mentionText
			= sentenceText.substring(mentionChunk.start(),
					mentionChunk.end());
			String mentionType = mentionChunk.type();
			Mention mention = mMentionFactory.create(mentionText,mentionType);
			int mentionId = mCoref.resolveMention(mention,sentenceNum);
			System.out.println("     mention text=" + mentionText
					+ " type=" + mentionType
					+ " id=" + mentionId);
			CorefChunk corefChunk = new CorefChunk(mentionId,mentionType,mentionChunk.start(),mentionChunk.end());
			ents.add(corefChunk);
		}
		return ents;
	}	

	public static void main(String[] args) 
			throws ClassNotFoundException, IOException {

		test1();
		// create NE chunker
	}


	static void addNonConflictingNpChunks(Set<Chunk> chunksToAdd,Set<Chunk> existingChunks) {

		Iterator<Chunk> it = chunksToAdd.iterator();
		while (it.hasNext()) {
			Iterator<Chunk> itExistingChunks = existingChunks.iterator();
			Chunk chunk = (Chunk) it.next();
			boolean conflictFound = false;
			while(itExistingChunks.hasNext()) {
				Chunk existingChunk = itExistingChunks.next();
				if (overlap(chunk.start(),chunk.end(),
						existingChunk.start(),existingChunk.end())) {
					conflictFound = true;
				}
			}
			if (!conflictFound) {
				existingChunks.add(chunk);
			}
		}
	}


	static void addRegexMatchingChunks(Pattern pattern, String type, String text, Set<Chunk> chunkSet) {
		java.util.regex.Matcher matcher = pattern.matcher(text);
		int pos = 0;
		while (matcher.find(pos)) {
			Chunk regexChunk = ChunkFactory.createChunk(matcher.start(),
					matcher.end(),
					type);
			Iterator it = chunkSet.iterator();
			while (it.hasNext()) {
				Chunk chunk = (Chunk) it.next();
				if (overlap(chunk.start(),chunk.end(),
						regexChunk.start(),regexChunk.end())) {
					it.remove();
				}
			}
			chunkSet.add(regexChunk);
			pos = matcher.end();
		}
	}

	static boolean overlap(int start1, int end1,
			int start2, int end2) {
		return java.lang.Math.max(start1,start2)
				< java.lang.Math.min(end1,end2);
	}
}
package com.lingpipe.cookbook.chapter7;

import com.aliasi.chunk.*;
import com.aliasi.coref.*;
import com.aliasi.sentences.*;
import com.aliasi.tokenizer.*;
import com.aliasi.util.*;


import java.io.*;
import java.util.*;
import java.util.regex.*;

public class NamedEntityCoreference {
	static TokenizerFactory mTokenizerFactory;
	static Chunker mSentenceChunker;
	static Chunker mNamedEntChunker;
	
	public static void main(String[] args) 
			throws ClassNotFoundException, IOException {
	
		String inputDoc = args.length > 0 ? args[0] : "data/simpleCoref.txt";
		System.out.println("Reading in file :" + inputDoc);
		mTokenizerFactory
		= IndoEuropeanTokenizerFactory.INSTANCE;
		SentenceModel sentenceModel
		= new IndoEuropeanSentenceModel();
		Chunker sentenceChunker 
		= new SentenceChunker(mTokenizerFactory,sentenceModel);
		File modelFile
		= new File("models/ne-en-news-muc6.AbstractCharLmRescoringChunker");
		Chunker namedEntChunker 
		= (Chunker) AbstractExternalizable.readObject(modelFile);
		MentionFactory mf = new EnglishMentionFactory();  
		WithinDocCoref coref = new WithinDocCoref(mf);
		File doc = new File(inputDoc);
		String text = Files.readFromFile(doc,Strings.UTF8);
		Chunking sentenceChunking
		= sentenceChunker.chunk(text);
		Iterator sentenceIt 
		= sentenceChunking.chunkSet().iterator();
		for (int sentenceNum = 0; sentenceIt.hasNext(); ++sentenceNum) {
			Chunk sentenceChunk = (Chunk) sentenceIt.next();
			String sentenceText 
			= text.substring(sentenceChunk.start(),
					sentenceChunk.end());
			System.out.println("Sentence Text=" + sentenceText);
			Chunking neChunking
			= namedEntChunker.chunk(sentenceText);
			for (Chunk neChunk : neChunking.chunkSet()) {
				String mentionText
					= sentenceText.substring(neChunk.start(),
						neChunk.end());
				String mentionType = neChunk.type();
				Mention mention = mf.create(mentionText,mentionType);
				int mentionId = coref.resolveMention(mention,sentenceNum);
				System.out.println("     mention text=" + mentionText
						+ " type=" + mentionType
						+ " id=" + mentionId);
			}
		}
	}
}
package com.lingpipe.cookbook.chapter7;

import com.aliasi.spell.TfIdfDistance;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.util.Distance;
import com.lingpipe.cookbook.chapter7.JohnSmith.Document;

public class TfIdfDocumentDistance implements Distance<Document> {

	TfIdfDistance mTfIdfDistance;

	public TfIdfDocumentDistance (TokenizerFactory tokenizerFactory) {
		mTfIdfDistance = new TfIdfDistance(tokenizerFactory);
	}
        
    public void train(CharSequence text) {
    	mTfIdfDistance.handle(text);
    }

	@Override
	public double distance(Document doc1, Document doc2) {
		// TODO Auto-generated method stub
		//return mTfIdfDistance.distance(doc1.mCoreferentText,doc2.mCoreferentText);
		return mTfIdfDistance.distance(doc1.mText,doc2.mText);
	}
	
}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.util.Streams;
//import com.aliasi.util.XML;

import com.aliasi.xml.DelegatingHandler;
import com.aliasi.xml.TextAccumulatorHandler;
import com.aliasi.xml.SAXWriter;

import java.io.BufferedInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStream;
import java.io.OutputStream;

import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.dict.MapDictionary;
import com.aliasi.dict.DictionaryEntry;
import com.aliasi.dict.ExactDictionaryChunker;


import org.xml.sax.Attributes;
import org.xml.sax.InputSource;
import org.xml.sax.SAXException;
import org.xml.sax.XMLReader;

import org.xml.sax.helpers.DefaultHandler;
import org.xml.sax.helpers.XMLReaderFactory;

public class Dictionary {

    private final Set<Long> mDictionaryIdSet;
    private final DictionaryEntitySpec[] mDictionaryEntitySpecs;
    private final String[] mStopPhrases;


    public Dictionary() {
        mDictionaryEntitySpecs = new DictionaryEntitySpec[0];
        mStopPhrases = new String[0];
        mDictionaryIdSet = new HashSet<Long>();
    }

    public Dictionary(DictionaryEntitySpec[] entitySpecs,
                      String[] stopPhrases) {
        mDictionaryEntitySpecs = entitySpecs;
        mStopPhrases = stopPhrases;
        mDictionaryIdSet = new HashSet<Long>();
        for (DictionaryEntitySpec spec : entitySpecs)
            mDictionaryIdSet.add(spec.id());
    }

    public boolean isDictionaryId(Long id) {
        return mDictionaryIdSet.contains(id);
    }

    public ExactDictionaryChunker chunker(TokenizerFactory tokenizerFactory,
                                          boolean returnAllMatches,
                                          boolean caseSensitive) {
        MapDictionary<String> dictionary 
            = new MapDictionary<String>();
        for (DictionaryEntitySpec entitySpec
                 : mDictionaryEntitySpecs) {
            String[] aliases = entitySpec.aliases();
            boolean[] xdcs = entitySpec.xdcs();
            for (int i = 0; i < aliases.length; ++i)
                if (xdcs[i])
                    dictionary.addEntry(new DictionaryEntry<String>(aliases[i],
                                                                    entitySpec.type()));
        }
        
        ExactDictionaryChunker chunker
            = new ExactDictionaryChunker(dictionary,
                                         tokenizerFactory,
                                         returnAllMatches,caseSensitive);
        return chunker;
    }
    

    public static Dictionary read(InputSource in)
        throws SAXException, IOException {

        XMLReader xmlReader = XMLReaderFactory.createXMLReader();
        //xmlReader.setFeature(XML.VALIDATION_FEATURE,false);

        Handler xmlHandler = new Handler();
        xmlReader.setContentHandler(xmlHandler);
        xmlReader.setDTDHandler(xmlHandler);
        xmlReader.setEntityResolver(xmlHandler);
        xmlReader.parse(in);
        return xmlHandler.getDictionary();
    }

    public void writeTo(SAXWriter writer)
        throws IOException, SAXException {

        writer.setDTDString("<!DOCTYPE dictionary PUBLIC \"alias-i-entity-dictionary\" \"http://www.alias-i.com/dtd/entity-dictionary.dtd\">");

        writer.startDocument();

        writer.characters("\n");
        writer.startSimpleElement("dictionary");

        writer.characters("\n");

        for (DictionaryEntitySpec entitySpec : mDictionaryEntitySpecs)
            entitySpec.writeContentTo(writer);

        writer.characters("\n");
        writer.startSimpleElement("stoplist");
        for (String phrase : mStopPhrases) {
            writer.characters("\n  ");
            writer.startSimpleElement("phrase");
            writer.characters(phrase.toCharArray());
            writer.endSimpleElement("phrase");
        }
        writer.characters("\n");
        writer.endSimpleElement("stoplist");
        writer.characters("\n\n");
        writer.endSimpleElement("dictionary");
        writer.characters("\n");
        writer.endDocument();
    }

    public DictionaryEntitySpec[] entitySpecs() {
        return mDictionaryEntitySpecs;
    }

    public String[] stopPhrases() {
        return mStopPhrases;
    }


    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("ENTITIES\n");
        for (DictionaryEntitySpec entitySpec : mDictionaryEntitySpecs)
            sb.append(entitySpec + "\n");
        sb.append("\nSTOP PHRASES\n");
        for (String stopPhrase : mStopPhrases)
            sb.append(stopPhrase + "\n");
        return sb.toString();
    }

    static class Handler extends DelegatingHandler {
        final DictionaryEntitySpec.Handler mEntitySpecHandler;
        final List<DictionaryEntitySpec> mEntitySpecList
            = new ArrayList<DictionaryEntitySpec>();
        final TextAccumulatorHandler mStopPhraseHandler
            = new TextAccumulatorHandler();
        final Set<String> mStopPhraseSet = new HashSet<String>();
        int mTimeoutInDays;

        public Handler() {
            mEntitySpecHandler = new DictionaryEntitySpec.Handler(this);
            setDelegate("entity",mEntitySpecHandler);
            setDelegate("phrase",mStopPhraseHandler);
        }
        public void finishDelegate(String qName, DefaultHandler handler) {
            if ("entity".equals(qName))
                mEntitySpecList.add(mEntitySpecHandler
                                    .getDictionaryEntitySpec());
            else if ("phrase".equals(qName))
                mStopPhraseSet.add(mStopPhraseHandler.getText());
        }
        Dictionary getDictionary() {
            DictionaryEntitySpec[] entitySpecs
                = mEntitySpecList
                .<DictionaryEntitySpec>
                toArray(new DictionaryEntitySpec[mEntitySpecList.size()]);
            String[] stopPhrases
                = mStopPhraseSet
                .<String>toArray(new String[mStopPhraseSet.size()]);
            return new Dictionary(entitySpecs,stopPhrases);
        }
        public InputSource resolveEntity(String publicId, String systemId)
            throws SAXException {

            if (DICTIONARY_PUBLIC_ID.equals(publicId)) {
                InputStream in
                    = this.getClass().getResourceAsStream(DTD_FILE_NAME);

                if (in == null)
                    throw new RuntimeException("need to put DTD=" + publicId + " on classpath");

                return new InputSource(in);
            }
            return super.resolveEntity(publicId,systemId);
        }


    }

    static final String DICTIONARY_PUBLIC_ID
        = "alias-i-entity-dictionary";

    static final String DTD_FILE_NAME
        = "entity-dictionary.dtd";


    public static Dictionary read(File dictFile, String charEncoding)
        throws IOException, SAXException {

        FileInputStream in = null;
        BufferedInputStream bufIn = null;
        try {
            in = new FileInputStream(dictFile);
            bufIn = new BufferedInputStream(in);
            InputSource inSource  = new InputSource(bufIn);
            inSource.setEncoding(charEncoding);
            Dictionary dictionary = Dictionary.read(inSource);
            return dictionary;
        } finally {
            Streams.closeInputStream(bufIn);
            Streams.closeInputStream(in);
        }
    }

    public static void main(String[] args) throws IOException, SAXException {
        Dictionary dictionary = Dictionary.read(new File(args[0]), com.aliasi.util.Strings.UTF8);

        System.out.println(dictionary);  // Dictionary.main

        SAXWriter writer = new SAXWriter(System.out,"ISO-8859-1");  // Dictionary.main
        dictionary.writeTo(writer);
    }


}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.xml.DelegatingHandler;
import com.aliasi.xml.DelegateHandler;
import com.aliasi.xml.SAXWriter;
import com.aliasi.xml.TextAccumulatorHandler;

import org.xml.sax.Attributes;
import org.xml.sax.SAXException;

import org.xml.sax.helpers.DefaultHandler;

import java.io.IOException;

import java.util.ArrayList;
import java.util.List;

public class DictionaryEntitySpec {

    final String mCanonicalName;
    final String[] mAliases;
    final boolean[] mXDCs;
    final String mType;
    final long mId;
    final boolean mAllowSpeculativeAliases;

    static String toString(boolean val) {
        return val ? "1" : "0";
    }

    void writeContentTo(SAXWriter writer) 
        throws IOException, SAXException {
        
        Attributes atts
            = SAXWriter
            .createAttributes("id",Long.toString(mId),
                              "type",mType,
                              "canonical",mCanonicalName,
                              "speculativeAliases",toString(mAllowSpeculativeAliases));
        writer.characters("\n");
        writer.startSimpleElement("entity",atts);
        for (int i = 0; i < mAliases.length; ++i) {
            writer.characters("\n  ");
            writer.startSimpleElement("alias","xdc",toString(mXDCs[i]));
            writer.characters(mAliases[i]);
            writer.endSimpleElement("alias");
        }
        writer.characters("\n");
        writer.endSimpleElement("entity");
        writer.characters("\n");
    }

    public DictionaryEntitySpec(String canonicalName,
                                String[] aliases,
                                boolean[] xdcs,
                                String type,
                                long id,
                                boolean allowSpeculativeAliases) {
        mCanonicalName = canonicalName;
        mAliases = aliases;
        mXDCs = xdcs;
        mType = type;
        mId = id;
        mAllowSpeculativeAliases = allowSpeculativeAliases;
    }

    public String canonicalName() {
        return mCanonicalName;
    }

    public String[] aliases() {
        return mAliases;
    }

    public boolean[] xdcs() {
        return mXDCs;
    }

    public String type() {
        return mType;
    }

    public long id() {
        return mId;
    }

    public boolean allowSpeculativeAliases() {
        return mAllowSpeculativeAliases;
    }

    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("<" + mId + "> ");
        sb.append(mCanonicalName);
        sb.append(": " + mType);
        sb.append(" / " + (mAllowSpeculativeAliases ? "+spec" : "-spec"));
        sb.append(" { ");
        for (int i = 0; i < mAliases.length; ++i) {
            if (i > 0) sb.append("; ");
            sb.append(mAliases[i]);
            if (mXDCs[i]) sb.append(" [+xdc]");
        }
        sb.append(" }");
        return sb.toString();
    }

    static class Handler extends DelegateHandler {
        final List<String> mAliasList = new ArrayList<String>();
        final List<Boolean> mXDCList = new ArrayList<Boolean>();
        final AliasHandler mAliasHandler;
        long mId;
        String mType;
        String mCanonicalName;
        boolean mAllowSpeculativeAliases;
        public Handler(DelegatingHandler parent) {
            super(parent);
            mAliasHandler = new AliasHandler();
            setDelegate("alias",mAliasHandler);
        }
        public void startDocument()
            throws SAXException {

            mAliasList.clear();
            mXDCList.clear();
            mId = -1L;
            mType = null;
            mCanonicalName = null;
            super.startDocument();
        }
        public void startElement(String url, String localName,
                                 String qName, Attributes atts)
            throws SAXException {

            if ("entity".equals(qName)) {
                mId = Long.parseLong(atts.getValue("id"));
                mType = atts.getValue("type");
                mCanonicalName = atts.getValue("canonical");
                mAllowSpeculativeAliases = "1".equals(atts.getValue("speculativeAliases"));
            }
            super.startElement(url,localName,qName,atts);
        }
        public void finishDelegate(String qName, DefaultHandler handler) {
            if ("alias".equals(qName)) {
                mAliasList.add(mAliasHandler.getText());
                mXDCList.add(Boolean.valueOf(mAliasHandler.mXDC));
            }
        }
        public DictionaryEntitySpec getDictionaryEntitySpec() {
            String[] aliases
                = mAliasList.<String>toArray(new String[mAliasList.size()]);
            boolean[] xdcs
                = new boolean[mXDCList.size()];
            for (int i = 0; i < xdcs.length; ++i)
                xdcs[i] = mXDCList.get(i).booleanValue();
            return new DictionaryEntitySpec(mCanonicalName,
                                            aliases,
                                            xdcs,
                                            mType,
                                            mId,
                                            mAllowSpeculativeAliases);
        }
    }

    static class AliasHandler extends TextAccumulatorHandler {
        boolean mXDC;
        public void startElement(String url, String localName,
                                 String qName, Attributes atts)
            throws SAXException {

            if ("alias".equals(qName)) {
                mXDC = "1".equals(atts.getValue("xdc"));
            }
            super.startElement(url,localName,qName,atts);
        }
    }

}
package com.lingpipe.cookbook.chapter7.tracker;



import com.aliasi.tokenizer.TokenizerFactory;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collection;
import java.util.Collections;
import java.util.HashSet;
import java.util.List;
import java.util.Set;

import java.io.IOException;
import java.io.Writer;


public class Entity {

    private final long mId;

    private String mEntityType;
    private boolean mAllowSpeculativeAliases = true;
    private String[] mUserXdcPhrases;
    private String[] mUserNonXdcPhrases;
    private String[] mSpeculativeXdcPhrases;
    private String[] mSpeculativeNonXdcPhrases;

    public Entity(long id,
                  String entityType,
                  Set<String> userXdcPhraseSet,
                  Set<String> userNonXdcPhraseSet,
                  Set<String> speculativeXdcPhraseSet,
                  Set<String> speculativeNonXdcPhraseSet) {

        mId = id;
        mEntityType = entityType;
        mUserXdcPhrases = toArray(userXdcPhraseSet);
        mUserNonXdcPhrases = toArray(userNonXdcPhraseSet);
        mSpeculativeXdcPhrases = toArray(speculativeXdcPhraseSet);
        mSpeculativeNonXdcPhrases = toArray(speculativeNonXdcPhraseSet);

    }

    /*
    public Entity(long id,
                  String entityType,
                  String[] userXdcPhraseSet,
                  String[] userNonXdcPhraseSet,
                  String[] speculativeXdcPhraseSet,
                  String[] speculativeNonXdcPhraseSet) {
        mId = id;
        mEntityType = entityType;

        mUserXdcPhrases = userXdcPhraseSet;
        mUserNonXdcPhrases = userNonXdcPhraseSet;
        mSpeculativeXdcPhrases = speculativeXdcPhraseSet;
        mSpeculativeNonXdcPhrases = speculativeNonXdcPhraseSet;
    }
    */

    // READS

    public void writeTo(Writer writer) throws IOException {
        writer.write(Long.toString(mId));
        writer.write(' ');
        writer.write(mEntityType);
        writer.write(' ');
        writer.write(mAllowSpeculativeAliases ? " +spec" : " -spec");
        writer.write('\n');
        write(writer,mUserNonXdcPhrases," +user,-xdc");
        write(writer,mUserXdcPhrases," +user,+xdc");
        write(writer,mSpeculativeNonXdcPhrases," -user,-xdc");
        write(writer,mSpeculativeXdcPhrases," -user,+xdc");
        write(writer,mSpeculativeNonXdcPhrases," -user,-xdc");
    }

    void write(Writer writer, String[] aliases, String desc) throws IOException {
        for (String alias : aliases) {
            writer.write(alias);
            writer.write(desc);
            writer.write('\n');
        }
    }

    public boolean isUserDefined() {
        return mUserXdcPhrases.length > 0
            || mUserNonXdcPhrases.length > 0;
    }

    public long id() {
        return mId;
    }

    public String type() {
        return mEntityType;
    }

    public boolean allowSpeculativeAliases() {
        return mAllowSpeculativeAliases;
    }

    public Set<String> tokens(TokenizerFactory tokenizerFactory) {
        Set<String> tokenSet = new HashSet<String>();
        addTokens(mUserXdcPhrases,tokenSet,tokenizerFactory);
        addTokens(mUserNonXdcPhrases,tokenSet,tokenizerFactory);
        addTokens(mSpeculativeXdcPhrases,tokenSet,tokenizerFactory);
        addTokens(mSpeculativeNonXdcPhrases,tokenSet,tokenizerFactory);
        return tokenSet;
    }

    public Set<String> xdcPhrases() {
        Set<String> xdcPhraseSet = new HashSet<String>();
        for (String phrase : mUserXdcPhrases)
            xdcPhraseSet.add(phrase);
        for (String phrase : mSpeculativeXdcPhrases)
            xdcPhraseSet.add(phrase);
        return xdcPhraseSet;
    }

    public Set<String> nonXdcPhrases() {
        Set<String> nonXdcPhraseSet = new HashSet<String>();
        for (String phrase : mUserNonXdcPhrases)
            nonXdcPhraseSet.add(phrase);
        for (String phrase : mSpeculativeNonXdcPhrases)
            nonXdcPhraseSet.add(phrase);
        return nonXdcPhraseSet;
    }

    public boolean containsPhrase(String phrase) {
        Set xdcPhraseSet = xdcPhrases();
        Set nonXdcPhraseSet = nonXdcPhrases();
        return xdcPhraseSet.contains(phrase) ||
        nonXdcPhraseSet.contains(phrase);
    }


    public int hashCode() {
        return (int) mId;
    }

    public boolean equals(Object that) {
        return (that instanceof Entity)
            && ((Entity)that).mId == mId;
    }

  public String toString() {
        return "id=" + mId
            + " type=" + mEntityType
            + " userDefined=" + isUserDefined()
            + " allowSpec=" + mAllowSpeculativeAliases
            + " user XDC=[" + join("| ",mUserXdcPhrases) + "]"
            + " user non-XDC=[" + join("| ",mUserNonXdcPhrases) + "]"
            + " spec XDC=[" + join("| ",mSpeculativeXdcPhrases) + "]"
            + " spec non-XDC=[" + join("| ",mSpeculativeNonXdcPhrases) + "]";
    }

   private String join(String joiner, String[] list) {
        StringBuffer sb = new StringBuffer();
        for(int i = 0; i < list.length; ++i) {
            sb.append(list[i]);
            if (i + 1 != list.length) 
                sb.append(joiner);
        }
        return sb.toString();
    }

    // WRITES

    public void setType(String type) {
        mEntityType = type;
    }

    /*    public void unifyType(String entityType) {
        if (type().equals(entityType)) return;
        setType(TTMatchers.unifyEntityTypes(type(),entityType));
    }
    */


    public void merge(DictionaryEntitySpec entitySpec) {
        if (entitySpec.id() != id()) {
            String msg = "non-matching ids."
                + " this=" + id()
                + " spec=" + entitySpec.id();
            throw new IllegalArgumentException(msg);
        }

        mEntityType = entitySpec.type();
        mAllowSpeculativeAliases = entitySpec.allowSpeculativeAliases();

        String[] aliases = entitySpec.aliases();
        boolean[] xdcs = entitySpec.xdcs();


        Set<String> userXdcPhraseSet = new HashSet<String>();
        Set<String> userNonXdcPhraseSet = new HashSet<String>();

        for (int i = 0; i < aliases.length; ++i) {
            if (xdcs[i])
                userXdcPhraseSet.add(aliases[i]);
            else
                userNonXdcPhraseSet.add(aliases[i]);
        }
        mUserXdcPhrases = toArray(userXdcPhraseSet);
        mUserNonXdcPhrases = toArray(userNonXdcPhraseSet);

        if (entitySpec.allowSpeculativeAliases()) {
            mSpeculativeXdcPhrases = filter(mSpeculativeXdcPhrases,aliases);
            mSpeculativeNonXdcPhrases = filter(mSpeculativeNonXdcPhrases,aliases);
        } else {
            mSpeculativeXdcPhrases = EMPTY_STRING_ARRAY;
            mSpeculativeNonXdcPhrases = EMPTY_STRING_ARRAY;
        }
    }

    public void addUserXdcPhrase(String phrase) {
        mUserXdcPhrases = add(mUserXdcPhrases,phrase);
    }

    public void addSpeculativeXdcPhrase(String phrase) {
        if (!allowSpeculativeAliases()) return;
        for (String userPhrase : mUserXdcPhrases)
            if (phrase.equals(userPhrase))
                return;
        mSpeculativeXdcPhrases = add(mSpeculativeXdcPhrases,phrase);
    }

    public void addSpeculativeNonXdcPhrase(String phrase) {
        if (!allowSpeculativeAliases()) return;
        mSpeculativeNonXdcPhrases = add(mSpeculativeNonXdcPhrases,phrase);
    }


    private static final String[] EMPTY_STRING_ARRAY
        = new String[0];

    private static void addTokens(String[] phrases, Set<String> tokenSet,
                   TokenizerFactory tokenizerFactory) {
        for (String phrase : phrases) {
            char[] cs = phrase.toCharArray();
            for (String token : tokenizerFactory.tokenizer(cs,0,cs.length))
                 tokenSet.add(token);
        }
    }

    private static String[] toArray(Collection<String> xs) {
        if (xs == null) return EMPTY_STRING_ARRAY;
        String[] result = new String[xs.size()];
        xs.toArray(result);
        return result;
    }

    private static String[] add(String[] xs, String y) {
        for (String x : xs)
            if (x.equals(y))
                return xs;
        String[] result = new String[xs.length + 1];
        for (int i = 0; i < xs.length; ++i)
            result[i] = xs[i];
        result[result.length-1] = y;
        return result;
    }

    private static String[] filter(String[] toKeep, String[] toRemove) {
        Set<String> result = new HashSet<String>((toKeep.length + toRemove.length) * 2);
        for (String alias : toKeep)
            result.add(alias);
        for (String alias : toRemove)
            result.remove(alias);
        return toArray(result);
    }



}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.ChunkFactory;
import com.aliasi.chunk.Chunking;
import com.aliasi.chunk.ChunkingImpl;
import com.aliasi.util.Strings;
import com.aliasi.hmm.HiddenMarkovModel;
import com.aliasi.hmm.HmmDecoder;

import com.aliasi.tag.Tagging;

import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.dict.MapDictionary;
import com.aliasi.dict.DictionaryEntry;
import com.aliasi.dict.ExactDictionaryChunker;

import java.util.Arrays;
import java.util.Set;
import java.util.HashSet;
import java.util.HashMap;
import java.util.Map;

import org.xml.sax.InputSource;
import org.xml.sax.SAXException;

import java.io.IOException;

public class EntityPhraseChunker implements Chunker {

    protected Chunker mNeSpeculativeChunker;
    protected Chunker mNePronounChunker = new PronounChunker();

    protected ExactDictionaryChunker mNeDictionaryChunker;
    //    private Dictionary mEntityDictionary;

    protected HmmDecoder mPosTagger;

    protected Set<String> mStopPhraseSet;

    protected String[]  mStopSubstringList = {};

    protected Map<String,Integer> mPhraseCounts;

    public EntityPhraseChunker (Chunker neSpeculativeChunker,
                                HmmDecoder posTagger,
                                ExactDictionaryChunker dictionary,
                                String[] stopPhrases,
                                String[] stopSubstrings,
                                Map<String,Integer> phraseCounts) {
        mNeSpeculativeChunker = neSpeculativeChunker;
        mPosTagger = posTagger;
        mNeDictionaryChunker = dictionary;
        setStopPhrases(stopPhrases);
        mStopSubstringList = stopSubstrings;
        mPhraseCounts = phraseCounts;
    }

    public Chunking chunk (CharSequence cSeq) {
        char[] cs = Strings.toCharArray(cSeq);
        return chunk(cs, 0, cs.length);
    }

    public Chunking chunk( char[] characters, int start, int end) {
        String text = String.valueOf(characters);
        ChunkingImpl chunking = new ChunkingImpl(characters, 0, characters.length);
        Set<Chunk> entityChunkSet = new HashSet<Chunk>();

        add(entityChunkSet,mNePronounChunker,text);
        //addDictionaryWithPosFilter(entityChunkSet,text);
        add(entityChunkSet,mNeDictionaryChunker,text);
        //addDictionaryWithPosPhraseCountFilter(entityChunkSet,text);

        addSpeculativeWithPosPhraseCountFilter(entityChunkSet,mNeSpeculativeChunker,text);
        for (Chunk chunk : entityChunkSet)
            chunking.add(chunk);
        return chunking;
    }


    public void setDictionaryChunker(ExactDictionaryChunker chunker) {
        mNeDictionaryChunker = chunker;
    }

    public synchronized void setStopPhrases(String[] stopPhrases) {
        Set<String> stopPhraseSet = new HashSet<String>();
        for (String phrase : stopPhrases)
            stopPhraseSet.add(phrase);
        mStopPhraseSet = stopPhraseSet;
    }

    public synchronized void setStopSubstringList(String[] stopSubstrings) {
        mStopSubstringList = stopSubstrings;
    }


    //boy this is getting ugly

    protected void addSpeculative(Set<Chunk> chunkSet, Chunker chunker, String input) {
        if (chunker == null) return;
        Set<Chunk> nextChunkSet = chunker.chunk(input).chunkSet();
        if (nextChunkSet == null) return; // probably not nec.
        for (Chunk chunk : nextChunkSet) {
            if (chunk.end() - chunk.start() < 2)
                continue;
            String text = input.substring(chunk.start(),chunk.end());
            if (mStopPhraseSet.contains(text)) continue;
            if (overlap(chunk,chunkSet)) continue;
            boolean pass = true;
            for (int i = 0; i < mStopSubstringList.length; ++i) {
                if (text.contains(mStopSubstringList[i]))
                    pass = false;
            }
            if (pass) {
                Chunk normChunk = ChunkFactory.createChunk(chunk.start(),chunk.end(),"OTHER");
                chunkSet.add(normChunk);
            }
        }
    }

    protected void addSpeculativeWithPosPhraseCountFilter (Set<Chunk> chunkSet, Chunker chunker, String input) {
        if (chunker == null) return;
        Set<Chunk> nextChunkSet = chunker.chunk(input).chunkSet();
        if (nextChunkSet == null) return; // probably not nec.
        PosTagFilter posTagFilter = new PosTagFilter(input,
                                                     mPosTagger,
                                                     mNeDictionaryChunker.tokenizerFactory());
        for (Chunk chunk : nextChunkSet) {
            if (chunk.end() - chunk.start() < 2)
                continue;
            String text = input.substring(chunk.start(),chunk.end());
            if (failPhraseCountFilter(text) && posTagFilter.fail(text)) {
                //              System.out.println("Skipping: " + text);
                continue;
            }
            if (mStopPhraseSet.contains(text)) continue;
            if (overlap(chunk,chunkSet)) continue;
            boolean pass = true;
            for (int i = 0; i < mStopSubstringList.length; ++i) {
                if (text.contains(mStopSubstringList[i]))
                    pass = false;
            }
            if (pass) {
                Chunk normChunk = ChunkFactory.createChunk(chunk.start(),chunk.end(),"OTHER");
                chunkSet.add(normChunk);
            }
        }
    }

    protected void addSpeculativeWithPosFilter(Set<Chunk> chunkSet, Chunker chunker, String input) {
        if (chunker == null) return;
        Set<Chunk> nextChunkSet = chunker.chunk(input).chunkSet();
        if (nextChunkSet == null) return; // probably not nec.
        char[] inputChar = input.toCharArray();
        Tokenizer tokenizer = mNeDictionaryChunker.tokenizerFactory().tokenizer(inputChar,0,inputChar.length);
        String[] tokens = tokenizer.tokenize();
        Tagging<String> taggedStrings = mPosTagger.tag(Arrays.asList(tokens));//this mess is due to version change from 3 to 4
	String[] tags = taggedStrings.tags().toArray(new String[0]);//continued messyness. 
        //        for (int i = 0; i < tokens.length; ++i)
            //          System.out.println(tokens[i] + " " + tags[i]);

        Chunk[] chunkArray
            = nextChunkSet.<Chunk>toArray(new Chunk[nextChunkSet.size()]);
        Arrays.sort(chunkArray,Chunk.TEXT_ORDER_COMPARATOR);
        int posIndex = 0;
        for (int i = 0; i < chunkArray.length; ++i) {
            boolean rejectEntity = false;
            Chunk chunk = chunkArray[i];

            if (chunk.end() - chunk.start() < 2)
                continue;
            String text = input.substring(chunk.start(),chunk.end());
            if (mStopPhraseSet.contains(text)) continue;
            if (overlap(chunk,chunkSet)) continue;
            boolean pass = true;
            for (int j = 0; j < mStopSubstringList.length; ++j) {
                if (text.contains(mStopSubstringList[j]))
                    pass = false;
            }
            if (pass) {
                Chunk normChunk = ChunkFactory.createChunk(chunk.start(),chunk.end(),"OTHER");
                chunkSet.add(normChunk);
            }
        }
    }

    protected void addDictionaryWithPosFilter(Set<Chunk> chunkSet, String input) {
        if (mNeDictionaryChunker == null) return;
        Set<Chunk> nextChunkSet = mNeDictionaryChunker.chunk(input).chunkSet();
        if (nextChunkSet == null) return; // probably not nec.
        Chunk[] chunkArray
            = nextChunkSet.<Chunk>toArray(new Chunk[nextChunkSet.size()]);
        Arrays.sort(chunkArray,Chunk.TEXT_ORDER_COMPARATOR);

        PosTagFilter posTagFilter = new PosTagFilter(input,
                                                     mPosTagger,
                                                     mNeDictionaryChunker.tokenizerFactory());

        for (int i = 0; i < chunkArray.length; ++i) {
            Chunk chunk = chunkArray[i];
            String text = input.substring(chunk.start(),chunk.end());
            if (posTagFilter.fail(text)) continue;
            if (mStopPhraseSet.contains(text)) continue;
            if (overlap(chunk,chunkSet)) continue;
            chunkSet.add(chunk);
            //System.out.println("accepting " + text);
        }
    }

    protected void addDictionaryWithPosPhraseCountFilter(Set<Chunk> chunkSet, String input) {
        if (mNeDictionaryChunker == null) return;
        Set<Chunk> nextChunkSet = mNeDictionaryChunker.chunk(input).chunkSet();
        if (nextChunkSet == null) return; // probably not nec.
        Chunk[] chunkArray
            = nextChunkSet.<Chunk>toArray(new Chunk[nextChunkSet.size()]);
        Arrays.sort(chunkArray,Chunk.TEXT_ORDER_COMPARATOR);

        PosTagFilter posTagFilter = new PosTagFilter(input,
                                                     mPosTagger,
                                                     mNeDictionaryChunker.tokenizerFactory());

        for (int i = 0; i < chunkArray.length; ++i) {
            Chunk chunk = chunkArray[i];
            String text = input.substring(chunk.start(),chunk.end());
            if (failPhraseCountFilter(text) && posTagFilter.fail(text)) {
                //              System.out.println("Skipping: " + text);
                continue;
            }
            if (mStopPhraseSet.contains(text)) continue;
            if (overlap(chunk,chunkSet)) continue;
            chunkSet.add(chunk);
            //System.out.println("accepting " + text);
        }
    }

    protected boolean failPhraseCountFilter (String phrase) {
        Integer count = mPhraseCounts.get(phrase);
        if (count == null)
            return false;
        if (count > 5)
            return true;
        return false;
    }

    class PosTagFilter {

        int mPosIndex;
        String[] mTokens;
        String[] mTags;
        HmmDecoder mHmm;
        TokenizerFactory mTokenizerFactory;

        public PosTagFilter(String input,
                            HmmDecoder hmm,
                            TokenizerFactory entityTokenizer) {
            mPosIndex = 0;
            mHmm = hmm;
            mTokenizerFactory = entityTokenizer;

            char[] inputChar
                = input.toCharArray();
            Tokenizer tokenizer
                = mTokenizerFactory.tokenizer(inputChar,0,inputChar.length);

            mTokens = tokenizer.tokenize();
	    Tagging<String> tagging = mPosTagger.tag(Arrays.asList(mTokens));
	    
            mTags = tagging.tags().toArray(new String[0]);

            //         for (int i = 0; i < mTokens.length; ++i)
            //     System.out.println(mTokens[i] + " " + mTags[i]);
        }

        protected boolean fail (String entityPhrase) {

            char[] textChar = entityPhrase.toCharArray();
            Tokenizer entityTokenizer = mTokenizerFactory.tokenizer(textChar,0,textChar.length);
            boolean rejectEntity = false;
            String[] entityTokens = entityTokenizer.tokenize();
            //            for (int i = 0; i < entityTokens.length; ++i)
                //                System.out.println(entityTokens[i]);
            int entityMatchIndex = 0;
            while (entityMatchIndex < entityTokens.length) {

                if (entityTokens[entityMatchIndex].equals(mTokens[mPosIndex]) ) {
                    if (! mTags[mPosIndex].equals("np")
                        && ! mTokens[mPosIndex].equals("-")) {
                        rejectEntity = true;
                        //System.out.println("rejecting: ");
                    }
                    ++entityMatchIndex;
                }
                ++mPosIndex;
            }
            return rejectEntity;
        }
    }

    // incredibly inefficient quadratic and growing
    // only add new chunks if don't overlap
    protected void add(Set<Chunk> chunkSet, Chunker chunker, String input) {
        if (chunker == null) return;
        Set<Chunk> nextChunkSet = chunker.chunk(input).chunkSet();
        if (nextChunkSet == null) return; // probably not nec.
        for (Chunk chunk : nextChunkSet) {
            String text = input.substring(chunk.start(),chunk.end());
            System.out.println("Got " + text);
            if (mStopPhraseSet.contains(text)) continue;
            if (overlap(chunk,chunkSet)) continue;
            chunkSet.add(chunk);
        }
    }


    protected static boolean overlap(Chunk chunk, Set<Chunk> chunkSet) {
        for (Chunk chunk2 : chunkSet)
            if (overlap(chunk,chunk2))
                return true;
        return false;
    }

    protected static boolean overlap(Chunk c1, Chunk c2) {
        return overlapLeft(c1,c2) || overlapLeft(c2,c1);
    }

    protected static boolean overlapLeft(Chunk c1, Chunk c2) {
        return c2.start() < c1.end()
            && c1.end() <= c2.end();
    }

}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;

import com.aliasi.util.ObjectToSet;
import com.aliasi.util.Streams;

import java.io.IOException;
import java.io.OutputStream;
import java.io.OutputStreamWriter;
import java.io.Writer;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

public class EntityUniverse {


    /** The <code>EnitityUniverse</code> is an in memory representation of the
     * global entities within a data collection. This is a minimal
     * implementation that contains only the information required for
     * <code>XDocCoref</code> to function.
     * @author Bob Carpenter, Breck Baldwin
     * @version 1.0
     * @since tracker 1.0
     **/


    private long mLastId = FIRST_SYSTEM_ID;

    private final TokenizerFactory mTokenizerFactory;

    private final Map<Long,Entity> mIdToEntity
        = new HashMap<Long,Entity>();

    private final ObjectToSet<String,Entity> mXdcPhraseToEntitySet
        = new ObjectToSet<String,Entity>();

    

    public EntityUniverse(TokenizerFactory tokenizerFactory) {
        mTokenizerFactory = tokenizerFactory;
    }


    // READERS

    public TokenizerFactory tokenizerFactory() {
        return mTokenizerFactory;
    }

    public Entity getEntity(long id) {
        return mIdToEntity.get(id);
    }

    public Set<Entity> xdcEntitiesWithPhrase(String phrase) {
        return mXdcPhraseToEntitySet.getSet(normalPhrase(phrase));
    }

    public Set<Entity> userDefinedEntitySet() {
        Set<Entity> result = new HashSet<Entity>();
        for (Entity entity : mIdToEntity.values())
            if (entity.isUserDefined())
                result.add(entity);
        return result;
    }

    // closes stream being written to
    public void writeTo(OutputStream out, String charset)
        throws IOException {
        Writer writer = null;
        try {
            writer =new OutputStreamWriter(out,charset);
            writer.write("lastId=" + mLastId+"\n");
            Long[] ids = mIdToEntity.keySet().<Long>toArray(new Long[mIdToEntity.size()]);
            Arrays.sort(ids);
            for (Long id : ids) {
                Entity entity = mIdToEntity.get(id);
                writer.write("\n");
                entity.writeTo(writer);
            }
        } finally {
            Streams.closeWriter(writer);
        }
    }

    // WRITERS

    // coming in from re-reading old docs
    public void addHistoricEntity(Long id, String type,
                                  Set<String> aliases) {
        Entity entity = mIdToEntity.get(id);
        if (entity != null) {
            entity.setType(type);
            for (String alias : aliases)
                addXdcPhrase(alias,entity);
        } else if (id < FIRST_SYSTEM_ID) {
            return; // dead user entity not in mIdToEntity map
        } else {
            entity = new Entity(id,type,aliases,null,null,null);
            add(entity);
        }
    }

    public void addPhraseToEntity(String phrase, Entity entity) {
        if (entity.containsPhrase(phrase))
            return;
        Set<String> xdcPhrases = entity.xdcPhrases();
        Set<String> nonXdcPhrases = entity.nonXdcPhrases();
        boolean hasMultiWordPhrases = false;
        for (String entityPhrase : xdcPhrases)
            if (entityPhrase.indexOf(' ') > -1 )
                hasMultiWordPhrases = true;

        for (String entityPhrase : nonXdcPhrases)
            if (entityPhrase.indexOf(' ') > -1 )
                hasMultiWordPhrases = true;

        if (isXdcPhrase(phrase,hasMultiWordPhrases))
            addXdcPhrase(phrase,entity);
        else
            entity.addSpeculativeNonXdcPhrase(phrase);
    }

    //if there is a longer string, then disallow single tokens
    //

    public Entity createEntitySpeculative(Set<String> phrases,
                                          String entityType) {
        Set<String> nonXdcPhrases = new HashSet<String>();
        Set<String> xdcPhrases = new HashSet<String>();
        boolean hasMultiWordPhrases = false;
        for (String phrase : phrases)
            if (phrase.indexOf(' ') > -1 )
                hasMultiWordPhrases = true;
        for (String phrase : phrases) {
            if (isXdcPhrase(phrase,hasMultiWordPhrases))
                xdcPhrases.add(phrase);
            else
                nonXdcPhrases.add(phrase);
        }
        while (mIdToEntity.containsKey(++mLastId)) ; // move up to next untaken ID
        Entity entity = new Entity(mLastId,entityType,
                                   null,null,xdcPhrases,nonXdcPhrases);
        add(entity);
        return entity;
    }

    public boolean isXdcPhrase(String phrase, boolean hasMultiWordPhrase) {

        if (mXdcPhraseToEntitySet.containsKey(normalPhrase(phrase)))
            return false;
        if (phrase.indexOf(' ') == -1 && hasMultiWordPhrase) {
            return false;
        }
        if (PronounChunker.isPronominal(phrase))
            return false;
        return true;
    }

    public Entity createEntityDictionary(DictionaryEntitySpec entitySpec) {
        Set<String> userXdcPhraseSet = new HashSet<String>();
        Set<String> userNonXdcPhraseSet = new HashSet<String>();
        String[] aliases = entitySpec.aliases();
        boolean[] xdcs = entitySpec.xdcs();
        for (int i = 0; i < aliases.length; ++i) {
            if (xdcs[i])
                userXdcPhraseSet.add(aliases[i]);
            else
                userNonXdcPhraseSet.add(aliases[i]);
        }
        long id = entitySpec.id();
        if (mIdToEntity.containsKey(id)) {
            String msg = "Entity ID already taken; must revise instead of create.";
            throw new IllegalArgumentException(msg);
        }
        Entity entity = new Entity(id,
                                   entitySpec.type(),
                                   userXdcPhraseSet,
                                   userNonXdcPhraseSet,
                                   null,
                                   null);
        add(entity);
        return entity;
    }


    public void updateEntitySpec(DictionaryEntitySpec entitySpec) {
        long id = entitySpec.id();
        Entity entity = mIdToEntity.get(id);
        if (entity == null) {
            createEntityDictionary(entitySpec);
            return;
        }
        mergeEntitySpec(entity,entitySpec);
    }


    public void remove(Entity entity) {
        long id = entity.id();
        mIdToEntity.remove(id);

        for (String rawPhrase : entity.xdcPhrases()) {
            String phrase = rawPhrase.toLowerCase();
            removePhraseToXdcToEntitySet(phrase,entity);
            // ?? allow other phrases in non-xdc to become xdc
        }
    }

    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append("Entity Universe: Last id= " + mLastId + "\n");
        sb.append("XDC Phrase to Entity mapping\n");
        for (String phrase : mXdcPhraseToEntitySet.keySet()) {
            sb.append("\n");
            sb.append(phrase);
            sb.append(": ");
            for (Entity entity : mXdcPhraseToEntitySet.get(phrase)) {
                sb.append(" " + entity.id());
            }
        }
        sb.append("\nEntities");
        for (Long key : mIdToEntity.keySet()) {
            sb.append("\n\n");
            sb.append(mIdToEntity.get(key).toString());
        }
        return sb.toString();
    }

    public void add(Entity e) {
        if (e.id() > mLastId)
            mLastId = e.id();
        mIdToEntity.put(new Long(e.id()),e);
        for (String phrase : e.xdcPhrases()) {
            addPhraseToXdcToEntitySet(phrase,e);
        }
    }

    public String normalPhrase(String phrase) {
        return concatenateNormalTokens(normalTokens(phrase));
    }

    public static String normalizeWhitespace(String whitespace) {
        return whitespace.length() > 0 ? " " : "";
    }

    public static String concatenateNormalTokens(String[] toks) {
        if (toks.length < 1) return "";
        if (toks.length == 1) return toks[0];
        StringBuilder sb = new StringBuilder(toks[0]);
        for (int i = 1; i < toks.length; ++i) {
            sb.append(' ');
            sb.append(toks[i]);
        }
        return sb.toString();
    }


    public String[] normalTokens(String phrase) {
        List<String> tokenList = new ArrayList<String>();
        char[] cs = phrase.toCharArray();
        Tokenizer tokenizer
            = mTokenizerFactory
            .tokenizer(cs,0,cs.length);
        for (String token : tokenizer)
            if (!nonEntityToken(token))
                tokenList.add(token);
        return tokenList.<String>toArray(new String[tokenList.size()]);
    }

    private static boolean nonEntityToken(String token) {
        char[] cs = token.toCharArray();
        for (int i = 0; i < cs.length; ++i)
            if (Character.isLetter(cs[i])
                || Character.isDigit(cs[i])) return false;
        return true;
    }

    private void addPhraseToXdcToEntitySet(String phrase,Entity entity) {
        mXdcPhraseToEntitySet.addMember(normalPhrase(phrase),entity);
    }

    private void removePhraseToXdcToEntitySet(String phrase,Entity entity) {
        mXdcPhraseToEntitySet.removeMember(normalPhrase(phrase),entity);
    }

    private void addXdcPhrase(String phrase, Entity entity) {
        addPhraseToXdcToEntitySet(phrase,entity);
        entity.addSpeculativeXdcPhrase(phrase);
    }


    private void mergeEntitySpec(Entity entity, DictionaryEntitySpec entitySpec) {
        if (!entitySpec.allowSpeculativeAliases()) {
            for (String xdcAlias : entity.xdcPhrases())
                removePhraseToXdcToEntitySet(xdcAlias,entity);
            String[] aliases = entitySpec.aliases();
            boolean[] xdcs = entitySpec.xdcs();
            for (int i = 0; i < aliases.length; ++i)
                if (xdcs[i])
                    addPhraseToXdcToEntitySet(aliases[i].toLowerCase(),entity);
        } else {
            String[] aliases = entitySpec.aliases();
            boolean[] xdcs = entitySpec.xdcs();
            for (int i = 0; i < aliases.length; ++i) {
                if (xdcs[i]) {
                    addPhraseToXdcToEntitySet(aliases[i],entity);
                } else {
                    removePhraseToXdcToEntitySet(aliases[i],entity);
                }
            }
        }
        // could just completely re-index after merge instead of above
        entity.merge(entitySpec);
    }
    
    public long getLastId () {
        return mLastId;
    }


    static final long FIRST_SYSTEM_ID = 1000000000;



}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.xml.DelegateHandler;
import com.aliasi.xml.DelegatingHandler;
import com.aliasi.xml.TextAccumulatorHandler;

//import com.aliasi.util.XML;

import java.io.IOException;
import java.io.InputStream;

import java.util.ArrayList;
import java.util.List;

import org.xml.sax.Attributes;
import org.xml.sax.InputSource;
import org.xml.sax.SAXException;
import org.xml.sax.XMLReader;

import org.xml.sax.helpers.DefaultHandler;
import org.xml.sax.helpers.XMLReaderFactory;

public class InputDocument {

    private final String mTitle;
    private final String mContent;
    private final String mId;

    public InputDocument(String id, String title, String content) {
        mId = id;
        mTitle = title;
        mContent = content;
    }

    public String title() {
        return mTitle;
    }

    public String content() {
        return mContent;
    }

    public String id() {
        return mId;
    }

    public String toString() {
        StringBuilder sb = new StringBuilder();
        sb.append(id());
        sb.append(" ");
        sb.append(title());
        sb.append("\n");
        sb.append(content());
        return sb.toString();
    }

    public static InputDocument[] parse(InputSource in)
        throws IOException, SAXException {
        XMLReader xmlReader = XMLReaderFactory.createXMLReader();
        //xmlReader.setFeature(XML.VALIDATION_FEATURE,false);

        Handler xmlHandler = new Handler();
        xmlReader.setContentHandler(xmlHandler);
        xmlReader.setDTDHandler(xmlHandler);
        xmlReader.setEntityResolver(xmlHandler);
        try {
            xmlReader.parse(in);
        } catch (IOException e) {
            return xmlHandler.documents();
        } catch (SAXException e) {
            return xmlHandler.documents();
        }
        return xmlHandler.documents();
    }

    static class Handler extends DelegatingHandler {
        List<InputDocument> mInputDocumentList
            = new ArrayList<InputDocument>();
        DocumentHandler mDocumentHandler;

        public Handler() {
            mDocumentHandler = new DocumentHandler(this);
            setDelegate("doc",mDocumentHandler);
        }

        public void startDocument() throws SAXException {
            mInputDocumentList.clear();
            super.startDocument();
        }
        public InputDocument[] documents() {
            return mInputDocumentList
                .<InputDocument>toArray(new InputDocument[mInputDocumentList
                                                          .size()]);
        }
        public void finishDelegate(String qName, DefaultHandler handler) {
            if ("doc".equals(qName)) {
                mInputDocumentList.add(mDocumentHandler.getDocument());
            }
        }
        public InputSource resolveEntity(String publicId, String systemId)
            throws SAXException {


            if (INPUT_DOCS_PUBLIC_ID.equals(publicId)) {
                InputStream in
                    = this.getClass().getResourceAsStream(INPUT_DOCS_FILENAME);

                if (in == null)
                    throw new RuntimeException("need to put DTD=" + publicId + " on classpath");

                return new InputSource(in);
            }
            return super.resolveEntity(publicId,systemId);
        }

    }

    static String INPUT_DOCS_PUBLIC_ID = "alias-i-tracker-input-documents";
    static String INPUT_DOCS_FILENAME = "input-documents.dtd";

    static class DocumentHandler extends DelegateHandler {
        private String mId;
        private final TextAccumulatorHandler mTitleHandler;
        private final TextAccumulatorHandler mContentHandler;
        public DocumentHandler(DelegatingHandler parent) {
            super(parent);
            mTitleHandler = new TextAccumulatorHandler();
            mContentHandler = new TextAccumulatorHandler();
            setDelegate("title",mTitleHandler);
            setDelegate("content",mContentHandler);
        }
        public void startDocument()
            throws SAXException {
            mTitleHandler.reset();
            mContentHandler.reset();
            mId = null;
            super.startDocument();
        }
        public void startElement(String url, String localName,
                                 String qName, Attributes atts)
            throws SAXException {

            if ("doc".equals(qName)) {
                mId = atts.getValue("id");
            }
            super.startElement(url,localName,qName,atts);
        }
        public InputDocument getDocument() {
            String title = mTitleHandler.getText();
            String content = mContentHandler.getText();
            return new InputDocument(mId,
                                     nullToEmpty(title),
                                     nullToEmpty(content));
        }
        static String nullToEmpty(String title) {
            return title != null ? title : "";
        }

    }

}package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.tokenizer.LowerCaseTokenizerFactory;

/**
 * This class provides a normalized tokenizer that only
 * iterates over normal tokens.  Normalization converts
 * tokens to lower case and eliminates stopped non-normal
 * tokens.
 */
public class NormalizedTokenizerFactory implements TokenizerFactory {

    private final TokenizerFactory mBaseTokenizerFactory;

    public NormalizedTokenizerFactory(TokenizerFactory factory) {
        mBaseTokenizerFactory = new LowerCaseTokenizerFactory(factory);
    }

    public Tokenizer tokenizer(char[] ch, int start, int length) {
        return mBaseTokenizerFactory.tokenizer(ch,start,length);
        
    }

}


package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunking;

import com.aliasi.util.ObjectToSet;
import com.aliasi.util.Strings;
//import com.aliasi.util.XML;

import com.aliasi.xml.DelegatingHandler;
import com.aliasi.xml.SAXWriter;
import com.aliasi.xml.TextAccumulatorHandler;

import java.io.ByteArrayOutputStream;
import java.io.IOException;

import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;

import org.xml.sax.Attributes;
import org.xml.sax.InputSource;
import org.xml.sax.SAXException;
import org.xml.sax.XMLReader;

import org.xml.sax.helpers.DefaultHandler;
import org.xml.sax.helpers.XMLReaderFactory;

public class OutputDocument {

    private final String mId;
    private final Chunking[] mTitleChunkings;
    private final Chunking[] mContentChunkings;

    public OutputDocument(String id,
                          Chunking[] titleChunkings,
                          Chunking[] contentChunkings) {
        mId = id;
        mTitleChunkings = titleChunkings;
        mContentChunkings = contentChunkings;
    }

    public String id() {
        return mId;
    }

    public Chunking[] titleChunkings() {
        return mTitleChunkings;
    }

    public Chunking[] contentChunkings() {
        return mContentChunkings;
    }

    public String toString() {
        try {
            ByteArrayOutputStream bytesOut
                = new ByteArrayOutputStream();
            SAXWriter writer = new SAXWriter(bytesOut,Strings.UTF8);
            writer.startDocument();
            writeTo(writer);
            writer.endDocument();
            byte[] bytes = bytesOut.toByteArray();
            return new String(bytes,Strings.UTF8);
        } catch (Exception e) {
            return "ERROR WRITING";
        }
    }

    public void writeTo(SAXWriter writer)
        throws IOException, SAXException {

        writer.startSimpleElement("doc","id",id());

        writer.startSimpleElement("title");
        int lastSentenceIndex = writeChunkingsTo(mTitleChunkings,writer,0);
        writer.endSimpleElement("title");

        writer.startSimpleElement("content");
        writeChunkingsTo(mContentChunkings,writer,lastSentenceIndex);
        writer.endSimpleElement("content");

        writer.endSimpleElement("doc");
    }

    static int writeChunkingsTo(Chunking[] chunkings,
                                 SAXWriter writer,
                                 int sentenceIndex)
        throws SAXException, IOException {

        for (Chunking chunking : chunkings)
            writeChunkingTo(chunking,writer,sentenceIndex++);
        return sentenceIndex;
    }

    static void writeChunkingTo(Chunking chunking,
                                SAXWriter writer,
                                int sentenceIndex)
        throws SAXException, IOException {

        writer.startSimpleElement("s","index",Integer.toString(sentenceIndex));
        Set<Chunk> chunkSet = chunking.chunkSet();
        char[] cs = Strings.toCharArray(chunking.charSequence());
        Chunk[] chunks = chunkSet.<Chunk>toArray(new Chunk[chunkSet.size()]);
        Arrays.sort(chunks,Chunk.TEXT_ORDER_COMPARATOR);
        int pos = 0;
        for (int i = 0; i < chunks.length; ++i) {
            Chunk chunk = chunks[i];
            int start = chunk.start();
            int end = chunk.end();
            String typeColonId = chunk.type();
            int idx = typeColonId.indexOf(':');
            String entityType = typeColonId.substring(0,idx);
            String id = typeColonId.substring(idx+1);
            writer.characters(cs,pos,start-pos);
            Attributes atts
                = SAXWriter
                .createAttributes("start",Integer.toString(start),
                                  "type",entityType,
                                  "id",id);
            writer.startSimpleElement("entity",atts);
            writer.characters(cs,start,end-start);
            writer.endSimpleElement("entity");
            pos = end;
        }
        writer.characters(cs,pos,cs.length-pos);
        writer.endSimpleElement("s");
    }

    // adds info it finds to ongling collection of entity info
    public static void readMentions(InputSource in,
                                    Map<Long,String> idToType,
                                    ObjectToSet<Long,String> idToAliases)
        throws SAXException, IOException {

        XMLReader xmlReader = XMLReaderFactory.createXMLReader();
	//        xmlReader.setFeature(XML.VALIDATION_FEATURE,false);

        Handler xmlHandler = new Handler(idToType,idToAliases);
        xmlReader.setContentHandler(xmlHandler);
        xmlReader.setDTDHandler(xmlHandler);
        xmlReader.setEntityResolver(xmlHandler);
        xmlReader.parse(in);
    }

    static class Handler extends DelegatingHandler {

        final EntityHandler mEntityHandler;

        final Map<Long,String> mIdToType;

        final ObjectToSet<Long,String> mIdToAliases;

        public Handler(Map<Long,String> idToType,
                       ObjectToSet<Long,String> idToAliases) {
            mEntityHandler = new EntityHandler();
            setDelegate("entity",mEntityHandler);
            mIdToType = idToType;
            mIdToAliases = idToAliases;
        }
        public void finishDelegate(String qName, DefaultHandler handler) {
            if ("entity".equals(qName)) {
                Long id = new Long(mEntityHandler.mId);
                String type = mEntityHandler.mType;
                String alias = mEntityHandler.getText();
                mIdToType.put(id,type); // may override; taken in chrono order, so ok
                mIdToAliases.addMember(id,alias);
            }
        }
    }

    static class EntityHandler extends TextAccumulatorHandler {
        long mId = -1L;
        String mType;
        public void startDocument() {
            mType = null;
            super.startDocument();
        }
        public void startElement(String url, String localName,
                                 String qName, Attributes atts)
            throws SAXException {

            if ("entity".equals(qName)) {
                String idString = atts.getValue("id");
                if (idString == null) {
                    String msg = "No id on entity elt.";
                    throw new SAXException(msg);
                }
                mId = Long.parseLong(idString);
                mType = atts.getValue("type");
            }
            super.startDocument();
        }
    }

    /*
    public static OutputDocument read(InputSource in)
        throws IOException, SAXException {

        XMLReader xmlReader = XMLReaderFactory.createXMLReader();
        xmlReader.setFeature(XML.VALIDATION_FEATURE,false);

        Handler xmlHandler = new Handler();
        xmlReader.setContentHandler(xmlHandler);
        xmlReader.setDTDHandler(xmlHandler);
        xmlReader.setEntityResolver(xmlHandler);
        xmlReader.parse(in);
        return xmlHandler.getDocument();

    }

    static class Handler extends DelegatingHandler {
        ChunksHandler mTitleHandler;
        ChunksHandler mContentHandler;
        String mId;
        public Handler() {
            mTitleHandler = new ChunksHandler(this);
            mContentHandler = new ChunksHandler(this);
            setDelegate("title",mTitleHandler);
            setDelegate("content",mContentHandler);
        }
        public void startDocument()
            throws SAXException {

            mId = null;
            super.startDocument();
        }
        public void startElement(String url, String localName,
                                 String qName, Attributes atts)
            throws SAXException {

            if ("doc".equals(qName)) {
                mId = atts.getValue("id");
            }
            super.startElement(url,localName,qName,atts);
        }
    }

    static class ChunksHandler extends DelegateHandler {

        SentenceHandler mSentenceHandler;
        List<Chunking> mChunkList


        public ChunksHandler(DelgatingHandler parent) {
            super(parent);
            mSentenceHandler = new SentenceHandler();
        }

    }
    */

}package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.chunk.Chunker;

import com.aliasi.tokenizer.TokenizerFactory;

import com.aliasi.util.Files;
import com.aliasi.util.ObjectToSet;
import com.aliasi.util.Streams;
import com.aliasi.util.Strings;

import com.aliasi.xml.SAXWriter;
import com.aliasi.hmm.HmmDecoder;

import java.io.BufferedInputStream;
import java.io.BufferedOutputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.InputStream;
import java.io.IOException;
import java.io.OutputStream;

import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;

import org.xml.sax.InputSource;
import org.xml.sax.SAXException;

public class PersistentTracker {

    private final TokenizerFactory mTokenizerFactory; // keep so we can make new trackers
    private final Chunker mSentenceChunker;
    private final Chunker mNeSpeculativeChunker;

    private final File mSaveDir;
    private final File mSaveDictFile;
    private final File mSaveDocsDir;

    private boolean mDocumentPersistence = true;

    private final Map<String,Integer> mPhraseCounts;
    private final HmmDecoder mPosTagger;

    private Tracker mTracker; // current tracker


    
    public PersistentTracker(TokenizerFactory tokenizerFactory,
                             Chunker sentenceChunker,
                             Chunker neSpeculativeChunker,
                             HmmDecoder posTagger,
                             Map<String,Integer> phraseCounts,
                             File saveDir,
                             boolean saveDocuments,
                             boolean addSpeculativeEntitiesToEntityUniverse)

        throws SAXException, IOException {
        mTokenizerFactory = tokenizerFactory;
        mSentenceChunker = sentenceChunker;
        mNeSpeculativeChunker = neSpeculativeChunker;
        mPosTagger = posTagger;
        mPhraseCounts = phraseCounts;
        mSaveDir = saveDir;
        mSaveDictFile = new File(mSaveDir,"entity-dictionary.xml");
        mSaveDocsDir = new File(mSaveDir,"docs");
        mDocumentPersistence = saveDocuments;
        try {
            mSaveDocsDir.mkdirs();
        } catch (SecurityException e) {
            String msg ="Persist docs dir=" + mSaveDocsDir
                + " exception=" + e;
            throw new IOException(msg);
        }
        init(addSpeculativeEntitiesToEntityUniverse); // going to reset the tracker
        
    }

    public PersistentTracker(TokenizerFactory tokenizerFactory,
                             Chunker sentenceChunker,
                             Chunker neSpeculativeChunker,
                             HmmDecoder posTagger,
                             Map<String,Integer> phraseCounts,
                             File saveDir)
        throws SAXException, IOException {
        this(tokenizerFactory,
                          sentenceChunker,
                          neSpeculativeChunker,
                          posTagger,
                          phraseCounts,
                          saveDir,
                          true,
                          true);

    }

    // READS

    public Tracker tracker() {
        return mTracker;
    }

    // WRITES

    public synchronized OutputDocument processDocument(InputDocument docIn)
        throws SAXException, IOException {
        OutputDocument docOut = mTracker.processDocument(docIn);
        if (mDocumentPersistence) {
            persist(docOut);
        }
        return docOut;
    }


    public synchronized OutputDocument[] processDocuments(InputSource in)
        throws SAXException, IOException {

        OutputDocument[] docs = mTracker.processDocuments(in);
        for (OutputDocument doc : docs)
            persist(doc);
        return docs;
    }


    public synchronized Dictionary resetDictionary(InputSource in)
        throws SAXException, IOException {

        Dictionary dict = Dictionary.read(in);
        mTracker.getMentionChunker()
            .setDictionaryChunker(dict
                                  .chunker(mTokenizerFactory,
                                    RETURN_ALL_MATCHES_FALSE,
                                    CASE_SENSITIVE_FALSE));
        mTracker.getMentionChunker()
            .setStopPhrases(dict.stopPhrases());

        mTracker.setDictionaryInEntityUniverse(dict);

        persist(dict);
        return dict;
    }


    public synchronized void expireDocsBefore(long msSinceEpoch)
        throws IOException, SAXException {

        expireDocs(mSaveDocsDir,msSinceEpoch);
        init(mTracker.mXDocCoref.mAddSpeculativeEntities);
    }



    private void expireDocs(File file, long msSinceEpoch)
        throws IOException {

        String docsDir = "docs";
        if (file.isDirectory()) {
            File[] files = file.listFiles();
            for (File subFile : files)
                expireDocs(subFile,msSinceEpoch);
            if (file.list().length == 0
                && !docsDir.equals(file.getName()) )
                file.delete();
        } else {
            String name = file.getName();
            int baseNameLen = name.length()-".xml".length();
            String baseName = name.substring(0,baseNameLen);
            int lastCharIndex = baseName.indexOf('_');
            if (lastCharIndex >= 0)
                baseName = baseName.substring(0,lastCharIndex);
            try {
                long t = Long.parseLong(baseName);
                if (t < msSinceEpoch) {
                    System.out.println("Deleting file: " + baseName);
                    file.delete();
                }
            } catch (NumberFormatException e) {
                String msg = "Bombing on parsing file baseName=" + baseName
                    + " file=" + file;
                System.out.println(msg);
                throw e;
            }
        }
    }


    private void init(boolean addSpeculativeEntitiesToEntityUniverse) throws IOException, SAXException {
        Dictionary dictionary = null;
        if (mSaveDictFile.exists()) 
            dictionary = Dictionary.read(mSaveDictFile,Strings.UTF8);
        else {
            dictionary = new Dictionary();
        }
        Map<Long,String> idToType = new HashMap<Long,String>();
        ObjectToSet<Long,String> idToAliases = new ObjectToSet<Long,String>();
        initMentions(mSaveDocsDir,idToType,idToAliases);

        // start-up new tracker with dict, idToType, idToAliases


        TokenizerFactory normTok = new NormalizedTokenizerFactory(mTokenizerFactory);
        EntityPhraseChunker entityPhraseChunker
            = new EntityPhraseChunker(mNeSpeculativeChunker,
                                      mPosTagger,
                                      dictionary.chunker(mTokenizerFactory,RETURN_ALL_MATCHES_FALSE,CASE_SENSITIVE_FALSE),
                                      dictionary.stopPhrases(),
                                      STOP_SUBSTRING_LIST,
                                      mPhraseCounts);

        mTracker = new Tracker(normTok,
                               mSentenceChunker,
                               entityPhraseChunker,
                               dictionary,
                               addSpeculativeEntitiesToEntityUniverse);

        addHistoricEntities(idToType,idToAliases);
    }

    private void addHistoricEntities(Map<Long,String> idToType,
                             ObjectToSet<Long,String> idToAliases) {
        for (Long id : idToType.keySet())
            mTracker
                .xDocCoref()
                .entityUniverse()
                .addHistoricEntity(id,
                                   idToType.get(id),
                                   idToAliases.getSet(id));
    }

    private void initMentions(File file,
                      Map<Long,String> idToType,
                      ObjectToSet<Long,String> idToAliases)
        throws SAXException, IOException {

        if (file.isDirectory()) {
            File[] files = file.listFiles();
            Arrays.sort(files);
            for (File subFile : files)
                initMentions(subFile,idToType,idToAliases);
        } else {
            InputStream in = null;
            BufferedInputStream bufIn = null;
            try {
                in = new FileInputStream(file);
                bufIn = new BufferedInputStream(in);
                InputSource inSource = new InputSource(bufIn);
                inSource.setEncoding(Strings.UTF8);
                OutputDocument.readMentions(inSource,idToType,idToAliases);
            } finally {
                Streams.closeInputStream(bufIn);
                Streams.closeInputStream(in);
            }
        }
    }

    private void persist(Dictionary dict) throws IOException, SAXException {
        OutputStream out = null;
        BufferedOutputStream bufOut = null;
        try {
            out = new FileOutputStream(mSaveDictFile);
            bufOut = new BufferedOutputStream(out);
            SAXWriter writer = new SAXWriter(bufOut,Strings.UTF8);
            dict.writeTo(writer);
        } finally {
            Streams.closeOutputStream(bufOut);
            Streams.closeOutputStream(out);
        }
    }

    private void persist(OutputDocument doc) throws IOException, SAXException {
        long t = System.currentTimeMillis();
        File file = msToFile(mSaveDocsDir,t);
        file.getParentFile().mkdirs();
        OutputStream out = null;
        BufferedOutputStream bufOut = null;
        try {
            out = new FileOutputStream(file);
            bufOut = new BufferedOutputStream(out);
            SAXWriter writer = new SAXWriter(bufOut,Strings.UTF8);
            // only single document, not docs; ok with DTD, but need "doc" as top-level elt
            writer.startDocument();
            doc.writeTo(writer);
            writer.endDocument();
        } catch (Exception e) {
            // log("exception");
            try {
                file.delete();
            } catch (Exception e2) {
                String msg = "Exception persisting doc.=" + e
                    + " Exception deleting doc.=" + e2;
                throw new IOException(msg);
            }
        } finally {
            Streams.closeOutputStream(bufOut);
            Streams.closeOutputStream(out);
        }
    }



    private static File msToFile(File saveDir, long ms) {
        String prefix = Long.toString(ms / HUNDRED_BILLION);
        File dir = new File(saveDir,prefix);
        for (long place = TEN_BILLION; place >= HUNDRED_THOUSAND; place /= 10L)
            dir = new File(dir,Long.toString((ms / place) % 10));
        for (int i = 0; ; ++i) {
            String name = Long.toString(ms % HUNDRED_THOUSAND);
            if (i > 0) name += ("_" + i);
            File file = new File(dir,name+".xml");
            if (!file.exists()) return file;
        }
    }

    // #milliseconds; // time
    private static final long THOUSAND = 1000L;  // 1s
    private static final long TEN_THOUSAND = 10L * THOUSAND; // 10s
    private static final long HUNDRED_THOUSAND = 10L * TEN_THOUSAND; // 1.6m
    private static final long MILLION = 10L * HUNDRED_THOUSAND;  // 16.6m
    private static final long TEN_MILLION = 10L * MILLION;  // 2.75h
    private static final long HUNDRED_MILLION = 10L * TEN_MILLION; // 1.2d
    private static final long BILLION = 10L * HUNDRED_MILLION; // 12d
    private static final long TEN_BILLION = 10L * BILLION; // 120d
    private static final long HUNDRED_BILLION = 10L * TEN_BILLION; // 3.3y
    private static final long TRILLION = 10L * HUNDRED_BILLION; // 33y

    public static final boolean RETURN_ALL_MATCHES_FALSE = false;
    public static final boolean CASE_SENSITIVE_FALSE = false;

    public static String[] STOP_SUBSTRING_LIST = {"www",
                                                 ". com",
                                                 ".com",
                                                 ". net",
                                                 ".net",
                                                 ". edu",
                                                 ".edu",
                                                 ".php",
                                                 ".doc",
                                                 "&lt",
                                                 "&gt",
                                                 "&amp",
                                                 "/",
                                                 "''",
                                                 "?",
                                                 ">",
                                                 "<",
                                                 ":",
                                                 "=",
                                                 "[",
                                                 "]",
                                                 "(",
                                                 ")",
                                                 "\"",
                                                  ";",
                                                  "="};


    public static void main(String args[]) throws Exception {
        for (int i = 0; i < 10; ++i) {
            long time = System.currentTimeMillis();
            File file = msToFile(new File("root"),time);
            System.out.println("time=" + time + " file=" + file); // PersistentTracker.main()
        }
    }


}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.dict.MapDictionary;
import com.aliasi.dict.DictionaryEntry;
import com.aliasi.dict.ExactDictionaryChunker;

import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;

import java.util.Set;
import java.util.HashSet;

public class PronounChunker extends ExactDictionaryChunker {

    public PronounChunker() {
        super(createDictionary(),
              IndoEuropeanTokenizerFactory.INSTANCE,
              false,true);
    }

    static MapDictionary createDictionary() {
        MapDictionary dictionary = new MapDictionary();
        for (String phrase : MALE_PRONOUN_SET)
            dictionary.addEntry(new DictionaryEntry(phrase,"MALE_PRONOUN",1.0));
        for (String phrase : FEMALE_PRONOUN_SET)
            dictionary.addEntry(new DictionaryEntry(phrase,"FEMALE_PRONOUN",1.0));
        return dictionary;
    }

    static Set<String> MALE_PRONOUN_SET = new HashSet<String>();
    static Set<String> FEMALE_PRONOUN_SET = new HashSet<String>();
    static Set<String> PRONOUN_SET = new HashSet<String>();
    static {
        MALE_PRONOUN_SET.add("he");
        MALE_PRONOUN_SET.add("him");
        MALE_PRONOUN_SET.add("his");
        MALE_PRONOUN_SET.add("He");
        MALE_PRONOUN_SET.add("Him");
        MALE_PRONOUN_SET.add("His");
        MALE_PRONOUN_SET.add("HE");
        MALE_PRONOUN_SET.add("HIM");
        MALE_PRONOUN_SET.add("HIS");

        FEMALE_PRONOUN_SET.add("she");
        FEMALE_PRONOUN_SET.add("her");
        FEMALE_PRONOUN_SET.add("hers");
        FEMALE_PRONOUN_SET.add("She");
        FEMALE_PRONOUN_SET.add("Her");
        FEMALE_PRONOUN_SET.add("Hers");
        FEMALE_PRONOUN_SET.add("SHE");
        FEMALE_PRONOUN_SET.add("HER");
        FEMALE_PRONOUN_SET.add("HERS");

        PRONOUN_SET.addAll(MALE_PRONOUN_SET);
        PRONOUN_SET.addAll(FEMALE_PRONOUN_SET);
    }

    public static boolean isPronominal(String phrase) {
        return PRONOUN_SET.contains(phrase);
    }

    public static String MALE_PRONOUN = "MALE_PRONOUN";
    public static String FEMALE_PRONOUN = "FEMALE_PRONOUN";


}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.io.FileExtensionFilter;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.Chunking;
import com.aliasi.hmm.HiddenMarkovModel;
import com.aliasi.hmm.HmmDecoder;

import com.aliasi.sentences.SentenceChunker;
import com.aliasi.sentences.SentenceModel;
import com.aliasi.sentences.IndoEuropeanSentenceModel;

import com.aliasi.dict.ExactDictionaryChunker;

import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.tokenizer.TokenizerFactory;

import com.aliasi.util.AbstractExternalizable;
import com.aliasi.util.Files;
import com.aliasi.util.Streams;
import com.aliasi.util.Strings;

import com.aliasi.xml.SAXWriter;

import java.io.File;
import java.io.FileOutputStream;
import java.io.InputStream;
import java.io.ObjectInputStream;
import java.io.OutputStream;

import java.util.Arrays;
import java.util.Set;
import java.util.HashMap;

import org.xml.sax.InputSource;

// run tracker over a directory structure
public class RunTracker {

	// usage  RunTracker <dictFile> <processDocFile> <inputDocDir> <outputDocDir> <neModelName>
	public static void main(String[] args) throws Exception {
		File dictFile = args.length > 0 ? new File(args[0]) : new File("data/xDoc/entity-dictionary.xml");
		File inputDir = args.length > 1 ? new File(args[1]) : new File("data/xDoc/input");
		File outputDir = args.length > 2 ? new File(args[2]) : new File("data/xDoc/output");
		String neModelFileName = args.length > 3 ? args[3] : "models/ne-en-news-muc6.AbstractCharLmRescoringChunker";
		String posModelFileName = args.length > 4 ? args[4] : "models/pos-en-general-brown.HiddenMarkovModel";
		String phraseCountFileName = args.length > 5 ? args[5] : "models/nGrams.txt.gz";

		Dictionary dictionary
		= Dictionary.read(dictFile,com.aliasi.util.Strings.UTF8);
		//System.out.println(dictionary);

		TokenizerFactory tokenizerFactory
		= IndoEuropeanTokenizerFactory.INSTANCE;

		SentenceModel sentenceModel
		= new IndoEuropeanSentenceModel(true,false);

		SentenceChunker sentenceChunker
		= new SentenceChunker(tokenizerFactory,sentenceModel);
		Chunker neChunker = (Chunker) AbstractExternalizable.readObject(new File(neModelFileName));
		HiddenMarkovModel posHmm
		= (HiddenMarkovModel) AbstractExternalizable.readObject(new File(posModelFileName));
		HmmDecoder posTagger= new HmmDecoder(posHmm);

		String phraseCounts =Files.readFromFile(new File(phraseCountFileName),Strings.UTF8);

		String[] lines = phraseCounts.split("\n");

		ExactDictionaryChunker dictChunker = dictionary.chunker(tokenizerFactory,
				PersistentTracker.RETURN_ALL_MATCHES_FALSE,
				PersistentTracker.CASE_SENSITIVE_FALSE);
		EntityPhraseChunker entityPhraseChunker 
		= new EntityPhraseChunker(neChunker,
				posTagger,
				dictChunker,
				dictionary.stopPhrases(),
				PersistentTracker.STOP_SUBSTRING_LIST,
				new HashMap <String, Integer>() );


		boolean addSpeculativeEntities = true;
		EntityUniverse entityUniverse = new EntityUniverse(tokenizerFactory);
		XDocCoref xDocCoref = new XDocCoref(entityUniverse,addSpeculativeEntities);
		TokenizerFactory normalizingTokenizerFactory = new NormalizedTokenizerFactory(tokenizerFactory);
		Tracker tracker
		= new Tracker(normalizingTokenizerFactory,
				sentenceChunker,
				entityPhraseChunker,
				dictionary,
				new XDocCoref(new EntityUniverse(normalizingTokenizerFactory),addSpeculativeEntities));

		// read input docs & write output docs
		for (File file : inputDir.listFiles(new FileExtensionFilter(".xml",false))) {
			System.out.println("\n\nINPUT FILE: " + file);  
			InputSource inSource = new InputSource(file.toURI().toURL().toString());
			inSource.setEncoding(Strings.UTF8);
			OutputDocument[] outDocs = tracker.processDocuments(inSource);
			System.out.println("# outDocs=" + outDocs.length); 
			for (OutputDocument doc : outDocs)
				System.out.println(doc);  
			File outFile = new File(outputDir,file.getName());
			System.out.println("OUTPUT FILE: " + outFile); 
			OutputStream out = new FileOutputStream(outFile);
			SAXWriter writer = new SAXWriter(out,Strings.UTF8);
			writer.startDocument();
			writer.startSimpleElement("docs");
			writer.characters("\n");
			for (OutputDocument doc : outDocs) {
				doc.writeTo(writer);
				writer.characters("\n");
			}
			writer.endSimpleElement("docs");
			writer.endDocument();
			Streams.closeOutputStream(out);
		}
	}

}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.coref.Killer;
import com.aliasi.coref.Matcher;
import com.aliasi.coref.Mention;
import com.aliasi.coref.MentionChain;

import com.aliasi.coref.matchers.ExactPhraseMatch;
import com.aliasi.coref.matchers.EntityTypeMatch;
import com.aliasi.coref.matchers.GenderKiller;
import com.aliasi.coref.matchers.HonorificConflictKiller;
import com.aliasi.coref.matchers.SequenceSubstringMatch;

//import com.aliasi.util.Collections;
import com.aliasi.util.ObjectToSet;
import com.aliasi.util.Strings;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashSet;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;

/**
 * A class providing matching functions and killing functions based on
 * entity type.  The static method {@link
 * #unifyEntityTypes(String,String)} is used to combiene entity types.
 * Because it returns <code>null</code> if the types cannot be
 * combiend, it may be used to test for consistency of types.  This is
 * used by an instance of {@link TypeConflictKiller} assgined to each
 * type.  This class also provides a pair of methods, {@link
 * #addSynonym(String,String)} and {@link #clearSynonyms()} to control
 * the synonym relationship used by the instance of {@link
 * TTSynonymMatch} assigned to each type.
 */
public final class TTMatchers {

    private final Map mTypeToMatchers = new HashMap();

    private final Map mTypeToKillers = new HashMap();

    private final Matcher mExactPhraseMatch
        = new ExactPhraseMatch(1);
    private final Matcher mPersonFemalePronounMatch
        = new EntityTypeMatch(3,TTMentionFactory.FEMALE_PRONOUN_TAG);
    private final Matcher mPersonMalePronounMatch
        = new EntityTypeMatch(3,TTMentionFactory.MALE_PRONOUN_TAG);
    private final Matcher mFemalePronounMatch
        = new EntityTypeMatch(1,TTMentionFactory.FEMALE_PRONOUN_TAG);
    private final Matcher mMalePronounMatch
        = new EntityTypeMatch(1,TTMentionFactory.MALE_PRONOUN_TAG);
    private final Matcher mSequenceSubstringMatch
        = new SequenceSubstringMatch(3);
    public final TTSynonymMatch mSynonymMatch
        = new TTSynonymMatch(3);


    /**
     * Adds the information that the two specified normal phrases
     * are synonyms.  This information is used by a matcher for
     * within-document coreference.
     *
     * @param normalPhrase1 First synonym.
     * @param normalPhrase2 Second synonym.
     */
    public void addSynonym(String normalPhrase1, String normalPhrase2) {
        mSynonymMatch.addSynonym(normalPhrase1,normalPhrase2);
    }


    /**
     * Resets the synonym map so that no phrases are considered
     * synonymous.
     */
    public void clearSynonyms() {
        mSynonymMatch.clearSynonyms();
    }

    /**
     * Returns the unification of the specified types, or
     * <code>null</code> if they cannot be unified.  Unification is
     * symmetric in that
     * <code>unifyEntityTypes(s1,s2).equals(unifyEntityTypes(s2,s1))</code>
     * is <code>true</code>.
     *
     * <P>The rules for unification are as follows:
     *
     * <UL>
     *
     *   <LI>Any type unifies with itself to produce itself.
     *
     *   <LI>Any type unifies with <code>null</code> to produce itself.
     *
     *   <LI><code>LOCATION</code> and <code>ORGANIZATION</code> unify
     *   to produce <code>LOCATION</code>.
     *
     *   <LI><code>PERSON</code> unifies with <code>(FE)MALE</code> or
     *   <code>(FE)MALE_PRONOUN</code> to produce
     *   <code>(FE)MALE</code>.
     *
     *   <LI><code>(FE)MALE</code> unifies with
     *   <code>(FE)MALE_PRONOUN</code> to produce
     *   <code>(FE)MALE</code>.
     *
     * </UL>
     *
     * All unspecified combinations return <code>null</code>.
     *
     * @param type1 First entity type.
     * @param type2 Second entity type.
     * @return Result of unifying first and second entity types.
     */
    static public String unifyEntityTypes(String type1, String type2) {
        if (type1 == null) return type2;
        if (type2 == null) return type1;
        if (type1.equals(type2)) return type1;
        HashMap map2 = (HashMap) UNIFICATION_MAP.get(type1);
        if (map2 == null) {
            return null;
        }
        Object result = map2.get(type2);
        if (result == null) {
            return null;
        }
        return result.toString();
    }

    static private void simpleType(String type) {
        subsumes(type,type);
    }

    static private void subsumes(String  type1, String type2) {
        unify(type1,type2,type2);
    }

    static private void unify(String type1, String type2, String type) {
        setMap(type1,type2,type);
        setMap(type2,type1,type);
    }

    static private void setMap(String type1, String type2, String type) {
        if (!UNIFICATION_MAP.containsKey(type1))
            UNIFICATION_MAP.put(type1,new HashMap());
        HashMap map = (HashMap) UNIFICATION_MAP.get(type1);
        map.put(type2,type);
    }

    static HashMap UNIFICATION_MAP = new HashMap();
    static {
        simpleType(TTMentionFactory.PERSON_TAG);
        simpleType(TTMentionFactory.MALE_TAG);
        simpleType(TTMentionFactory.MALE_PRONOUN_TAG);
        simpleType(TTMentionFactory.FEMALE_TAG);
        simpleType(TTMentionFactory.FEMALE_PRONOUN_TAG);
        simpleType(TTMentionFactory.LOCATION_TAG);
        simpleType(TTMentionFactory.ORGANIZATION_TAG);
        simpleType(TTMentionFactory.OTHER_TAG);

        // subsumes(TTMentionFactory.OTHER_TAG,TTMentionFactory.PERSON_TAG);
        // subsumes(TTMentionFactory.OTHER_TAG,TTMentionFactory.MALE_TAG);
        // subsumes(TTMentionFactory.OTHER_TAG,TTMentionFactory.MALE_PRONOUN_TAG);
        // subsumes(TTMentionFactory.OTHER_TAG,TTMentionFactory.FEMALE_TAG);
        // subsumes(TTMentionFactory.OTHER_TAG,TTMentionFactory.FEMALE_PRONOUN_TAG);
        // subsumes(TTMentionFactory.OTHER_TAG,TTMentionFactory.LOCATION_TAG);
        // subsumes(TTMentionFactory.OTHER_TAG,TTMentionFactory.ORGANIZATION_TAG);

        // subsumes(TTMentionFactory.ORGANIZATION_TAG,TTMentionFactory.PERSON_TAG);
        // subsumes(TTMentionFactory.ORGANIZATION_TAG,TTMentionFactory.MALE_TAG);
        // subsumes(TTMentionFactory.ORGANIZATION_TAG,TTMentionFactory.MALE_PRONOUN_TAG);
        // subsumes(TTMentionFactory.ORGANIZATION_TAG,TTMentionFactory.FEMALE_TAG);
        // subsumes(TTMentionFactory.ORGANIZATION_TAG,TTMentionFactory.FEMALE_PRONOUN_TAG);
        subsumes(TTMentionFactory.ORGANIZATION_TAG,TTMentionFactory.LOCATION_TAG);

        // subsumes(TTMentionFactory.LOCATION_TAG,TTMentionFactory.PERSON_TAG);
        // subsumes(TTMentionFactory.LOCATION_TAG,TTMentionFactory.MALE_TAG);
        // subsumes(TTMentionFactory.LOCATION_TAG,TTMentionFactory.MALE_PRONOUN_TAG);
        // subsumes(TTMentionFactory.LOCATION_TAG,TTMentionFactory.FEMALE_TAG);
        // subsumes(TTMentionFactory.LOCATION_TAG,TTMentionFactory.FEMALE_PRONOUN_TAG);

        subsumes(TTMentionFactory.PERSON_TAG,TTMentionFactory.MALE_TAG);
        subsumes(TTMentionFactory.PERSON_TAG,TTMentionFactory.FEMALE_TAG);

        subsumes(TTMentionFactory.MALE_PRONOUN_TAG,TTMentionFactory.MALE_TAG);

        subsumes(TTMentionFactory.FEMALE_PRONOUN_TAG,TTMentionFactory.FEMALE_TAG);

        unify(TTMentionFactory.PERSON_TAG,TTMentionFactory.FEMALE_PRONOUN_TAG,
              TTMentionFactory.FEMALE_TAG);

        unify(TTMentionFactory.PERSON_TAG,TTMentionFactory.MALE_PRONOUN_TAG,
              TTMentionFactory.MALE_TAG);

        unify(TTMentionFactory.MALE_TAG,TTMentionFactory.FEMALE_TAG,
              TTMentionFactory.PERSON_TAG);
    }

    private final Matcher[] mMaleAndFemaleMatchers = new Matcher[] {
        mExactPhraseMatch,
        mPersonFemalePronounMatch,
        mPersonMalePronounMatch,
        mSequenceSubstringMatch,
        mSynonymMatch
    };
    private final Matcher[] mFemaleMatchers = new Matcher[] {
        mExactPhraseMatch,
        mFemalePronounMatch,
        mSequenceSubstringMatch,
        mSynonymMatch
    };
    private final Matcher[] mFemalePronounMatchers = new Matcher[] {
        mFemalePronounMatch
    };
    private final Matcher[] mMaleMatchers = new Matcher[] {
        mExactPhraseMatch,
        mSequenceSubstringMatch,
        mMalePronounMatch,
        mSynonymMatch
    };
    private final Matcher[] mMalePronounMatchers = new Matcher[] {
        mMalePronounMatch
    };
    private final Matcher[] mThingMatchers = new Matcher[] {
        mExactPhraseMatch,
        mSequenceSubstringMatch,
        mSynonymMatch
    };
    private final Matcher[] mUnknownMatchers = new Matcher[] {
        mExactPhraseMatch,
        mSequenceSubstringMatch,
        mSynonymMatch
    };

    private final Killer mHonorificConflictKiller
        = new HonorificConflictKiller();

    private final Killer mGenderKiller
        = new GenderKiller();

    private final Killer mTypeConflictKiller
        = new TypeConflictKiller();

    private final Killer[] mMaleAndFemaleKillers = new Killer[] {
        mHonorificConflictKiller,
        mTypeConflictKiller
    };
    private final Killer[] mMaleKillers = new Killer[] {
        mHonorificConflictKiller,
        mGenderKiller,
        mTypeConflictKiller
    };
    private final Killer[] mMalePronounKillers = new Killer[] {
        mHonorificConflictKiller,
        mGenderKiller,
        mTypeConflictKiller
    };
    private final Killer[] mFemaleKillers = new Killer[] {
        mHonorificConflictKiller,
        mGenderKiller,
        mTypeConflictKiller
    };
    private final Killer[] mFemalePronounKillers = new Killer[] {
        mHonorificConflictKiller,
        mGenderKiller,
        mTypeConflictKiller
    };
    private final Killer[] mThingKillers = new Killer[] {
        mHonorificConflictKiller,
        mTypeConflictKiller
    };
    private final Killer[] mUnknownKillers = new Killer[] {
        mHonorificConflictKiller,
        mTypeConflictKiller
    };


    /**
     * Construct an instance of a matcher set using hard-coded
     * constraints.
     */
    public TTMatchers() {
        mTypeToMatchers.put(TTMentionFactory.PERSON_TAG,mMaleAndFemaleMatchers);
        mTypeToMatchers.put(TTMentionFactory.FEMALE_TAG,mFemaleMatchers);
        mTypeToMatchers.put(TTMentionFactory.FEMALE_PRONOUN_TAG,mFemalePronounMatchers);
        mTypeToMatchers.put(TTMentionFactory.MALE_TAG,mMaleMatchers);
        mTypeToMatchers.put(TTMentionFactory.MALE_PRONOUN_TAG,mMalePronounMatchers);
        mTypeToMatchers.put(TTMentionFactory.ORGANIZATION_TAG,mThingMatchers);
        mTypeToMatchers.put(TTMentionFactory.LOCATION_TAG,mThingMatchers);
        mTypeToMatchers.put(TTMentionFactory.OTHER_TAG,mThingMatchers);
        mTypeToKillers.put(TTMentionFactory.PERSON_TAG,mMaleAndFemaleKillers);
        mTypeToKillers.put(TTMentionFactory.FEMALE_TAG,mFemaleKillers);
        mTypeToKillers.put(TTMentionFactory.FEMALE_PRONOUN_TAG,mFemalePronounKillers);
        mTypeToKillers.put(TTMentionFactory.MALE_TAG,mMaleKillers);
        mTypeToKillers.put(TTMentionFactory.MALE_PRONOUN_TAG,mMalePronounKillers);
        mTypeToKillers.put(TTMentionFactory.ORGANIZATION_TAG,mThingKillers);
        mTypeToKillers.put(TTMentionFactory.LOCATION_TAG,mThingKillers);
        mTypeToKillers.put(TTMentionFactory.OTHER_TAG,mThingKillers);
    }

    /**
     * Return this matcher set's database-driven synonym matcher.
     *
     * @return This matcher set's database-driven synonym matcher.
     */
    private TTSynonymMatch synonymMatcher() {
        return mSynonymMatch;
    }

    /**
     * Return the array of matchers for the specified entity type.
     *
     * @param entityType Type of entity whose matchers are returned.
     * @return Array of matchers for the specified entity type.
     */
    public Matcher[] getMatchers(String entityType) {
        Object result = mTypeToMatchers.get(entityType);
        if (result == null) return mUnknownMatchers;
        return (Matcher[]) result;
    }

    /**
     * Return the array of killers for the specified entity type.
     *
     * @param entityType Type of entity whose killers are returned.
     * @return Array of killers for the specified entity type.
     */
    public Killer[] getKillers(String entityType) {
        Object result = mTypeToKillers.get(entityType);
        if (result == null) return mUnknownKillers;
        return (Killer[]) result;
    }


}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.coref.CachedMention;
import com.aliasi.coref.Mention;

import java.util.Set;

/**
 * A <code>TTMention</code> is a cached mention that is identified
 * by reference, rather than by content.  This allows instances to
 * be discriminated so that a mapping from mention instances may be
 * created.
 */
public class TTMention extends CachedMention {

    /**
     * Construct a mention with the specified phrase, entity type,
     * set of honorifics, array of normal tokens, gender and
     * indication of pronominal-ness.
     *
     * @param phrase Underlying phrase for the mention.
     * @param entityType The type of the mention.
     * @param honorifics The honorifics for the mention.
     * @param normalTokens The sequence of normal tokens for the mention.
     * @param gender The gender of the mention constructed.
     * @param isPronominal <code>true</code> if this mention is a
     * pronoun.
     */
    public TTMention(String phrase, String entityType,
                     Set honorifics, String[] normalTokens,
                     String gender, boolean isPronominal) {
        super(phrase,entityType,
              honorifics,normalTokens,
              gender,isPronominal);
    }


}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.coref.AbstractMentionChain;
import com.aliasi.coref.Matcher;
import com.aliasi.coref.Mention;
import com.aliasi.coref.Killer;

import java.util.HashSet;
import java.util.Set;

/**
 * A <code>TTMentionChain</code> is the threat tracker implementaiton
 * of mention chains.  A threat-tracker mention chain manages the
 * matchers for its mentions through an instance of {@link TTMatchers}.
 * Mention chains may only be constructed from a mention, but then
 * further mentions may be added through {@link #add(Mention)}.
 */

public class TTMentionChain extends AbstractMentionChain {

    private final Set<String> mNormalPhrases = new HashSet<String>();

    private final Set<String> mNormalTokens = new HashSet<String>();

    private final TTMatchers mMatchers;

    /**
     * Construct a mention chain from the specified mention at
     * the specified offset and with the specified identifier.
     *
     * @param mention Mention underlying this singleton mention chain.
     * @param offset Offset of the mention added to this chain.
     */
    TTMentionChain(TTMention mention, int offset, int id,
                   TTMatchers matchers) {
        super(mention,offset,id);
        addMentionPhrases(mention);
        mMatchers = matchers;
    }

    /**
     * Adds the specified mention to this mention chain.
     *
     * @param mention Mention to add to th is mention chain.
     */
    public void add(Mention mention) {
        unifyWithType(mention.entityType());
        addMentionPhrases(mention);
    }

    /**
     * Returns the set of normal phrases derived from the mentions
     * making up this mention chain.  Phrases derived from pronominal
     * mentions are not included.
     *
     * @return The set of normal phrases derived from the mentions
     * making up this mention chain.
     */
    public Set<String> normalPhrases() {
        return mNormalPhrases;
    }

    /**
     * Returns the set of tokens derived from the normal phrases
     * making up this mention chain.  Tokens derived from pronominal
     * mentions are not included.
     *
     * @return The set of tokens derived from the normal phrases
     * making up this mention chain.
     */
    public Set normalTokens() {
        return mNormalTokens;
    }

    /**
     * Return the array of matching functions for this mention chain.
     *
     * @return Array of matching functions for this mention chain.
     */
    public Matcher[] matchers() {
        return mMatchers.getMatchers(entityType());
    }

    /**
     * Return the array of killing functions for this mention chain.
     *
     * @return Array of killing functions for this mention chain.
     */
    public Killer[] killers() {
        return mMatchers.getKillers(entityType());
    }

    private void unifyWithType(String type) {
        if (entityType().equals(type)) return;
        setEntityType(TTMatchers.unifyEntityTypes(entityType(),type));
    }

    private void addMentionPhrases(Mention mention) {
        if (mention.isPronominal()) return;
        if (!mNormalPhrases.add(mention.normalPhrase()))
            return; // already exists
        String[] tokens = mention.normalTokens();
        for (int i = 0; i < tokens.length; ++i)
            mNormalTokens.add(tokens[i]);
    }


}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.coref.Mention;
import com.aliasi.coref.MentionChain;
import com.aliasi.coref.MentionFactory;

import com.aliasi.tokenizer.TokenizerFactory;

import java.util.Collections;

/**
 * An instance of <code>TTMentionFactory</code> is used to create
 * mentions and mention chains for the coreference engine.
 */

public class TTMentionFactory implements MentionFactory {

    private static int mNextId = 0;

    public TTMatchers mMatchers = new TTMatchers();

    private final TokenizerFactory mTokenizerFactory;

    /**
     * Construct an instance of a mention factory.
     *
     * @param tokenizerFactory The tokenizer factory
     * for this mention factory.
     */
    public TTMentionFactory(TokenizerFactory tokenizerFactory) {
        mTokenizerFactory = tokenizerFactory;
    }

    /**
     * Returns the matching function mapping for this mention factory.
     *
     * @return The matching function mapping for this mention factory.
     */
    public TTMatchers matchers() {
        return mMatchers;
    }

    /**
     * Returns the TokenizerFactory for this MentionFactory
     *
     * @return TokenizerFactory
     */

    public TokenizerFactory tokenizerFactory() {
        return mTokenizerFactory;
    }

    /**
     * Returns a newly created mention of the specified phrase and
     * entity type.
     *
     * @param phrase Raw phrase underlying this mention.
     * @param entityType Type of this entity.
     * @return The created mention, an instance of {@link TTMention}.
     */
    public Mention create(String phrase, String entityType) {
        String gender = computeGender(entityType);
        boolean isPronominal = isPronominal(entityType);
        char[] cs = phrase.toCharArray();
        String[] tokens = mTokenizerFactory.tokenizer(cs,0,cs.length).tokenize();
        return new TTMention(phrase,entityType,
                             Collections.EMPTY_SET,
                             tokens,gender,isPronominal);
    }

    /**
     * Returns a newly created mention chain constructed from the
     * specified mention occurring at the specified sentence offset in
     * the document to a mention chain.
     *
     * @param mention Mention to promote to a mention chain.
     * @param sentenceOffset Sentence offset of mention in the document from
     * which it was drawn.
     * @return The created mention chain, an instance of {@link
     * TTMentionChain}.
     */
    public MentionChain promote(Mention mention, int sentenceOffset) {
        return new TTMentionChain((TTMention) mention,sentenceOffset,
                                  mNextId++,
                                  mMatchers);
    }


    /**
     * Returns the gender of the specified entity type.
     *
     * @param type Entity type whose gender is returned.
     * @return Gender associated with the specified entity type.
     */
    public static String computeGender(String type) {
        if (type.equals(MALE_TAG)) return MALE_GENDER;
        if (type.equals(MALE_PRONOUN_TAG)) return MALE_GENDER;
        if (type.equals(FEMALE_TAG)) return FEMALE_GENDER;
        if (type.equals(FEMALE_PRONOUN_TAG)) return FEMALE_GENDER;
        if (type.equals(PERSON_TAG)) return null;
        return NEUTER_GENDER;
    }

    /**
     * Returns <code>true</code> if the specified entity type
     * picks out an entity with male gender.
     *
     * @param type Entity type to check.
     * @return <code>true</code> if the specified entity type
     * picks out an entity with male gender.
     */
    public static boolean isMale(String type) {
        String gender = computeGender(type);
        return gender != null && gender.equals(MALE_GENDER);
    }


    /**
     * Returns <code>true</code> if the specified entity type
     * picks out an entity with female gender.
     *
     * @param type Entity type to check.
     * @return <code>true</code> if the specified entity type
     * picks out an entity with female gender.
     */
    public static boolean isFemale(String type) {
        String gender = computeGender(type);
        return gender != null && gender.equals(FEMALE_GENDER);
    }


    /**
     * Returns <code>true</code> if the specified entity type is
     * pronominal.
     *
     * @param type Entity type to check.
     * @return <code>true</code> if the specified entity type is
     * pronominal.
     */
    public static boolean isPronominal(String type) {
        return type.equals(MALE_PRONOUN_TAG)
            || type.equals(FEMALE_PRONOUN_TAG);
    }

    /**
     * Returns <code>true</code> if the genders implied by the
     * specified entity types are compatible.
     *
     * @param entityType1 First type to test.
     * @param entityType2 Second type to test.
     * @return <code>true</code> if the genders of the specified types
     * are compatible.
     */
    public static boolean genderMatch(String entityType1, String entityType2) {
        String gender1 = computeGender(entityType1);
        String gender2 = computeGender(entityType2);
        if (gender1 == null)
            return gender2 == null
                || gender2.equals(MALE_GENDER)
                || gender2.equals(FEMALE_GENDER);
        if (gender1.equals(MALE_GENDER))
            return gender2 == null || gender2.equals(MALE_GENDER);
        if (gender1.equals(FEMALE_GENDER))
            return gender2 == null || gender2.equals(FEMALE_GENDER);
        return gender1.equals(gender2);
    }

    /**
     * Male gender value.
     */
    public static final String MALE_GENDER = "m";

    /**
     * Female gender value.
     */
    public static final String FEMALE_GENDER = "f";

    /**
     * Neuter gender value.
     */
    public static final String NEUTER_GENDER = "n";


    /** The named-entity tag assigned to people of unknown gender. */
    public static final String PERSON_TAG = "PERSON";

    /** The named-entity tag assigned to female people. */
    public static final String FEMALE_TAG = "FEMALE";

    /** The named-entity tag assigned to female pronouns. */
    public static final String FEMALE_PRONOUN_TAG = "FEMALE_PRONOUN";

    /** The named-entity tag assigned to male people. */
    public static final String MALE_TAG = "MALE";

    /** The named-entity tag assigned to male pronouns. */
    public static final String MALE_PRONOUN_TAG = "MALE_PRONOUN";

    /** The named-entity tag assigned to organizations. */
    public static final String ORGANIZATION_TAG = "ORGANIZATION";

    /** The named-entity tag assigned to locations. */
    public static final String LOCATION_TAG = "LOCATION";

    /** The named-entity tag assigned to entities that does not belong to another class. */
    public static final String OTHER_TAG = "OTHER";



}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.coref.BooleanMatcherAdapter;
import com.aliasi.coref.Mention;
import com.aliasi.coref.MentionChain;

import com.aliasi.util.ObjectToSet;

import java.util.HashSet;
import java.util.Iterator;
import java.util.Set;

/**
 * Implements a matching function that returns the score specified in
 * the constructor if the mention has a synonym in the mention set as
 * specified by the synonym dictionary.  Synonyms are defined over
 * normalized phrases of the mention and the phrases of the mentions
 * in the mention chains.  Pairs of synonymous phrases are added to
 * the matcher with the method {@link #addSynonym(String,String)}.
 */
/**
 * An extension of <code>SynonymMatch</code> that exposes
 * the object-to-set synonym mapping.
 */
public class TTSynonymMatch extends BooleanMatcherAdapter {

    /**
     * The underlying mapping from phrases to their set of synonyms.
     * It's symmetric in that if the value for
     * <code>mSynonymMap.getSet(x).contains(y)</code> should be
     * the same as that for then
     * <code/>mSynonymMap.getSet(y).contains(x)</code>.
     */
    private final ObjectToSet mSynonymMap = new ObjectToSet();

    /**
     * Construct an instance of the synonym matcher.
     *
     * @param score Score to assign to a successful synonym match.
     */
    public TTSynonymMatch(int score) {
        super(score);
    }

    /**
     * Returns the synonym dictionary for
     */
    private ObjectToSet synonymMap() {
        return mSynonymMap;
    }

    /**
     * Returns <code>true</code> if the mention's normal phrase has a
     * synonym that is the normal phrase of one of the chain's mentions.
     *
     * @param mention Mention to test.
     * @param chain Mention chain to test.
     * @return <code>true</code> if there is a sequence substring
     * match between the mention and chain.
     */
    public boolean matchBoolean(Mention mention, MentionChain chain) {
        String phrase = mention.normalPhrase();
        if (!mSynonymMap.containsKey(phrase)) return false;
        Set synonyms = mSynonymMap.getSet(phrase);
        Iterator synonymIterator = synonyms.iterator();
        while (synonymIterator.hasNext()) {
            String synonym = synonymIterator.next().toString();
            Iterator chainMentionIterator = chain.mentions().iterator();
            while (chainMentionIterator.hasNext()) {
                Mention chainMention = (Mention) chainMentionIterator.next();
                if (synonym.equals(chainMention.normalPhrase()))
                    return true;
            }
        }
        return false;
    }

    /**
     * Returns <code>true</code> if the specified normal phrases are
     * synonyms of one another.
     * 
     * @param normalPhrase1 First phrase to test.
     * @param normalPhrase2 Second phrase to test.
     * @return <code>true</code> if the specified normal phrases are
     * synonyms of one another.
     */
    public boolean areSynonyms(String normalPhrase1, String normalPhrase2) {
	return mSynonymMap.getSet(normalPhrase1).contains(normalPhrase2);
    }

    /**
     * Adds the two normal phrases as synonyms for one another.  The
     * operation is symmetric, so that they do not need to be added in
     * the reverse order.  But it is not transitive, so it is possible to
     * have &quot;Bobby&quot; and &quot;Robert&quot; as synonyms,
     * and have &quot;Robby&quot; and &quot;Robert&quot; as synonyms,
     * without having &quot;Bobby&quot; and &quot;Robby&quot; as synonyms.
     *
     * @param normalPhrase1 First normal phrase in the synonym pair.
     * @param normalPhrase2 Second normal phrase in the synonym pair.
     */
    public void addSynonym(String normalPhrase1, String normalPhrase2) {
	if (normalPhrase1.equals(normalPhrase2)) return;
	addSynonymOneWay(normalPhrase1,normalPhrase2);
	addSynonymOneWay(normalPhrase2,normalPhrase1);
    }

    /**
     * Clears all of the synonyms from the matcher.
     */
    public void clearSynonyms() {
	mSynonymMap.clear();
    }

    private void addSynonymOneWay(String normalPhrase1, String normalPhrase2) {
	mSynonymMap.addMember(normalPhrase1,normalPhrase2);
	/* this commented out block ensures transitivity
	Iterator it = new HashSet(mSynonymMap.getSet(normalPhrase1)).iterator();
	while (it.hasNext()) {
	    Object next = it.next();
	    mSynonymMap.addMember(next,normalPhrase2);
	    Iterator it2 = new HashSet(mSynonymMap.getSet(normalPhrase2)).iterator();
	    while (it2.hasNext()) {
		Object next2 = it2.next();
		mSynonymMap.addMember(next,next2);
	    }
	}	
	*/
    }



}

package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.chunk.Chunk;
import com.aliasi.chunk.Chunker;
import com.aliasi.chunk.ChunkFactory;
import com.aliasi.chunk.Chunking;
import com.aliasi.chunk.ChunkingImpl;

import com.aliasi.coref.Mention;
import com.aliasi.coref.MentionChain;
import com.aliasi.coref.WithinDocCoref;

import com.aliasi.dict.DictionaryEntry;
import com.aliasi.dict.ExactDictionaryChunker;
import com.aliasi.dict.MapDictionary;

import com.aliasi.sentences.SentenceChunker;
import com.aliasi.tokenizer.TokenizerFactory;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;

import com.lingpipe.cookbook.chapter7.tracker.TTMention;
import com.lingpipe.cookbook.chapter7.tracker.TTMentionFactory;
import com.lingpipe.cookbook.chapter7.tracker.TTSynonymMatch;

import com.aliasi.util.Strings;

import com.aliasi.xml.SAXWriter;

import java.io.File;
import java.io.IOException;
import java.io.OutputStream;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;



import org.xml.sax.InputSource;
import org.xml.sax.SAXException;

public class Tracker {

	static boolean DEBUG = false;

	TTSynonymMatch mSynonymMatch;

	private final Chunker mSentenceChunker;

	private final TTMentionFactory mMentionFactory;

	protected final XDocCoref mXDocCoref;

	private EntityPhraseChunker mEntityPhraseChunker;

	public Tracker(TokenizerFactory tokenizerFactory,
			Chunker sentenceChunker,
			EntityPhraseChunker entityPhraseChunker,
			Dictionary dictionary) {
		this(tokenizerFactory,
				sentenceChunker,
				entityPhraseChunker,
				dictionary,
				true);
	}
	

	public Tracker(TokenizerFactory tokenizerFactory,
			Chunker sentenceChunker,
			EntityPhraseChunker entityPhraseChunker,
			Dictionary dictionary,
			boolean addSpeculativeEntitiesToEntityUniverse) {
		this(tokenizerFactory,
				sentenceChunker,
				entityPhraseChunker,
				dictionary,
				new XDocCoref(new EntityUniverse(tokenizerFactory),addSpeculativeEntitiesToEntityUniverse));
	}
	
	public Tracker(TokenizerFactory tokenizerFactory,
			Chunker sentenceChunker,
			EntityPhraseChunker entityPhraseChunker,
			Dictionary dictionary,
			XDocCoref xDocCoref) {

		mSentenceChunker = sentenceChunker;
		mMentionFactory
		= new TTMentionFactory(tokenizerFactory);
		mXDocCoref = xDocCoref;
		mEntityPhraseChunker = entityPhraseChunker;
		setDictionaryInEntityUniverse(dictionary);
	}

	// READERS

	

	public Chunker sentenceChunker() {
		return mSentenceChunker;
	}

	public TTMentionFactory mentionFactory() {
		return mMentionFactory;
	}

	public EntityPhraseChunker getMentionChunker() {
		return mEntityPhraseChunker;
	}


	public XDocCoref xDocCoref() {
		return mXDocCoref;
	}


	// WRITERS


	public synchronized OutputDocument[] processDocuments(InputSource in)
			throws SAXException, IOException {
		InputDocument[] docsIn = InputDocument.parse(in);
		OutputDocument[] docsOut = new OutputDocument[docsIn.length];
		for (int i = 0; i < docsIn.length; ++i)
			docsOut[i] = processDocument(docsIn[i]);
		return docsOut;
	}

	public synchronized OutputDocument processDocument(InputDocument document) {
		WithinDocCoref coref
		= new WithinDocCoref(mMentionFactory);

		String title = document.title();
		String content = document.content();

		List<String> sentenceTextList = new ArrayList<String>();
		List<Mention[]> sentenceMentionList = new ArrayList<Mention[]>();
		List<int[]> mentionStartList = new ArrayList<int[]>();
		List<int[]> mentionEndList = new ArrayList<int[]>();

		int firstContentSentenceIndex
		= processBlock(title,0,
				sentenceTextList,
				sentenceMentionList,
				mentionStartList,mentionEndList,
				coref);

		processBlock(content,firstContentSentenceIndex,
				sentenceTextList,
				sentenceMentionList,
				mentionStartList,mentionEndList,
				coref);        

		MentionChain[] chains = coref.mentionChains();

		Entity[] entities  = mXDocCoref.xdocCoref(chains);

		Map<Mention,Entity> mentionToEntityMap
		= new HashMap<Mention,Entity>();
		for (int i = 0; i < chains.length; ++i) {
			for (Mention mention : chains[i].mentions()) {
				mentionToEntityMap.put(mention,entities[i]);
			}
		}

		String[] sentenceTexts
		= sentenceTextList
		.<String>toArray(new String[sentenceTextList.size()]);

		Mention[][] sentenceMentions
		= sentenceMentionList
		.<Mention[]>toArray(new Mention[sentenceMentionList.size()][]);

		int[][] mentionStarts
		= mentionStartList
		.<int[]>toArray(new int[mentionStartList.size()][]);

		int[][] mentionEnds
		= mentionEndList
		.<int[]>toArray(new int[mentionEndList.size()][]);

		Chunking[] chunkings = new Chunking[sentenceTexts.length];
		for (int i = 0; i < chunkings.length; ++i) {
			ChunkingImpl chunking = new ChunkingImpl(sentenceTexts[i]);
			chunkings[i] = chunking;
			for (int j = 0; j < sentenceMentions[i].length; ++j) {
				Mention mention = sentenceMentions[i][j];
				Entity entity = mentionToEntityMap.get(mention);
				if (entity == null) {
					Chunk chunk = ChunkFactory.createChunk(mentionStarts[i][j],
							mentionEnds[i][j],
							mention.entityType()
							+ ":-1");
					//chunking.add(chunk); //uncomment to get unresolved ents as -1 indexed.
				} else {
					Chunk chunk = ChunkFactory.createChunk(mentionStarts[i][j],
							mentionEnds[i][j],
							entity.type()
							+ ":" + entity.id());
					chunking.add(chunk);
				}
			}
		}

		// needless allocation here and last, but simple
		Chunking[] titleChunkings = new Chunking[firstContentSentenceIndex];
		for (int i = 0; i < titleChunkings.length; ++i)
			titleChunkings[i] = chunkings[i];

		Chunking[] bodyChunkings = new Chunking[chunkings.length - firstContentSentenceIndex];
		for (int i = 0; i < bodyChunkings.length; ++i)
			bodyChunkings[i] = chunkings[firstContentSentenceIndex+i];

		String id = document.id();

		OutputDocument result = new OutputDocument(id,titleChunkings,bodyChunkings);
		return result;
	}


	int processBlock(String text,
			int sentenceCount,
			List<String> sentenceTextList,
			List<Mention[]> sentenceMentionList,
			List<int[]> mentionStartList,
			List<int[]> mentionEndList,
			WithinDocCoref coref) {

		Chunking sentenceChunking = mSentenceChunker.chunk(text);
		for (Chunk sentenceChunk : sentenceChunking.chunkSet()) {

			String sentenceText
			= text.substring(sentenceChunk.start(),
					sentenceChunk.end());

			Set<Chunk> entityChunkSet = mEntityPhraseChunker.chunk(sentenceText).chunkSet();

			//mLog.trace("\nChunk set=" + entityChunkSet + "\n");
			Chunk[] chunks
			= entityChunkSet.<Chunk>toArray(new Chunk[entityChunkSet.size()]);
			Arrays.sort(chunks,Chunk.TEXT_ORDER_COMPARATOR);
			Mention[] mentions = new Mention[chunks.length];
			int[] sentMentStarts = new int[chunks.length];
			int[] sentMentEnds = new int[chunks.length];
			int mentionIndex = 0;
			for (Chunk entityChunk : chunks) {
				String chunkText
				= sentenceText.substring(entityChunk.start(),
						entityChunk.end());
				Mention mention
				= mMentionFactory.create(chunkText,
						entityChunk.type());
				sentMentStarts[mentionIndex] = entityChunk.start();
				sentMentEnds[mentionIndex] = entityChunk.end();
				mentions[mentionIndex++] = mention;
				// int withinDocId =  // don't need it -- use chains later
						coref.resolveMention(mention,sentenceCount);
			}
			sentenceTextList.add(sentenceText);
			sentenceMentionList.add(mentions);
			mentionStartList.add(sentMentStarts);
			mentionEndList.add(sentMentEnds);
			++sentenceCount;
		}
		return sentenceCount;
	}

	/*
    //Needed by DictionaryServlet for runtime dictionary updates
    public synchronized Dictionary resetDictionary(InputSource in)
        throws SAXException, IOException {
        Dictionary dict = Dictionary.read(in);
        setDictionary(dict);
        mEntityPhraseChunker.resetDictionary(dict);
        return dict;
    }
	 */

	public synchronized void setDictionaryInEntityUniverse(Dictionary dictionary) {
		EntityUniverse entityUniverse = mXDocCoref.entityUniverse();

		// - in new dict, +user currently
		// entity is dangling if was user defined and not in new dict
		Set<Entity> danglingEntitySet
		= new HashSet<Entity>(entityUniverse.userDefinedEntitySet());
		for (DictionaryEntitySpec entitySpec : dictionary.entitySpecs()) {
			Entity e = entityUniverse.getEntity(entitySpec.id());
			danglingEntitySet.remove(e);
		}
		for (Entity entity : danglingEntitySet)
			entityUniverse.remove(entity);

		// +in new dict
		for (DictionaryEntitySpec entitySpec : dictionary.entitySpecs())
			entityUniverse.updateEntitySpec(entitySpec);

		//System.out.println("Got Dict" + dictionary.stopPhrases());
		//mEntityPhraseChunker.setStopPhrases(dictionary.stopPhrases());
		//        TokenizerFactory casePreservingTokenizer = IndoEuropeanTokenizerFactory.FACTORY;
		//mEntityPhraseChunker.setDictionaryChunker(dictionary,casePreservingTokenizer);
		//mEntityPhraseChunker.setStopSubstringList(STOP_SUBSTRING_LIST);

		setSynonymMatcher(dictionary);



		//        mEntityDictionary = dictionary;
	}



	private void setSynonymMatcher(Dictionary dictionary) {
		TTSynonymMatch ttSynonymMatcher
		= mMentionFactory.mMatchers.mSynonymMatch;
		ttSynonymMatcher.clearSynonyms();
		for (DictionaryEntitySpec entitySpec : dictionary.entitySpecs()) {
			String[] aliases = entitySpec.aliases();
			for (int i = 0; i < aliases.length; ++i)
				for (int j = i + 1; j < aliases.length; ++j)
					ttSynonymMatcher.addSynonym(aliases[i],aliases[j]);
		}
	}




}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.coref.Killer;
import com.aliasi.coref.Mention;
import com.aliasi.coref.MentionChain;

/**
 * The <code>TypeConflictKiller</code> implements a killing function
 * that defeats the matching of a mention with chain of incompatible
 * type.  
 */
public class TypeConflictKiller implements Killer {

    /**
     * Returns <code>true</code> if the specified mention and mention
     * chain have types that cannot be unified.  This is determined
     * by whether the static method {@link TTMatchers#unifyEntityTypes(String,String)}
     * applied to the type of the mention and the type of the chain
     * returns <code>null</code>.
     *
     * @param mention Mention to test.
     * @param chain Mention chain to test.
     * @return <code>true</code> if the mention has a  type that
     * is incompatible with the type of the chain.
     */
    public boolean kill(Mention mention, MentionChain chain) {
	String unifiedType
	    = TTMatchers.unifyEntityTypes(mention.entityType(),
					  chain.entityType());
	return unifiedType == null;
    }

}
package com.lingpipe.cookbook.chapter7.tracker;

import com.aliasi.coref.Mention;
import com.aliasi.coref.MentionChain;

import com.aliasi.tokenizer.Tokenizer;
import com.aliasi.tokenizer.TokenizerFactory;


//import com.aliasi.util.Collections;
import com.aliasi.util.ObjectToSet;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Set;

public class XDocCoref {

    protected final EntityUniverse mEntityUniverse;

    private final TokenizerFactory mTokenizerFactory;
    protected boolean mAddSpeculativeEntities;

    
    public XDocCoref(EntityUniverse entitySet) {
        this(entitySet, true);
    }

    public XDocCoref(EntityUniverse entitySet,
                     boolean addSpeculativeEntitiesToEntityUniverse) {
        mEntityUniverse = entitySet;
        mTokenizerFactory = entitySet.tokenizerFactory();
        mAddSpeculativeEntities = addSpeculativeEntitiesToEntityUniverse;
    }

    // READERS

    public EntityUniverse entityUniverse() {
        return mEntityUniverse;
    }

    /*    // should use this to normalize everywhere, but need Xdoc handle
    private String normalPhrase(String phrase) {
        return concatenateNormalTokens(normalTokens(phrase));
    }

    private String[] normalTokens(String phrase) {
        List<String> tokenList = new ArrayList<String>();
        char[] cs = phrase.toCharArray();
        Tokenizer tokenizer
            = mTokenizerFactory
            .tokenizer(cs,0,cs.length);
        for (String token : tokenizer)
            if (!nonEntityToken(token))
                tokenList.add(token);
        return tokenList.<String>toArray(new String[tokenList.size()]);
    }

    */

    // WRITERS

    // returns map from TTMention to ids
    // massive side-effects on entity set
    public Entity[] xdocCoref(MentionChain[] chains) {
        Long localId = mEntityUniverse.getLastId();
        Entity[] entities = new Entity[chains.length];

        Map<MentionChain,Entity> chainToEntity
            = new HashMap<MentionChain,Entity>();
        ObjectToSet<Entity,MentionChain> entityToChainSet
            = new ObjectToSet<Entity,MentionChain>();

        for (MentionChain chain : chains)
            resolveMentionChain((TTMentionChain) chain,
                                chainToEntity, entityToChainSet);

        for (int i = 0; i < chains.length; ++i) {
            TTMentionChain chain = (TTMentionChain) chains[i];
            Entity entity = chainToEntity.get(chain);

            if (entity != null) {
                 if (Tracker.DEBUG) {
                   System.out.println("XDOC: resolved to " + entity);  //
                   Set chainSetForEntity = entityToChainSet.get(entity);
                   if (chainSetForEntity.size() > 1) {
                     System.out.println("XDOC: multiple chains resolved to same entity " + entity.id());
                     
                   }
                 }

                entities[i] = entity;
                if (entity.allowSpeculativeAliases())
                    addMentionChainToEntity(chain,entity);
            } else {

                Entity newEntity 
                    = mAddSpeculativeEntities  
                    ? promote(chain) 
                    :promoteButDoNotAddToEntityUniverse(chain, ++localId);
                entities[i] = newEntity;
            }
        }
        return entities;
    }
    
    public void setAddSpeculativeEntities (boolean value) {
        mAddSpeculativeEntities = value;
    }

    private void resolveMentionChain(TTMentionChain chain,
                                     Map<MentionChain,Entity> chainToEntity,
                                     ObjectToSet<Entity,MentionChain> entityToChainSet) {
         if (Tracker.DEBUG)
             System.out.println("XDOC: resolving mention chain " + chain);
        int maxLengthAliasOnMentionChain = 0;
        int maxLengthAliasResolvedToEntityFromMentionChain = -1;
        Set<String> tokens = new HashSet<String>();
        Set<Entity> candidateEntities = new HashSet<Entity>();
        for (String phrase : chain.normalPhrases()) {
            String[] phraseTokens = mEntityUniverse.normalTokens(phrase);
            String normalPhrase = EntityUniverse.concatenateNormalTokens(phraseTokens);
            for (int i = 0; i < phraseTokens.length; ++i)
                    tokens.add(phraseTokens[i]);
            int length = phraseTokens.length;
            if (length > maxLengthAliasOnMentionChain)
                maxLengthAliasOnMentionChain = length;
            Set<Entity> matchingEntities
                = mEntityUniverse.xdcEntitiesWithPhrase(phrase);

            for (Entity entity : matchingEntities) {
                //System.out.println("Candidate Entity: " + entity);
                if (null != TTMatchers.unifyEntityTypes(chain.entityType(),
                                                        entity.type())) {
                    if (maxLengthAliasResolvedToEntityFromMentionChain < length)
                        maxLengthAliasResolvedToEntityFromMentionChain = length;
                    candidateEntities.add(entity);
                }
            }
        }
        resolveCandidates(chain,
                          tokens,
                          candidateEntities,
                          maxLengthAliasResolvedToEntityFromMentionChain == maxLengthAliasOnMentionChain,
                          chainToEntity,
                          entityToChainSet);
    }


    private void resolveCandidates(TTMentionChain chain,
                                   Set<String> tokens,
                                   Set<Entity> candidateEntities,
                                   boolean resolvedAtMaxLength,
                                   Map<MentionChain,Entity> chainToEntity,
                                   ObjectToSet<Entity,MentionChain> entityToChainSet) {
        filterCandidates(chain,tokens,candidateEntities,resolvedAtMaxLength);
        if (candidateEntities.size() == 0)
            return;
        if (candidateEntities.size() == 1) {
            Entity entity = candidateEntities.iterator().next();
            chainToEntity.put(chain,entity);
            entityToChainSet.addMember(entity,chain);
            return;
        }
        if (Tracker.DEBUG) {
            System.out.println("Blown UP; candidateEntities.size()=" + candidateEntities.size());
            for (Entity entity : candidateEntities ) 
                System.out.println(entity);
        }
    }



    protected void filterCandidates(TTMentionChain chain,
                                  Set<String> tokens,
                                  Set<Entity> candidateEntities,
                                  boolean resolvedAtMaxLength) {
        if (candidateEntities.size() < 1) return;
        if (resolvedAtMaxLength) return;
        List<Entity> filteredEntities = new ArrayList<Entity>();
        Iterator<Entity> candidateIterator = candidateEntities.iterator();
        while (candidateIterator.hasNext()) {
            Entity entity = candidateIterator.next();
            Set<String> entityTokens = entity.tokens(mTokenizerFactory);
            if (entityTokens.size() == 1) continue;
            Set<String> intersection = new HashSet<String>(tokens);
            intersection.retainAll(entityTokens);
            int entityTokensSize = entityTokens.size();
            int intersectionSize = intersection.size();
            if ((entityTokensSize <= 2 && intersectionSize < 2)
                || (entityTokensSize > 2
                    && intersectionSize < tokens.size() - 1)
                || !TTMentionFactory.genderMatch(chain.entityType(),
                                                 entity.type())) {
                candidateIterator.remove();
            }
        }
    }

    private Entity promoteButDoNotAddToEntityUniverse(TTMentionChain chain, Long localId) {
        Entity entity = new Entity(localId,chain.entityType(),
                                   null,null,chain.normalPhrases(),new HashSet<String>());
        return entity;
    }

    private Entity promote(TTMentionChain chain) {
        Entity entity
            = mEntityUniverse.createEntitySpeculative(chain.normalPhrases(),
                                                      chain.entityType());
         if (Tracker.DEBUG)
         System.out.println("XDOC: promoted " + entity);
        return entity;
    }


    private void addMentionChainToEntity(TTMentionChain chain, Entity entity) {
        for (String phrase : chain.normalPhrases()) {
            mEntityUniverse.addPhraseToEntity(phrase,entity);
        }
    }

    private static boolean nonEntityToken(String token) {
        char[] cs = token.toCharArray();
        for (int i = 0; i < cs.length; ++i)
            if (Character.isLetter(cs[i])
                || Character.isDigit(cs[i])) return false;
        return true;
    }

    int tokenLength(String phrase) {
        int length = 1;
        for (int i = 0; i < phrase.length(); ++i)
            if (phrase.charAt(i) == ' ')
                ++length;
        return length;
    }
    /*
    private static String normalizeWhitespace(String whitespace) {
        return whitespace.length() > 0 ? " " : "";
    }

    private static String concatenateNormalTokens(String[] toks) {
        if (toks.length < 1) return "";
        if (toks.length == 1) return toks[0];
        StringBuilder sb = new StringBuilder(toks[0]);
        for (int i = 1; i < toks.length; ++i) {
            sb.append(' ');
            sb.append(toks[i]);
        }
        return sb.toString();
    }
    */

}
